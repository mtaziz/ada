{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports (same as tuto ML)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder # is this really needed ?\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV, PredefinedSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define long custom chart drawing and comparison functions we reuse a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use this a lot :D\n",
    "def drawPies(u_rates, t_rates, labels, supertitle):\n",
    "    \"\"\"draws pretty comparative pie charts\n",
    "    u_rates : rates for untreated group\n",
    "    t_rates : rates for treated group\n",
    "    lables: labels for values in rates\n",
    "    supertitle: title of chart\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    fig.suptitle(supertitle)\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.pie(u_rates, labels = labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"control group\")\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.pie(t_rates, labels = labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"treated group\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groups(table) :\n",
    "    \"\"\"\n",
    "    Compares groups in merged dataframe\n",
    "    table : dataframe to compare treated and untreated in\n",
    "    \"\"\"\n",
    "    columns = ['untreated', 'treated']\n",
    "    #index = ['age', 'educ', 'black', 'hisp', 'married', 'no_degree']\n",
    "    index = ['black', 'hisp', 'married', 'no_degree']\n",
    "    result = pd.DataFrame(columns=columns, index=index)\n",
    "    #result['untreated']['age'] = table['age_y'].mean()\n",
    "    #result['untreated']['educ'] = table['educ_y'].mean()\n",
    "    result['untreated']['black'] = table['black_y'].mean()\n",
    "    result['untreated']['hisp'] = table['hispan_y'].mean()\n",
    "    result['untreated']['married'] = table['married_y'].mean()\n",
    "    result['untreated']['no_degree'] = table['nodegree_y'].mean()\n",
    "    #result['treated']['age'] = table['age_x'].mean()\n",
    "    #result['treated']['educ'] = table['educ_x'].mean()\n",
    "    result['treated']['black'] = table['black_x'].mean()\n",
    "    result['treated']['hisp'] = table['hispan_x'].mean()\n",
    "    result['treated']['married'] = table['married_x'].mean()\n",
    "    result['treated']['no_degree'] = table['nodegree_x'].mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "#####mean isn't perfectly accurate!!!!!!!!!\n",
    "#####it doesn't tell us about variance etc\n",
    "####only use it for binary variables, then it gives us percentages\n",
    "###use other function for interval data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preform a naive data analysis using plots and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the data set\n",
    "lalonde_df = pd.read_csv('lalonde.csv')\n",
    "#give a first look\n",
    "lalonde_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that a naive researcher unfamiliar with observational studies would treat the data as if it was a randomized trial, not taking into consideration the hidden correlates.\n",
    "\n",
    "We can easily imagine that the first thing he would do is split the salary (_['re78']_) data into 2 sets: treated and untreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masks to be used alot later\n",
    "treated = (lambda x: x.treat == 1)\n",
    "untreated = (lambda x: x.treat == 0)\n",
    "\n",
    "#apply masks to get treated and untreated\n",
    "treated_salary = lalonde_df[treated(lalonde_df)]['re78']\n",
    "untreated_salary = lalonde_df[untreated(lalonde_df)]['re78']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Describing the numbers**\n",
    "\n",
    "We first look at the numbers to see how many subjects in each group we have an how the values are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lalonde_df.groupby('treat')['re78'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numbers above, we extract the following information from the data :\n",
    "\n",
    "- The untreated group has more people.\n",
    "- The untreated group's salaries have a higher mean.\n",
    "- However, the max salary in the treated group is 3x higher! The 1st quartile is also two times higher on the treated group.\n",
    "- Finally, we have that the second and third quartiles are higher in the untreated. Quartiles are more resistent to outliers, hence indicating that the control group is faring better.\n",
    "- The interquartile distance is larger in the untreated set, as we have outliers in the set this is a better measure for 'variance'. Thus we say that the control group has a more heterogenous salary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Visualizing the data:**\n",
    "\n",
    "We now plot the final salary data in a histogram to find the distribution of salaries of the two groups. We add weights so we can look at percentages instead of number of people in both bins, as the number of people in the two groups is not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "#define same bin size\n",
    "bins = np.linspace(0, max(lalonde_df['re78']), 50)\n",
    "#add weights to get percentages\n",
    "plt.hist(untreated_salary, weights=np.ones(len(untreated_salary))/len(untreated_salary), alpha=.5 , bins=bins)\n",
    "plt.hist(treated_salary, weights=np.ones(len(treated_salary))/len(treated_salary), alpha=.5, bins=bins)\n",
    "plt.title('Histogram showing salary treated and untreated groups')\n",
    "plt.legend(['untreated', 'treated'])\n",
    "plt.xlabel('Yearly salary')\n",
    "plt.ylabel('percentage of subjects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First insights:\n",
    "\n",
    "By looking at the graph, we see a very similar distribution for both functions, except that outliers are present in the treated group.\n",
    "\n",
    "We also note that there are more relatively more subjects in the untreated group with a salary between 10k and 20k, while both groups have a similar ratio of subjects in the <10k section of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Boxplot:**\n",
    "\n",
    "A boxplot will illustrate the above more consciely the 5 number summary we presented first. Note that we remove the outliers, as we consider them to not be representative of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_treat_untreat(treated_salary, untreated_salary):\n",
    "    \"\"\"draws boxplot of distribution of salary \"\"\"\n",
    "    plt.boxplot([treated_salary, untreated_salary], labels=['treated', 'untreated'], showfliers=False)\n",
    "    plt.title('Distribution of salary by treated and untreated')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "\n",
    "boxplot_treat_untreat(treated_salary, untreated_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "If the treatment was effective, we should see that the treated group is more sucessful on average, as they were placed in program, wherease the untreated group was left to fend for themselves.\n",
    "\n",
    "By merging all of the insights the researcher has drawn from the 3 steps of his analysis, he can conclude that **the treatment shows no effect**. The salary distributions are similar in both cases, indicating that the treatment isn't effective.\n",
    "\n",
    "Additionaly, the treated group has in average a lower salary (except for the handful of people get lucky and find a good job). This is shown by the boxplot: the whiskers extend higher in the untreated group and the median and lower wisker as situated higher up. Since the difference is somewhat small this may just be due to chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing a simplistic analysis of the data ignoring underlying factors –such as race and education– that could influence the outcome, we start looking at the whole table assuming the other features will impact _['re78']_.\n",
    "\n",
    "We split out analysis in by **categorical** and **intervall** data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Categorical data :**\n",
    "\n",
    "Regarding categorical data, we should look at percentages present in each population, as the treated and control group have different sizes. Thus we define the rates for race, degree and mariage depending on each treatment to be able to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['black', 'hispan', 'married', 'nodegree']\n",
    "\n",
    "#as the values are binary the mean is equal to the percentage of occurence\n",
    "percentages = lalonde_df.groupby('treat')[categorical].mean()\n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Race ratios:\n",
    "\n",
    "We will start with race. As we do not have numbers for \"White\" participants, we get the number of \"Blacks\" and \"Hispanics\" for each treatment group and substract the total. We then compare the rates of each race using pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_u, black_t = percentages['black']\n",
    "hispan_u, hispan_t = percentages['hispan']\n",
    "#there is no overlap in the hispan and black categories, \n",
    "#we assume people that are neither are white (which we checked, it is the case)\n",
    "white_u, white_t = (1 - black_u - hispan_u, 1 - black_t - hispan_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_race_rates = [black_u, hispan_u, white_u]\n",
    "t_race_rates = [black_t, hispan_t, white_t]\n",
    "#give name to lable\n",
    "race_labels = 'Black', 'Hispanic', 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawPies(u_race_rates, t_race_rates, race_labels, 'Racial groups in percent by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are significantly more black subjects in the treatment group than in the untreated group. We also note that there are more hispanic people in the control group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Degree ratios:\n",
    "\n",
    "To have a better understanding of the difference of salaries, we also need to look at the level of education of the participants of each treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_u, degree_t = percentages['nodegree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate rate for degree havers in treated and untreated group\n",
    "u_degree_rates = [degree_u, 1 - degree_u]\n",
    "t_degree_rates = [degree_t, 1 - degree_t]\n",
    "degree_labels = 'Degree', 'No degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw pie diagram\n",
    "drawPies(u_degree_rates, t_degree_rates, degree_labels, 'Percentage of individuals with a degree, by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the treated group is less educated, by a difference of over 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Marriage ratios:\n",
    "Finally, we look at the rates of married people among both groups as it is our last feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#married and unmarried by treatment\n",
    "married_u, married_t = percentages['married']\n",
    "not_married_u, not_married_t = (1 - married_u, 1 - married_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_marriage_rates = [married_u, not_married_u]\n",
    "t_marriage_rates = [married_t, not_married_t]\n",
    "mariage_labels = ['Married', 'Not married']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawPies(u_marriage_rates, t_marriage_rates, mariage_labels, 'Percentage of married individuals by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again note that the treated group contains less married individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Unemployment ratios:\n",
    "\n",
    "Even though salaries are not categories but intervals, it is important to compare unemployment rates between both groups (which we define at categories, employed and unemployed). To get better insights, we will plot the years 1974 and 1975."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining list of non binary variables\n",
    "salaries = ['re74', 're75']\n",
    "cat_salaries = lalonde_df.copy()\n",
    "\n",
    "unemployed_labels = 'Employed', 'Unemployed'\n",
    "#for each column draw a Boxplot\n",
    "for sal in salaries:\n",
    "    #get employed = 1 vs unemployed = 0\n",
    "    cat_salaries[sal] = cat_salaries[sal].map(lambda x : 0 if x == 0 else 1)\n",
    "    #get percentages\n",
    "    u_employed, t_employed = cat_salaries.groupby('treat')[sal].mean()\n",
    "    drawPies([1-u_employed, u_employed],[1-t_employed, t_employed], \n",
    "             unemployed_labels, 'Unemployment rates by treatment in 19'+sal[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our assumtion that both groups are balanced pre-treatment is wrong for unemplyment aswell.\n",
    "There are much more unemployed people in the treated group than in the untreated group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the categorical data, we can say that the underlying factors between the two groups are not similar at all.\n",
    "The treated group is significantly more black, less educated, less employed and less married. All these factors influence employment and should be taken into consideration.\n",
    "\n",
    "All these factors influence salary:\n",
    "- due to racial inequelity (especialy given it was 1970) non-whites were likely to work in less well paying professionsand/or be underpayed\n",
    "- better education gets bettr jobs\n",
    "- people who already had jobs in 1974/5 are more likely to still have a job in 1978, and may have had a raise.\n",
    "- marriage is an indicator of stability, which may influence job prospects. These individuals may have gotten married because they had a stable job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Interval data :**\n",
    "\n",
    "We look at non binary data and the their distribution.\n",
    "\n",
    "\n",
    "To do this we first do a box plot and relative fequency histogram for the intervall variables:\n",
    "\n",
    "*note that we remove outliers from the box plot, this is justified as we care about the general distibution of values in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we define a function that compares interval data, will be used again for 1.4\n",
    "\n",
    "#defining list of non binary variables\n",
    "intervals = ['age', 'educ', 're74', 're75'] #is reused in later fuction\n",
    "\n",
    "def intervall_plots(df):\n",
    "    \"\"\"constructs boxplot and relative frequency histogram for data frame\n",
    "    df: dataframe to be analyzed\n",
    "    \"\"\"\n",
    "    #for each column draw a Boxplot\n",
    "    for col in intervals:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        treated_ = df[treated(df)][col]\n",
    "        untreated_ = df[untreated(df)][col]\n",
    "\n",
    "        #boxplot\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.title(\"Boxplot of \" + col)\n",
    "        plt.boxplot([untreated_, treated_], \n",
    "                    labels=['untreated', 'treated'], showfliers=False)\n",
    "        plt.ylabel(col)\n",
    "\n",
    "        #histogram\n",
    "        plt.subplot(2,2,2)\n",
    "        bins = np.linspace(min(df[col]), max(lalonde_df[col]), 50)\n",
    "        plt.title(\"Relative frequency histogram of \" + col)\n",
    "        plt.ylabel('percentage')\n",
    "        plt.xlabel(col)\n",
    "        plt.hist(untreated_, weights=np.ones(len(untreated_))/len(untreated_), alpha=.5 , bins=bins)\n",
    "        plt.hist(treated_, weights=np.ones(len(treated_))/len(treated_), alpha=.5, bins=bins)\n",
    "        plt.legend(['untreated', 'treated'])\n",
    "        \n",
    "intervall_plots(lalonde_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \n",
    "- pre treatment, the salaries are very unbalanced, the treatment group earning much less than the untreated group.\n",
    "- there is a different age distribution in the two groups, the treated group being a bit younger, containg a lot of individuals in their 20's.\n",
    "- their education level is fairly similar over all, but there are more well educated people in teh controll group, and years of education does not indicate the accuirement of a degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Evolution:\n",
    "\n",
    "Even though it is useful to plot the salaries to see the difference between the years, it is much more useful to understand how the salary of each participant has changed over the years. \n",
    "It would be especially interesting to see if the in the outliers for the 3 years are connected.\n",
    "To do so, we will visualize our data using a parallel plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement parallel plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "parallel_coordinates(lalonde_df[untreated(lalonde_df)][['id','re74', 're75', 're78']], 'id', color='Blue', alpha=0.5)\n",
    "parplot = parallel_coordinates(lalonde_df[treated(lalonde_df)][['id','re74', 're75', 're78']], 'id', color='Orange' , alpha=0.7)\n",
    "#remove legend for readability\n",
    "parplot.legend_.remove()\n",
    "plt.title('Salary over time for each participant')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Annualy Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orange represents the treated group, blue the untreated group.\n",
    "\n",
    "We see that:\n",
    "- The treated group started out with a lower salary, as we can see the 15000 and above brachet is all blue.\n",
    "- 75 was a bad year for everybody, treated or untreated, as we can see by the indent in the plot.\n",
    "- The outliers are partialy people who were already well payed in 74, partialy people who 'made it'.\n",
    "- There is a lot of movement up for the treated group between 75 and 78, we see a slight upwards movement for the untreated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Salary by race, education and marital status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at salary in relation the categorical variables discussed before.\n",
    "\n",
    "Above we hypothesized that these would influence the salary. Here we try to see if we can see such an influence on the salary.\n",
    "\n",
    "**Plotting by race:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining mask for white\n",
    "white = (lalonde_df['black'] == 0) & (lalonde_df['hispan'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years = ['re74', 're75', 're78']\n",
    "for year in years:\n",
    "    plt.boxplot([lalonde_df[white][year], lalonde_df[lalonde_df['black'] == 1][year],\n",
    "                 lalonde_df[lalonde_df['hispan'] == 1][year]], \n",
    "                labels=['white', 'black', 'hispanic'])\n",
    "    plt.title('salary distribution by race in 19'+year[-2:])\n",
    "    plt.ylabel('annual earnings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \n",
    "- there is a racial discrepancy in salary in our dataset\n",
    "- the outliers are all black individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By martial status and degree:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    #for every year we plot marriage\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('unmarried vs married salary in 19'+year[-2:])\n",
    "    sns.boxplot(data=lalonde_df, x='married', y=year, hue='treat')\n",
    "    plt.ylabel('salary')\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(range(2),('unmarried', 'married'))\n",
    "    \n",
    "    #and degree\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('degree vs nodegree salary in 19'+year[-2:])\n",
    "    sns.boxplot(data=lalonde_df, x='nodegree', y=year, hue='treat')\n",
    "    plt.xticks(range(2),('degree', 'no degree'))\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that:\n",
    "- Married people make more than unmarried people, no matter the treatment or year\n",
    "- People with degrees make more money that peopel with no degree, no matter the treatment or year\n",
    "- Treatment seems even out the difference with the group, but not much between them. We see this by comparing the years pre treatment (1974/5) and the year after treatment (1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at age and salary**\n",
    "\n",
    "\n",
    "It is well known that age my influence a persons prospect at finding a job and their potential salary, we thus map salary by age, and treatment for the 3 observed years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    sns.factorplot(data=lalonde_df, x='age', y=year, hue='treat',aspect=4, size=3)\n",
    "    plt.title('salary by age and treatment in 19'+year[-2:])\n",
    "    plt.yticks(np.linspace(0, 40000, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "\n",
    "- Pre-treatment, the treated group makes less that the controll group.\n",
    "- Young people easely in the treatment group catch up on the control group post-treatment.\n",
    "- Treatment group in general catches up, or at leats finds employment - the treatment seems to have an effect\n",
    "- 75 bad year, low salary in general (look at y axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the interval data, and more specificaly at the salaries of participants, we can say that the underlying conditions such as race, education marital status influence the salary.\n",
    "As our two groups are not balanced, this interferes with our first analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Correlation data :**\n",
    "\n",
    "After working on each value alone, we want to understand how each value is (linearly) linked to others on each pair of features. We look at the pairplot, as correlation by itself does not give us any insights, as clearly the data is not linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we can somewhat seperate the two groups, inicating that they are not the same.\n",
    "\n",
    "It does not tell us alot, other than we can not see any linear dependence in the group.\n",
    "We leave it as it is part of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(lalonde_df[['treat', 're78']+intervals], markers='+', hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A propsensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen our naive analysis was preformed based the untrue assumption of a fair distribution in both sets.\n",
    "This is not the case.\n",
    "To create a fair set to use on our observational study, we calculate the propensity score based on the underlying factors before treatment:\n",
    "\n",
    "[age, educ, hispan, black, nodegree, re74, re75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table = lalonde_df.copy() #otherwise we modify lalonde_df when we modify prop_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our target and training data:\n",
    "\n",
    "X = prop_table.iloc[:, 2:-1] #rows age to 're75'\n",
    "y = prop_table.iloc[:, 1:2] #treated or not\n",
    "y = np.ravel(y) #flatten array\n",
    "print('First elements of Y : \\n', y[0:5],'\\nFirst elements of X\\n', X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define our model\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X, y)\n",
    "print('Accuracy of prediction: ',logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of prediction : \", logistic.predict(X[0:6]), ' reality :', y[0:6])\n",
    "print('Example of prediction in percent : \\n', logistic.predict_proba(X[0:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get propensity scores, probability of \"being a subject\"\n",
    "prop_table['propensity_scores'] = pd.Series(logistic.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the propensity scores to find a matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try finding a matching based on the propensity scores to get a more balanced set.\n",
    "\n",
    "Matching the two is an equivalent problem to find a matching in a bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "B = nx.Graph()\n",
    "#1. Creat graph with nodes as id\n",
    "B.add_nodes_from(prop_table['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Add edges from each treated to each untreated subject\n",
    "#    with weight on each node being the difference between the two\n",
    "for row_i in prop_table[treated(prop_table)].iterrows():\n",
    "    for row_j in prop_table[untreated(prop_table)].iterrows():\n",
    "        B.add_edge(row_i[1]['id'],row_j[1]['id'], \n",
    "                   #-x to transform minimisation problem into maximisation problem\n",
    "                   weight= 1 - np.abs(row_i[1].propensity_scores - row_j[1].propensity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Find matching\n",
    "matching_dict = nx.max_weight_matching(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example matches:')\n",
    "list(matching_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get matching\n",
    "matched = prop_table.copy()[prop_table['id'].isin(matching_dict)]\n",
    "print('we have : ',len(matched)/2, ' matched subjects') #pairs appear in 2 order s ab and ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking at the outcome\n",
    "We compare the outcome [re78] for treated and untreated groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(data=matched, x='treat', y='re78')\n",
    "plt.xticks(range(2), ('control', 'treatment'))\n",
    "plt.title('Salary in 1978')\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(data=matched, x='treat', y='re78',  showfliers=False)\n",
    "plt.xticks(range(2), ('control', 'treatment'))\n",
    "plt.title('Salary in 1978, no outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the treatment group's salary distribution in 1978 is slightly higher than the control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the feature distrbution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preforme a similary analysis to the one done in 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_percentages = matched.groupby('treat')[categorical].mean() #unbalanced\n",
    "matched_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see that for the categorical varibles, marriage and degree rates are more balanced, while the racial distribution in the two groups is still very skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look at intervall data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervall_plots(matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that they are all much more balanced than before, especially educ and re75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Balancing the groups further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noted above there are still way more black subjects in the treated group than in the untreated group.\n",
    "Additionaly, we still have outliers in the treated group.\n",
    "\n",
    "We try to balance the both groups by removing white subjects matched with black subjects.\n",
    "We als balance the distribution of teh hispanic subjects in both groups\n",
    "We first merge the two dataframes to have the matched subjects side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergin dataframe and adding propensity score difference column .. do we still need this?\n",
    "matched['match'] = matched['id'].map(matching_dict)\n",
    "balanced_match = matched[treated(matched)].merge(matched[untreated(matched)], left_on='id', right_on='match')\n",
    "balanced_match['difference'] = abs(balanced_match['propensity_scores_x'] - balanced_match['propensity_scores_y'])\n",
    "print('we have : ', len(balanced_match), ' matched subjects')\n",
    "balanced_match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove the matched pairs that are white/black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches that are black/white hispanic/white missmatched\n",
    "race_bool_1 = (balanced_match['black_x'] == 1) & (balanced_match['black_y'] == 0)\n",
    "#this removes black hispanic pairs??? is this what you want??? it actuay doesn't change anything at all anyway\n",
    "#tell me why...\n",
    "#race_bool_2 = (balanced_match['black_x'] == 1) & (balanced_match['hispan_y'] == 1)\n",
    "#other syntax didn't work\n",
    "balanced_match = balanced_match.drop(balanced_match[race_bool_1].index)\n",
    "#balanced_match = balanced_match.drop(balanced_match[race_bool_2].index)\n",
    "print('we have : ', len(balanced_match), ' matched subjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if the data set is more balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_groups(balanced_match)\n",
    "#but now married is mismatched smh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_blanced_interval(df):\n",
    "    df1 = df.iloc[:,:11]\n",
    "    df2 = df.iloc[:, 13:-3]\n",
    "    df2.columns = lalonde_df.columns\n",
    "    df1.columns = lalonde_df.columns\n",
    "    balanced_df = pd.concat([df1, df2])\n",
    "    intervall_plots(balanced_df)\n",
    "    \n",
    "plot_blanced_interval(balanced_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches that are black/white missmatched and have a large difference in propensity scores\n",
    "marriage_bool_1 = (balanced_match['married_x'] == 1) & (balanced_match['married_y'] == 0)\n",
    "marriage_bool_2 = (balanced_match['married_y'] == 1) & (balanced_match['married_x'] == 0)\n",
    "balanced_match = balanced_match.drop(balanced_match[marriage_bool_1 | marriage_bool_2].index)\n",
    "print('we have : ', len(balanced_match), ' matched subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_groups(balanced_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match seems balanced blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at intervall data and see blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blanced_interval(balanced_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finaly we remove outliers from the lucky few from the treatment set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers - no need unless looking at mean\n",
    "balanced_match = balanced_match.drop(balanced_match[balanced_match.re78_x > 30000].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_treat_untreat(balanced_match['re78_x'], balanced_match['re78_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat([balanced_match.re78_y.describe(),balanced_match.re78_x.describe()], axis=1)\n",
    "stats.columns = ['untreated', 'treated']\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After controlling for unerlying factors we see that the treated population fares better than the untreated population, which we can see as the overall distribution of the salary in the two groups is moved up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the TF-IDF features of our dataset, using a vectorizer. As we understood the question, what was asked was not to use any of the given datasets from sklearn, but to use all of the data. Thus, we do not use the train and test subsets given to us in sklearn, but will create our own such subsets, adding a validation subset.\n",
    "\n",
    "Also note that we **remove the headers, footers and quotes**, as proposed in the <a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\">sklearn tutorial</a> of the dataset, as to have something more realistic and without any of the metadata. \n",
    "This significantly decreases our accuracy, as we show later.\n",
    "\n",
    "Note also that we did not use the *sklearn.datasets.fetch_20newsgroups_vectorized* function that returns the TF-IDF features directly, as it would defeat the purpose of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data we need to use the vectorizer on. Remove metadata as proposed by sci-kit tutorial\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_with_headera = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked in the question, before seperating in subsets, we will use the vectorizer on the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors is a sparse matrix\n",
    "vectors = tfidf.fit_transform(newsgroups_all.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to seperate the dataset into three sets: train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we seperate train from the rest. Random_state given to have a seed.\n",
    "newsgroups_train, newsgroups_inter, vect_train, vect_inter = \\\n",
    "    train_test_split(newsgroups_all.target, vectors, test_size=0.2, random_state=1)\n",
    "\n",
    "# then we seperate again to get validation and test seperately\n",
    "newsgroups_test, newsgroups_valid, vect_test, vect_valid = \\\n",
    "    train_test_split(newsgroups_inter, vect_inter, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train a random forest on our training set. For this, we will use the RandomForestClassifier, as it contains the parameters talked about in the exercise. But first, we need to ask ourselves what we want to set the parameters (*max_depth* and *n_estimators*) to.\n",
    "\n",
    "According to the ADA course, we know that the number of trees will be in the 100's and the depth will be betweem 20 to 30. Thus for the training set, we set *n_estimators* to 100 and *max_depth* to 25.\n",
    "\n",
    "For the predictions, we can't use the training set, as we just trained on it and thus would get very good results regardless. So prediction has to be on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find estimators and depth first. We use random_state to have a seed again.\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=25, random_state=1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(pred, newsgroups_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, predictions aren't that great.\n",
    "\n",
    "We try to fine tune on the validation set. We tried to use skikit's GridSearchCV function, but could not find the right parameters to make thefunction work. In fact, the best parameters were almost always the highest ones, yet the results of predictions stayed quite poor.\n",
    "\n",
    "Thus, we decided to use a double for loop. Even if this type of computation is very heavy and takes a lot more time, it seemed the easier.\n",
    "\n",
    "Please note that the fit takes a lot of time to compute, as there are a very large numbers of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DF to be able to use all this data effectively\n",
    "columns=['Depth', 'Estimators', 'Prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "# We made a few tries already to determine which values yield better results.\n",
    "# We have a double for loop. Which is aweful and will take a lot of time, but necessary.\n",
    "for i in range(30, 160, 10): #making sure the depth is not better above 30\n",
    "    for j in range(300, 1100, 100): #step of 100 to not take too much time\n",
    "        clf = RandomForestClassifier(n_estimators=j, max_depth=i)\n",
    "        clf.fit(vect_train, newsgroups_train)\n",
    "        pred = clf.predict(vect_valid)\n",
    "        pred_result = metrics.accuracy_score(newsgroups_valid, pred)\n",
    "        # uncomment next line to be able to know where computation is at\n",
    "        print(i,\" ; \", j)\n",
    "        rows.append([i, j, pred_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=400, max_depth=100, random_state=0)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.accuracy_score(newsgroups_valid, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200, max_depth=200, random_state=0)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.accuracy_score(newsgroups_valid, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.DataFrame(rows, columns=columns) # we get all the predictions in a dataframe\n",
    "# we sort by best prediction\n",
    "all_predictions = all_predictions.sort_values(by=['Prediction'], ascending=False)\n",
    "# we show the best 10\n",
    "all_predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the pickle. Do not run if we did not make a new pandas.\n",
    "# all_predictions.to_pickle(\"all_predictions.pkl\")\n",
    "# read the pickle.\n",
    "# all_predictions = pd.read_pickle(\"all_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 \t800 \t0.680106\n",
    "\n",
    "90 \t900 \t0.679576\n",
    "\n",
    "90 \t500 \t0.678515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, te best resuts are when *n_estimators* is set around 800 to 900 and *max_depth* is set around 47. Note that because we have a bit of randomness, the results are not deterministic.\n",
    "\n",
    "Now, we use thos numbers to create a random forest and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=800, max_depth=47, random_state=1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(pred, newsgroups_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf.predict(vect_valid)\n",
    "cm = metrics.confusion_matrix(newsgroups_test, pred_test)\n",
    "#normalize the matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to read this matrix, lets put it into a DataFrame and add the right column names, then show the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cm)\n",
    "df.columns = newsgroups_all.target_names\n",
    "df.index = newsgroups_all.target_names\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(df, cmap='OrRd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us inspect the `feature_importances_` attribute of our random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get names of features and their importance\n",
    "features = pd.DataFrame(list(zip(tfidf.get_feature_names(), clf.feature_importances_)))\n",
    "features = features.sort_values(by=[1], ascending=False)\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also have an idea of hom many features are important or not by plotting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clf.feature_importances_, bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Number of features (log)');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
