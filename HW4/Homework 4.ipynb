{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports (same as tuto ML)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder # is this really needed ?\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV, PredefinedSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preform a naive data analysis using plots and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the data set\n",
    "lalonde_df = pd.read_csv('lalonde.csv')\n",
    "#give a first look\n",
    "lalonde_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that a naive researcher unfamiliar with observational studies would treat the data as if it was a randomized trial, not taking into consideration the hidden correlates.\n",
    "\n",
    "We can easily imagine that the first thing he would do is split the salary (_['re78']_) data into 2 sets: treated and untreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masks to be used alot later\n",
    "treated = lalonde_df.treat == 1\n",
    "untreated = lalonde_df.treat == 0\n",
    "\n",
    "#apply masks to get treated and untreated\n",
    "treated_salary = lalonde_df[treated]['re78']\n",
    "untreated_salary = lalonde_df[untreated]['re78']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Visualizing the data:**\n",
    "\n",
    "We first plot the final salary data in a histogram to find the distribution of salaries between the two groups.\n",
    "We add weights so we can look at percentages instead of number of people in both bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "#define same bin size\n",
    "bins = np.linspace(0, max(lalonde_df['re78']), 50)\n",
    "#add weights to get percentages\n",
    "plt.hist(untreated_salary, weights=np.ones(len(untreated_salary))/len(untreated_salary), alpha=.5 , bins=bins)\n",
    "plt.hist(treated_salary, weights=np.ones(len(treated_salary))/len(treated_salary), alpha=.5, bins=bins)\n",
    "plt.title('Histogram showing salary treated and untreated groups')\n",
    "plt.legend(['untreated', 'treated'])\n",
    "plt.xlabel('Yearly salary')\n",
    "plt.ylabel('percentage of subjects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First insights:\n",
    "\n",
    "By looking at the graph, we see a very similar distribution for both functions, except that outliers are present in the treated group.\n",
    "\n",
    "Another very visible element is the fact that the function of the treated group's salaries is shifted to the bottom. Very quickly (cf paragraph below), a simple explanation arises: the number of members in the treated group is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Describing the numbers**\n",
    "\n",
    "Thus, we determined that he would only look at the basic descriptions of the data (mean, std and 5 number summary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lalonde_df.groupby('treat')['re78'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numbers above, we assume that the research would extract the following information from the data :\n",
    "\n",
    "- The untreated group has more people.\n",
    "- The untreated group's salaries are higher (higher mean).\n",
    "- However, the max salary in the treated group is 3x higher! The 1st quartile is also twice higher on the treated group.\n",
    "- Finally, we have that the second and third quartiles are higher in the first group. Quartiles are more resistent to outliers, so we should put more consideration\n",
    "- The interquartile distance is larger in the untreated set, as we have outliers in the set this is a better measure for 'variance'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Boxplot:**\n",
    "\n",
    "A boxplot will illustrate the above more consciely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([treated_salary, untreated_salary], labels=['treated', 'untreated'])\n",
    "plt.title('Distribution of salary by treated')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "By merging all of the insights the researcher has drawn from the 3 steps of his analysis, he can conclude that **the treatment is ineffective**. Even though salary distributions are similar in both cases, the treated group has in average a lower salary (and only a handful of rich people get lucky). This is shown by the boxplot: the whiskers extend higher in the untreated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing a **simplistic** analysis of the data ignoring underlying factors –such as race and education– that could influence the outcome. We start looking at the whole table, namely at the other features which surely have an impact on the variable we want to understand at the end of this exercise: _['re78']_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Interval data :**\n",
    "\n",
    "We start our analysis by visualizing the data to see if the two groups have different underlying distrubutions of factors. We split out analysis in by **categorical** and **intervall** data. In the beginning, we will focus on the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#defining list of non binary variables\n",
    "intervals = ['age', 'educ', 're74', 're75']\n",
    "\n",
    "#for each column draw a Boxplot\n",
    "for col in intervals:\n",
    "    plt.title(\"Boxplot of \" + col)\n",
    "    plt.boxplot([lalonde_df[untreated][col], lalonde_df[treated][col]], labels=['untreated', 'treated'])\n",
    "    plt.ylabel(col)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\NOT RIGHT REPLACE From the graphs above, we can see very clearly 2 elements. First, the data distribution is always similar between the 2 groups (even though the number of participants is different due to the difference in the number of people in both groups). On top of this, 3 types of distributions pop out:\n",
    "- Poisson: this is the distribution representing the **age** of participants <- rly??? that's not what I'm seeing\n",
    "- Gaussian : this distribution models the **level of education** of participants\n",
    "- Power law : this type of distribution is appropriate to understand the **salaries** of participants (_['re74'], ['re75'] and ['re78']_ all have the same form of distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Salaries:\n",
    "\n",
    "The main thing we need to look at is the salaries with respect to the education. Education likely influences the salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.barplot(y=\"re78\", x='educ', data=lalonde_df, hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Evolution:\n",
    "\n",
    "Even though it is useful to plot the salaries to see the difference between the years, it is much more useful to understand how the salary of each participant has changed over the years. To do so, we will visualize our data using a parallel plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement parallel plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "parallel_coordinates(lalonde_df[untreated][['id','re74', 're75', 're78']], 'id', color='Blue', alpha=0.5)\n",
    "parplot = parallel_coordinates(lalonde_df[treated][['id','re74', 're75', 're78']], 'id', color='Orange' , alpha=0.7)\n",
    "#remove legend for readability\n",
    "parplot.legend_.remove()\n",
    "plt.title('Salary over time for each participant')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Annualy Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- the treated group started out with a lower salary\n",
    "- 75 was a bad year for everybody, treated or untreated.\n",
    "- the outliers are people partialy people who were already well payed in 74, partialy people who 'made it'.\n",
    "- there is a lot of movement up for the treated group between 75 and 78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the interval data, and more specificaly at the salaries of participants, we can say that …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Categorical data :**\n",
    "\n",
    "Regarding categorical data, we should look at rates (makes much more sense than looking at just the numbers). Thus we define the rates for race, degree and mariage depending on each treatment to be able to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as the values are binary the mean is equal to the percentage of occurence\n",
    "percentages = lalonde_df.groupby('treat').mean()\n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Race ratios:\n",
    "\n",
    "We will start with race. As we do not have numbers for \"White\" participants, we get the number of \"Blacks\" and \"Hispanics\" for each treatment group and substract the total. We then compare the rates of each race using pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_u, black_t = percentages['black']\n",
    "hispan_u, hispan_t = percentages['hispan']\n",
    "#there is no overlap in the hispan and black categories, \n",
    "#we assume people that are neither are white (which we checked, it is the case)\n",
    "white_u, white_t = (1 - black_u - hispan_u, 1 - black_t - hispan_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_race_rates = [black_u, hispan_u, white_u]\n",
    "t_race_rates = [black_t, hispan_t, white_t]\n",
    "#give name to lable\n",
    "race_labels = 'Black', 'Hispanic', 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(u_race_rates, labels = race_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rates of races in the untreated group\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.pie(t_race_rates, labels = race_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rates of races in the treated group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are way more black subjects in the treatment group than in the untreated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Degree ratios:\n",
    "\n",
    "To have a better understanding of the difference of salaries, we also need to look at the level of education of the participants of each treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodegree_u, nodegree_t = percentages['nodegree']\n",
    "degree_u, degree_t = (1 - nodegree_u, 1 - nodegree_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate rate for degree havers in treated and untreated group\n",
    "u_degree_rates = [1 - nodegree_u, nodegree_u]\n",
    "t_degree_rates = [1 - degree_t, nodegree_t]\n",
    "degree_labels = 'Degree', 'No degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw pie diagram\n",
    "plt.pie(u_degree_rates, labels = degree_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rate of people with degrees in the untreated group\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.pie(t_degree_rates, labels = degree_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rate of people with degrees in the treated group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the treated group is less educated, by a difference of over 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Marriage ratios:\n",
    "\n",
    "Finally, we look at the rates of married people among both groups as it is our last feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#married and unmarried by treatment\n",
    "married_u, married_t = percentages['married']\n",
    "not_married_u, not_married_t = (1 - married_u, 1 - married_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mariage_labels = 'Married', 'Not married'\n",
    "#untreated group\n",
    "plt.pie([married_u, not_married_u], labels = mariage_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rate of people with degrees in the untreated group\")\n",
    "#treated group\n",
    "plt.figure()\n",
    "plt.pie([married_t, not_married_t], labels = mariage_labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Rate of people with degrees in the treated group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again note that the treated group contains less married individuals.\n",
    "Marriage can be an indicator of stability and thus indicate how likely somebody is to preform consistenly well on job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the categorical data, we can say that the underlying factors between the two groups are not similar.\n",
    "The treated group is significantly more black, less educated and married. All these factors influence employment and should be taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Correlated data :**\n",
    "\n",
    "After working on each value alone, we want to understand how each value is (linearly) linked to others on each pair of features. We look at the pairplot, as correlation by itself does not give us any insights, as clearly the data is not linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we can somewhat seperate the two groups, inicating that they ate not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(lalonde_df[['treat', 're78']+intervals], markers='+', hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A propsensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create a fair set to use on our observational study, we calculate the propensity score based on the underlying factors before treatment:\n",
    "\n",
    "[age, educ, hispan, black, nodegree, re74, re75]\n",
    "\n",
    "**note: we consider all these factors to have been recorded before treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table = lalonde_df.copy() #otherwise we modify lalonde_df when we modify prop_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our target and training data:\n",
    "\n",
    "X = prop_table.iloc[:, 2:-1] #rows age to 're75'\n",
    "y = prop_table.iloc[:, 1:2] #treated or not\n",
    "y = np.ravel(y) #flatten array\n",
    "print('First elements of Y : \\n', y[0:5],'\\nFirst elements of X\\n', X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define our model\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X, y)\n",
    "print('Accuracy of prediction: ',logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of prediction : \", logistic.predict(X[0:6]), ' reality :', y[0:6])\n",
    "print('Example of prediction in percent : \\n', logistic.predict_proba(X[0:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get propensity scores, probability of \"being a subject\"\n",
    "prop_table['propensity_scores'] = pd.Series(logistic.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the propensity scores to find a matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the two is an equivalent problem to find a matching in a bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "B = nx.Graph()\n",
    "#1. Creat graph with nodes as id\n",
    "B.add_nodes_from(prop_table['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Add edges from each treated to each untreated subject\n",
    "#    with weight on each node being the difference between the two\n",
    "for row_i in prop_table[treated].iterrows():\n",
    "    for row_j in prop_table[untreated].iterrows():\n",
    "        B.add_edge(row_i[1]['id'],row_j[1]['id'], \n",
    "                   #-x to transform minimisation problem into maximisation problem\n",
    "                   weight= 1 - np.abs(row_i[1].propensity_scores - row_j[1].propensity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Find matching\n",
    "matching_dict = nx.max_weight_matching(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example matches:')\n",
    "list(matching_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get matching\n",
    "remaning_subjects = prop_table.copy()[prop_table['id'].isin(matching_dict)]\n",
    "print('we have : ',len(remaning_subjects)/2, ' matched subjects') #pairs appear in 2 order s ab and ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate treated and untreated\n",
    "remaning_subjects.groupby('treat').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=remaning_subjects, x='treat', y='re78') # this one is similar! good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaning_subjects[remaning_subjects['black'] == 1].groupby('treat')['id'].count() #unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaning_subjects[remaning_subjects['hispan'] == 1].groupby('treat')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(prop_table[intervals+['treat']], markers='+', hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Balancing the groups further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are still way more black subjects in the treated group than in the untreated group.\n",
    "Additionaly, we still have outliers in the treated group.\n",
    "\n",
    "We try to balance the both groups by removing white subjects matched with outlying black subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaning_subjects['match'] = remaning_subjects['id'].map(matching_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaning_subjects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = remaning_subjects[remaning_subjects.treat == 1]\n",
    "right = remaning_subjects[remaning_subjects.treat == 0]\n",
    "matches = left.merge(right, left_on='id', right_on='match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches['difference'] = abs(matches['propensity_scores_x'] - matches['propensity_scores_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches that are black/white missmatched and have a large difference in propensity scores\n",
    "to_drop = matches[(matches['black_x'] == 1) & (matches['black_y'] == 0) & (matches['hispan_y'] == 0) & (matches.difference > .5)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matches = matches.drop(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matches.groupby('black_x')['id_x'].count() #treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matches.groupby('black_y')['id_y'].count() #untreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matches.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "nana = final_matches.drop(final_matches[final_matches.re78_x > 30000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([nana['re78_x'], nana['re78_y']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nana.re78_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nana.re78_y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After controlling for unerlying factors we see that the treated population fares better than the untreated population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the TF-IDF features of our dataset, using a vectorizer. As we understood the question, what was asked was not to use any of the given datasets from sklearn, but to use all of the data. Thus, we do not use the train and test subsets given to us in sklearn, but will create our own such subsets, adding a validation subset.\n",
    "\n",
    "Also note that we remove the headers, footers and quotes, as proposed in the <a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\">sklearn tutorial</a> of the dataset, as to have something more realistic and without any of the metadata. Note also that we did not use the *sklearn.datasets.fetch_20newsgroups_vectorized* function that returns the TF-IDF features directly, as it would defeat the purpose of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data we need to use the vectorizer on. Remove metadata as proposed by sci-kit tutorial\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked in the question, before seperating in subsets, we will use the vectorizer on the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors is a sparse matrix\n",
    "vectors = tfidf.fit_transform(newsgroups_all.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to seperate the dataset into three sets: train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we seperate train from the rest. Random_state given to have a seed.\n",
    "newsgroups_train, newsgroups_inter, vect_train, vect_inter = \\\n",
    "    train_test_split(newsgroups_all.target, vectors, test_size=0.2, random_state=1)\n",
    "\n",
    "# then we seperate again to get validation and test seperately\n",
    "newsgroups_test, newsgroups_valid, vect_test, vect_valid = \\\n",
    "    train_test_split(newsgroups_inter, vect_inter, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train a random forest on our training set. For this, we will use the RandomForestClassifier, as it contains the parameters talked about in the exercise. But first, we need to ask ourselves what we want to set the parameters (*max_depth* and *n_estimators*) to.\n",
    "\n",
    "According to the ADA course, we know that the number of trees will be in the 10's and the depth will be betweem 20 to 30. Thus for the training set, we set *n_estimators* to 10 and *max_depth* to 25.\n",
    "\n",
    "For the predictions, we can't use the training set, as we just trained on it and thus would get very good results regardless. So prediction has to be on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find estimators and depth first. We use random_state to have a seed again.\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=25, random_state=1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, predictions aren't that great.\n",
    "\n",
    "We try to fine tune on the validation set. Note that to do this, we usethe GridSearch implemented in sklearn. We first chose he estimators between 100 and 1000 and a depth between 20 and 30 as it is what we have seen during the lessons, but seeing as the results for the best parameters were the upper limit (30 and 1000) we decided to look if it would still be the same by taking a larger upper limit (35 and 1500).\n",
    "\n",
    "Also, as we have already our own training, validation and test sets, we need to use *PredefinedSplit* in the GridSearch.\n",
    "\n",
    "Please note that the fit takes a lot of time to compute, as there are a very large numbers of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [250, 500]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.fit(vect_valid, newsgroups_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=250, random_state=1, n_jobs=-1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, te best resuts are when *n_estimators* is set around X and *max_depth* is set to X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us inspect the `feature_importances_` attribute of our random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
