{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports (same as tuto ML)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder # is this really needed ?\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV, PredefinedSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define custom chart drawing functions we reuse a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we use this a lot :D\n",
    "def drawPies(u_rates, t_rates, labels, supertitle):\n",
    "    \"\"\"draws pretty comparative pie charts\n",
    "    u_rates : rates for untreated group\n",
    "    t_rates : rates for treated group\n",
    "    lables: labels for values in rates\n",
    "    supertitle: title of chart\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    fig.suptitle(supertitle)\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.pie(u_rates, labels = labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"untreated group\")\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.pie(t_rates, labels = labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"treated group\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preform a naive data analysis using plots and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the data set\n",
    "lalonde_df = pd.read_csv('lalonde.csv')\n",
    "#give a first look\n",
    "lalonde_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that a naive researcher unfamiliar with observational studies would treat the data as if it was a randomized trial, not taking into consideration the hidden correlates.\n",
    "\n",
    "We can easily imagine that the first thing he would do is split the salary (_['re78']_) data into 2 sets: treated and untreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#masks to be used alot later\n",
    "treated = (lambda x: x.treat == 1)\n",
    "untreated = (lambda x: x.treat == 0)\n",
    "\n",
    "#apply masks to get treated and untreated\n",
    "treated_salary = lalonde_df[treated(lalonde_df)]['re78']\n",
    "untreated_salary = lalonde_df[untreated(lalonde_df)]['re78']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Describing the numbers**\n",
    "\n",
    "We first look at the numbers to see how many subjects in each group we have an how the values are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lalonde_df.groupby('treat')['re78'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numbers above, we extract the following information from the data :\n",
    "\n",
    "- The untreated group has more people.\n",
    "- The untreated group's salaries have a higher mean.\n",
    "- However, the max salary in the treated group is 3x higher! The 1st quartile is also two times higher on the treated group.\n",
    "- Finally, we have that the second and third quartiles are higher in the untreated. Quartiles are more resistent to outliers, so we should put more consideration into these values\n",
    "- The interquartile distance is larger in the untreated set, as we have outliers in the set this is a better measure for 'variance'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Visualizing the data:**\n",
    "\n",
    "We now plot the final salary data in a histogram to find the distribution of salaries of the two groups. We add weights so we can look at percentages instead of number of people in both bins, as the number of people in the two groups is not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "#define same bin size\n",
    "bins = np.linspace(0, max(lalonde_df['re78']), 50)\n",
    "#add weights to get percentages\n",
    "plt.hist(untreated_salary, weights=np.ones(len(untreated_salary))/len(untreated_salary), alpha=.5 , bins=bins)\n",
    "plt.hist(treated_salary, weights=np.ones(len(treated_salary))/len(treated_salary), alpha=.5, bins=bins)\n",
    "plt.title('Histogram showing salary treated and untreated groups')\n",
    "plt.legend(['untreated', 'treated'])\n",
    "plt.xlabel('Yearly salary')\n",
    "plt.ylabel('percentage of subjects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First insights:\n",
    "\n",
    "By looking at the graph, we see a very similar distribution for both functions, except that outliers are present in the treated group.\n",
    "\n",
    "We also note that there are more relatively more subjects in the untreated group with a salary between 10k and 20k, while both groups have a similar ratio of subjects in the <10k section of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Boxplot:**\n",
    "\n",
    "A boxplot will illustrate the above more consciely the 5 number summary we presented first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([treated_salary, untreated_salary], labels=['treated', 'untreated'])\n",
    "plt.title('Distribution of salary by treated')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "If the treatment was effective, we should see that the treated group is more sucessful on average, as they were placed in program, wherease the untreated group was left to fend for themselves.\n",
    "\n",
    "By merging all of the insights the researcher has drawn from the 3 steps of his analysis, he can conclude that **the treatment shows no effect**. The salary distributions are similar in both cases, indicating that the treatment isn't effective.\n",
    "\n",
    "Additionaly, the treated group has in average a lower salary (except for the handful of people get lucky and find a good job). This is shown by the boxplot: the whiskers extend higher in the untreated group and the median and lower wisker as situated higher up. Since the difference is small this may just be due to chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing a **simplistic** analysis of the data ignoring underlying factors –such as race and education– that could influence the outcome, we start looking at the whole table assuming the other features will impact _['re78']_.\n",
    "\n",
    "We split out analysis in by **categorical** and **intervall** data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i - Categorical data :**\n",
    "\n",
    "Regarding categorical data, we should look at rates (makes much more sense than looking at just the numbers). Thus we define the rates for race, degree and mariage depending on each treatment to be able to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#as the values are binary the mean is equal to the percentage of occurence\n",
    "percentages = lalonde_df.groupby('treat').mean()\n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Race ratios:\n",
    "\n",
    "We will start with race. As we do not have numbers for \"White\" participants, we get the number of \"Blacks\" and \"Hispanics\" for each treatment group and substract the total. We then compare the rates of each race using pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "black_u, black_t = percentages['black']\n",
    "hispan_u, hispan_t = percentages['hispan']\n",
    "#there is no overlap in the hispan and black categories, \n",
    "#we assume people that are neither are white (which we checked, it is the case)\n",
    "white_u, white_t = (1 - black_u - hispan_u, 1 - black_t - hispan_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_race_rates = [black_u, hispan_u, white_u]\n",
    "t_race_rates = [black_t, hispan_t, white_t]\n",
    "#give name to lable\n",
    "race_labels = 'Black', 'Hispanic', 'White'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawPies(u_race_rates, t_race_rates, race_labels, 'Racial groups in percent by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are way more black subjects in the treatment group than in the untreated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Degree ratios:\n",
    "\n",
    "To have a better understanding of the difference of salaries, we also need to look at the level of education of the participants of each treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree_u, degree_t = percentages['nodegree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate rate for degree havers in treated and untreated group\n",
    "u_degree_rates = [degree_u, 1 - degree_u]\n",
    "t_degree_rates = [degree_t, 1 - degree_t]\n",
    "degree_labels = 'Degree', 'No degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#draw pie diagram\n",
    "drawPies(u_degree_rates, t_degree_rates, degree_labels, 'Percentage of individuals with a degree, by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the treated group is less educated, by a difference of over 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Marriage ratios:\n",
    "Finally, we look at the rates of married people among both groups as it is our last feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#married and unmarried by treatment\n",
    "married_u, married_t = percentages['married']\n",
    "not_married_u, not_married_t = (1 - married_u, 1 - married_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_marriage_rates = [married_u, not_married_u]\n",
    "t_marriage_rates = [married_t, not_married_t]\n",
    "mariage_labels = ['Married', 'Not married']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawPies(u_marriage_rates, t_marriage_rates, mariage_labels, 'Percentage of married individuals by treatment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again note that the treated group contains less married individuals.\n",
    "Marriage can be an indicator of stability and thus indicate how likely somebody is to preform consistenly well on job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Unemployment ratios:\n",
    "\n",
    "Even though salaries are not categories but intervals, it is important to compare unemployment rates between both groups (which we define at categories, employed and unemployed). To get better insights, we will plot the years 1974 and 1975."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining list of non binary variables\n",
    "salaries = ['re74', 're75']\n",
    "cat_salaries = lalonde_df.copy()\n",
    "cat_salaries[salaries].apply(lambda x: 1 if any(x == 0) else 2)\n",
    "unemployed_labels = 'Employed', 'Unemployed'\n",
    "#for each column draw a Boxplot\n",
    "for sal in salaries:\n",
    "    cat_salaries[sal] = cat_salaries[sal].map(lambda x : 0 if x == 0 else 1)\n",
    "    u_employed, t_employed = cat_salaries.groupby('treat')[sal].mean()\n",
    "    drawPies([1-u_employed, u_employed],[1-t_employed, t_employed], unemployed_labels, 'Unemployment rates by treatment in 19'+sal[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our assumtion that both groups are balanced pre-treatment is wrong for unemplyment aswell.\n",
    "There are much more unemployed people in the treated group than in the untreated group. Who already has a job will be able to move up the ladder more easely, skewing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the categorical data, we can say that the underlying factors between the two groups are not similar at all.\n",
    "The treated group is significantly more black, less educated, less employed and less married. All these factors influence employment and should be taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii - Interval data :**\n",
    "\n",
    "we look at non binary data and the their distribution.\n",
    "\n",
    "\n",
    "To do this we first do a box plot and relative fequency histogram for the intervall variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#defining list of non binary variables\n",
    "intervals = ['age', 'educ', 're74', 're75']\n",
    "\n",
    "#for each column draw a Boxplot\n",
    "for col in intervals:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    treated_ = lalonde_df[treated(lalonde_df)][col]\n",
    "    untreated_ = lalonde_df[untreated(lalonde_df)][col]\n",
    "    \n",
    "    #boxplot\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Boxplot of \" + col)\n",
    "    plt.boxplot([untreated_, treated_], \n",
    "                labels=['untreated', 'treated'])\n",
    "    plt.ylabel(col)\n",
    "    \n",
    "    #histogram\n",
    "    plt.subplot(2,2,2)\n",
    "    bins = np.linspace(min(lalonde_df[col]), max(lalonde_df[col]), 50)\n",
    "    plt.title(\"Relative frequency histogram of \" + col)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.xlabel(col)\n",
    "    plt.hist(untreated_, weights=np.ones(len(untreated_))/len(untreated_), alpha=.5 , bins=bins)\n",
    "    plt.hist(treated_, weights=np.ones(len(treated_))/len(treated_), alpha=.5, bins=bins)\n",
    "    plt.legend(['untreated', 'treated'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that pre treatment, the salaries are very unbalanced, the treatment group earning much less than the untreated group.\n",
    "\n",
    "We can also observe a different age distribution in the two groups, the treated group being a bit younger, containg a lot of individuals in their 20's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Evolution:\n",
    "\n",
    "Even though it is useful to plot the salaries to see the difference between the years, it is much more useful to understand how the salary of each participant has changed over the years. To do so, we will visualize our data using a parallel plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Implement parallel plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "parallel_coordinates(lalonde_df[untreated(lalonde_df)][['id','re74', 're75', 're78']], 'id', color='Blue', alpha=0.5)\n",
    "parplot = parallel_coordinates(lalonde_df[treated(lalonde_df)][['id','re74', 're75', 're78']], 'id', color='Orange' , alpha=0.7)\n",
    "#remove legend for readability\n",
    "parplot.legend_.remove()\n",
    "plt.title('Salary over time for each participant')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Annualy Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- the treated group started out with a lower salary\n",
    "- 75 was a bad year for everybody, treated or untreated.\n",
    "- the outliers are people partialy people who were already well payed in 74, partialy people who 'made it'.\n",
    "- there is a lot of movement up for the treated group between 75 and 78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Salary by race, education and marital status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By race:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining mask for white\n",
    "white = (lalonde_df['black'] == 0) & (lalonde_df['hispan'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years = ['re74', 're75', 're78']\n",
    "for year in years:\n",
    "    plt.boxplot([lalonde_df[white][year], lalonde_df[lalonde_df['black'] == 1][year],\n",
    "                 lalonde_df[lalonde_df['hispan'] == 1][year]], \n",
    "                labels=['white', 'black', 'hispanic'])\n",
    "    plt.title('salary distribution by race in 19'+year[-2:])\n",
    "    plt.ylabel('annual earnings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \n",
    "- there is a racial discrepancy in salary in our dataset\n",
    "- the outliers are all black individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By martial status and degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    #for every year we plot marriage and degree\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('unmarried vs married salary in 19'+year[-2:])\n",
    "    sns.boxplot(data=lalonde_df, x='married', y=year, hue='treat')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('degree vs nodegree salary in 19'+year[-2:])\n",
    "    sns.boxplot(data=lalonde_df, x='nodegree', y=year, hue='treat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    sns.factorplot(data=lalonde_df, x='age', y=year, hue='treat',aspect=4, size=3)\n",
    "    plt.title('salary by age and treatment in 19'+year[-2:])\n",
    "    plt.yticks(np.linspace(0, 40000, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- its easier to find a job when your young, young people easely catch up over the years.\n",
    "- untreated +40 are very unemployed\n",
    "- treatment group catches up, the treatment seems to have an effect\n",
    "- 75 bad year, low salary in general (look at y axis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "By looking at the interval data, and more specificaly at the salaries of participants, we can say that the underlying conditions such as race, education marital status influence the salary.\n",
    "As our two groups are not balanced, this interferes with out analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii - Correlation data :**\n",
    "\n",
    "After working on each value alone, we want to understand how each value is (linearly) linked to others on each pair of features. We look at the pairplot, as correlation by itself does not give us any insights, as clearly the data is not linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we can somewhat seperate the two groups, inicating that they ate not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(lalonde_df[['treat', 're78']+intervals], markers='+', hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A propsensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create a fair set to use on our observational study, we calculate the propensity score based on the underlying factors before treatment:\n",
    "\n",
    "[age, educ, hispan, black, nodegree, re74, re75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_table = lalonde_df.copy() #otherwise we modify lalonde_df when we modify prop_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create our target and training data:\n",
    "\n",
    "X = prop_table.iloc[:, 2:-1] #rows age to 're75'\n",
    "y = prop_table.iloc[:, 1:2] #treated or not\n",
    "y = np.ravel(y) #flatten array\n",
    "print('First elements of Y : \\n', y[0:5],'\\nFirst elements of X\\n', X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define our model\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X, y)\n",
    "print('Accuracy of prediction: ',logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Example of prediction : \", logistic.predict(X[0:6]), ' reality :', y[0:6])\n",
    "print('Example of prediction in percent : \\n', logistic.predict_proba(X[0:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get propensity scores, probability of \"being a subject\"\n",
    "prop_table['propensity_scores'] = pd.Series(logistic.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the propensity scores to find a matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the two is an equivalent problem to find a matching in a bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "B = nx.Graph()\n",
    "#1. Creat graph with nodes as id\n",
    "B.add_nodes_from(prop_table['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Add edges from each treated to each untreated subject\n",
    "#    with weight on each node being the difference between the two\n",
    "for row_i in prop_table[treated(prop_table)].iterrows():\n",
    "    for row_j in prop_table[untreated(prop_table)].iterrows():\n",
    "        B.add_edge(row_i[1]['id'],row_j[1]['id'], \n",
    "                   #-x to transform minimisation problem into maximisation problem\n",
    "                   weight= 1 - np.abs(row_i[1].propensity_scores - row_j[1].propensity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Find matching\n",
    "matching_dict = nx.max_weight_matching(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Example matches:')\n",
    "list(matching_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get matching\n",
    "matched = prop_table.copy()[prop_table['id'].isin(matching_dict)]\n",
    "print('we have : ',len(matched)/2, ' matched subjects') #pairs appear in 2 order s ab and ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#separate treated and untreated\n",
    "matched.groupby('treat').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=matched, x='treat', y='re78') # this one is similar! good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matched[matched['black'] == 1].groupby('treat')['id'].count() #unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matched[matched['hispan'] == 1].groupby('treat')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(prop_table[intervals+['treat']], markers='+', hue='treat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Balancing the groups further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_groups(table) :\n",
    "    columns = ['untreated', 'treated']\n",
    "    index = ['age', 'educ', 'black', 'hisp', 'married', 'no_degree']\n",
    "    result = pd.DataFrame(columns=columns, index=index)\n",
    "    result['untreated']['age'] = table['age_y'].mean()\n",
    "    result['untreated']['educ'] = table['educ_y'].mean()\n",
    "    result['untreated']['black'] = table['black_y'].mean()\n",
    "    result['untreated']['hisp'] = table['hispan_y'].mean()\n",
    "    result['untreated']['married'] = table['married_y'].mean()\n",
    "    result['untreated']['no_degree'] = table['nodegree_y'].mean()\n",
    "    result['treated']['age'] = table['age_x'].mean()\n",
    "    result['treated']['educ'] = table['educ_x'].mean()\n",
    "    result['treated']['black'] = table['black_x'].mean()\n",
    "    result['treated']['hisp'] = table['hispan_x'].mean()\n",
    "    result['treated']['married'] = table['married_x'].mean()\n",
    "    result['treated']['no_degree'] = table['nodegree_x'].mean()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are still way more black subjects in the treated group than in the untreated group.\n",
    "Additionaly, we still have outliers in the treated group.\n",
    "\n",
    "We try to balance the both groups by removing white subjects matched with outlying black subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matched['match'] = matched['id'].map(matching_dict)\n",
    "balanced_match = matched[treated(matched)].merge(matched[untreated(matched)], left_on='id', right_on='match')\n",
    "balanced_match['difference'] = abs(balanced_match['propensity_scores_x'] - balanced_match['propensity_scores_y'])\n",
    "print('we have : ', len(balanced_match), ' matched subjects')\n",
    "balanced_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matches that are black/white missmatched and have a large difference in propensity scores\n",
    "race_bool_1 = (balanced_match['black_x'] == 1) & (balanced_match['black_y'] == 0) & (balanced_match['hispan_y'] == 0)\n",
    "race_bool_2 = (balanced_match['black_x'] == 1) & (balanced_match['black_y'] == 0) & (balanced_match['hispan_y'] == 1)\n",
    "balanced_match = balanced_match.drop(balanced_match[race_bool_1 | race_bool_2].index)\n",
    "print('we have : ', len(balanced_match), ' matched subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_u = balanced_match.groupby('black_y')['id_y'].count()\n",
    "u_race_rates = (race_u[0]/race_u.sum(), race_u[1]/race_u.sum())\n",
    "race_t = balanced_match.groupby('black_x')['id_x'].count()\n",
    "t_race_rates = (race_t[0]/race_t.sum(), race_t[1]/race_t.sum())\n",
    "race_labels = ('black', 'other race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_groups(balanced_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matches that are black/white missmatched and have a large difference in propensity scores\n",
    "marriage_bool_1 = (balanced_match['married_x'] == 1) & (balanced_match['married_y'] == 0)\n",
    "marriage_bool_2 = (balanced_match['married_y'] == 1) & (balanced_match['married_x'] == 0)\n",
    "balanced_match = balanced_match.drop(balanced_match[marriage_bool_1 | marriage_bool_2].index)\n",
    "print('we have : ', len(balanced_match), ' matched subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marriage_u = balanced_match.groupby('married_y')['id_y'].count()\n",
    "u_marriage_rates = (marriage_u[0]/marriage_u.sum(), marriage_u[1]/marriage_u.sum())\n",
    "marriage_t = balanced_match.groupby('married_x')['id_x'].count()\n",
    "t_marriage_rates = (marriage_t[0]/marriage_t.sum(), marriage_t[1]/marriage_t.sum())\n",
    "marriage_labels = ('married', 'not married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_groups(balanced_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "balanced_match = balanced_match.drop(balanced_match[balanced_match.re78_x > 30000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_groups(balanced_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([balanced_match['re78_x'], balanced_match['re78_y']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = pd.concat([balanced_match.re78_y.describe(),balanced_match.re78_x.describe()], axis=1)\n",
    "stats.columns = ['untreated', 'treated']\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After controlling for unerlying factors we see that the treated population fares better than the untreated population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the TF-IDF features of our dataset, using a vectorizer. As we understood the question, what was asked was not to use any of the given datasets from sklearn, but to use all of the data. Thus, we do not use the train and test subsets given to us in sklearn, but will create our own such subsets, adding a validation subset.\n",
    "\n",
    "Also note that we remove the headers, footers and quotes, as proposed in the <a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\">sklearn tutorial</a> of the dataset, as to have something more realistic and without any of the metadata. Note also that we did not use the *sklearn.datasets.fetch_20newsgroups_vectorized* function that returns the TF-IDF features directly, as it would defeat the purpose of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data we need to use the vectorizer on. Remove metadata as proposed by sci-kit tutorial\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked in the question, before seperating in subsets, we will use the vectorizer on the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectors is a sparse matrix\n",
    "vectors = tfidf.fit_transform(newsgroups_all.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to seperate the dataset into three sets: train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we seperate train from the rest. Random_state given to have a seed.\n",
    "newsgroups_train, newsgroups_inter, vect_train, vect_inter = \\\n",
    "    train_test_split(newsgroups_all.target, vectors, test_size=0.2, random_state=1)\n",
    "\n",
    "# then we seperate again to get validation and test seperately\n",
    "newsgroups_test, newsgroups_valid, vect_test, vect_valid = \\\n",
    "    train_test_split(newsgroups_inter, vect_inter, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train a random forest on our training set. For this, we will use the RandomForestClassifier, as it contains the parameters talked about in the exercise. But first, we need to ask ourselves what we want to set the parameters (*max_depth* and *n_estimators*) to.\n",
    "\n",
    "According to the ADA course, we know that the number of trees will be in the 10's and the depth will be betweem 20 to 30. Thus for the training set, we set *n_estimators* to 10 and *max_depth* to 25.\n",
    "\n",
    "For the predictions, we can't use the training set, as we just trained on it and thus would get very good results regardless. So prediction has to be on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to find estimators and depth first. We use random_state to have a seed again.\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=25, random_state=1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, predictions aren't that great.\n",
    "\n",
    "We try to fine tune on the validation set. Note that to do this, we usethe GridSearch implemented in sklearn. We first chose he estimators between 100 and 1000 and a depth between 20 and 30 as it is what we have seen during the lessons, but seeing as the results for the best parameters were the upper limit (30 and 1000) we decided to look if it would still be the same by taking a larger upper limit (35 and 1500).\n",
    "\n",
    "Also, as we have already our own training, validation and test sets, we need to use *PredefinedSplit* in the GridSearch.\n",
    "\n",
    "Please note that the fit takes a lot of time to compute, as there are a very large numbers of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [250, 500]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_rfc.fit(vect_valid, newsgroups_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=250, random_state=1, n_jobs=-1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, te best resuts are when *n_estimators* is set around X and *max_depth* is set to X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us inspect the `feature_importances_` attribute of our random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
