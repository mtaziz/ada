{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports (same as tuto ML)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder # is this really needed ?\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV, PredefinedSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preform a naive data analysis using plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data set\n",
    "lalonde_df = pd.read_csv('lalonde.csv')\n",
    "#give a first look\n",
    "lalonde_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at numbers**\n",
    "\n",
    "We use a simple summary to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_df.groupby('treat')['re78'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that there are many less people in the treated group compared to the untreated group.\n",
    "- The standard deviation for the treated group is much higher, there is more variety in the outcome\n",
    "- The mean of the untreated group is higher\n",
    "- but the max salary in the treated group is 3x higher!\n",
    " - given that the 75% for the untreated group is much higher, this could indicate that the max for the treated group is an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can visualize these numbers nicely with a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the two categories\n",
    "treated_salary = lalonde_df[lalonde_df.treat ==1 ]['re78']\n",
    "untreated_salary = lalonde_df[lalonde_df.treat == 0 ]['re78']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([treated_salary, untreated_salary], labels=['treated', 'untreated'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at plots**\n",
    "\n",
    "we naively (like us at first) try to use a histogram we will see that taking into account that there are much less people in the treated group, both groups fare similarly.\n",
    "\n",
    "We note that we have a log-like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw the plots on top of each other with same bin size\n",
    "bins = np.linspace(0, 60500, 30)\n",
    "plt.hist(untreated_salary, bins, alpha=0.5, label='untreated')\n",
    "plt.hist(treated_salary, bins, alpha=0.5, label='treated')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A propsensity score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Balancing the groups further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the TF-IDF features of our dataset, using a vectorizer. As we understood the question, what was asked was not to use any of the given datasets from sklearn, but to use all of the data. Thus, we do not use the train and test subsets given to us in sklearn, but will create our own such subsets, adding a validation subset.\n",
    "\n",
    "Also note that we remove the headers, footers and quotes, as proposed in the <a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\">sklearn tutorial</a> of the dataset, as to have something more realistic and without any of the metadata. Note also that we did not use the *sklearn.datasets.fetch_20newsgroups_vectorized* function that returns the TF-IDF features directly, as it would defeat the purpose of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data we need to use the vectorizer on. Remove metadata as proposed by sci-kit tutorial\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As asked in the question, before seperating in subsets, we will use the vectorizer on the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectors is a sparse matrix\n",
    "vectors = tfidf.fit_transform(newsgroups_all.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to seperate the dataset into three sets: train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we seperate train from the rest. Random_state given to have a seed.\n",
    "newsgroups_train, newsgroups_inter, vect_train, vect_inter = \\\n",
    "    train_test_split(newsgroups_all.target, vectors, test_size=0.2, random_state=1)\n",
    "\n",
    "# then we seperate again to get validation and test seperately\n",
    "newsgroups_test, newsgroups_valid, vect_test, vect_valid = \\\n",
    "    train_test_split(newsgroups_inter, vect_inter, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train a random forest on our training set. For this, we will use the RandomForestClassifier, as it contains the parameters talked about in the exercise. But first, we need to ask ourselves what we want to set the parameters (*max_depth* and *n_estimators*) to.\n",
    "\n",
    "According to the ADA course, we know that the number of trees will be in the 10's and the depth will be betweem 20 to 30. Thus for the training set, we set *n_estimators* to 10 and *max_depth* to 25.\n",
    "\n",
    "For the predictions, we can't use the training set, as we just trained on it and thus would get very good results regardless. So prediction has to be on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find estimators and depth first. We use random_state to have a seed again.\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=25, random_state=1)\n",
    "clf.fit(vect_train, newsgroups_train)\n",
    "pred = clf.predict(vect_valid)\n",
    "metrics.f1_score(newsgroups_valid, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, predictions aren't that great.\n",
    "\n",
    "We try to fine tune on the validation set. Note that to do this, we usethe GridSearch implemented in sklearn. We first chose he estimators between 100 and 1000 and a depth between 20 and 30 as it is what we have seen during the lessons, but seeing as the results for the best parameters were the upper limit (30 and 1000) we decided to look if it would still be the same by taking a larger upper limit (35 and 1500).\n",
    "\n",
    "Also, as we have already our own training, validation and test sets, we need to use *PredefinedSplit* in the GridSearch.\n",
    "\n",
    "Please note that the fit takes a lot of time to compute, as there are a very large numbers of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 1500],\n",
    "    'max_depth': [20, 35]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.fit(vect_train, newsgroups_train)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, te best resuts are when *n_estimators* is set around X and *max_depth* is set to X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us inspect the `feature_importances_` attribute of our random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
