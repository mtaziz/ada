{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "We assume that all exploration has been done before, thus we take out unnecessary fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    example = json.load(data_file)\n",
    "cleaned = json_normalize(example)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparatory Steps :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning :\n",
    "\n",
    "Looking at all the columns we have, only 10 are interesting to keep :\n",
    "- main: this is the tweet in itself\n",
    "- published: this is the time when the tweet was published\n",
    "- source_spam_probability: necessary to get out of useless tweets\n",
    "- source_location: this is the place where the tweet was published\n",
    "- tags: this is useful as # give us concise information\n",
    "- lang: allows us to choose the languages we will work with\n",
    "- sentiment: allows us to choose our subset\n",
    "- author_gender: can help us get more insight\n",
    "- source_followers: can help us get more insight\n",
    "- source_following: can help us get more insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only keeping the necessary columns\n",
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by only keeping the languages we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_mask = (cleaned['lang'] != 'de') & (cleaned['lang'] != 'fr') & (cleaned['lang'] != 'en')\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to drop all tweets that have a 'POSITIVE' emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop all tweets that have at least 50% of probability of being spams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing :\n",
    "\n",
    "Some data is not proper to be used. To do this, we format the data following multiple steps, starting with the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will focus on the main element of our analysis, the tweets. We start by putting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "cleaned['main'] = cleaned['main'].astype(str).str.lower().\\\n",
    "                    apply(lambda tweet: unicodedata.normalize('NFD', tweet).\\\n",
    "                    encode('ascii', 'ignore').decode('utf-8'))\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take out urls and non alphanumerical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(\"www\\S+\", '').str.replace(\"http\\S+\", '').\\\n",
    "                    str.replace(\"pic.twitter\\S+\", '').str.replace('[^\\w\\s]', '')\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is removing stopwords and stemming (getting only the racial of the word) using an NLP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def process_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].str.split().\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set]).\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_words('english')\n",
    "process_words('french')\n",
    "process_words('german')\n",
    "cleaned['main'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
