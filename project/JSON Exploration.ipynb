{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of example set to me more exploratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we preform an exploration of the partial Spinn3r dataset provided.\n",
    "\n",
    "We first find how to explore the data using one sampel json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = pd.read_json(EXAMPLE_PATH)\n",
    "example.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the data we are interessted in is contained in the _source variable, in object format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object format is a nested json that we could not find how to extract dirrectly using the read json funtion provided. We thus use a json normaliter contained in the pandas libary to extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set max dispaly columns to a larger number in able to see all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 58)\n",
    "example1 = json_normalize(example._source[0])\n",
    "print(example1.columns)\n",
    "print(len(example1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the spinner documentation we see that:\n",
    "\n",
    "The values we are most interessted in:\n",
    " - main: content of tweet\n",
    " - sentiment: overal sentiment of tweet\n",
    " - author_gender: gender of author\n",
    " - source_location: to know where was posted\n",
    " - lang: to get the language of the tweet\n",
    " - source_created: to know when it was made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us the the first things we need to be careful about:\n",
    " - what does the sentiment realy say about the tweet?\n",
    " - is the location accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, after some trying around we find that the inbuilt read_json function was not ideal for or pourposed. We thus open the file using 'open' and transform it into a dataframe using json_normalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXAMPLE_PATH\n",
    "         ) as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how data looks like, for first tweet\n",
    "data[0]['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nicer naming\n",
    "twitter_df.columns = [ column.replace('_source.','') for column in twitter_df.columns]\n",
    "print(twitter_df.columns)\n",
    "print(twitter_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_freq = twitter_df['lang'].value_counts()\n",
    "language_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_freq = twitter_df['author_gender'].value_counts()\n",
    "gender_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_freq = twitter_df['sentiment'].value_counts()\n",
    "sentiment_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = twitter_df[twitter_df['sentiment'] == 'NEGATIVE']['main'] \n",
    "negative_tweets.map(lambda x: ':(' in x).value_counts() ##so it's just the smileys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_df[twitter_df['sentiment'] == 'POSITIVE']['main'] \n",
    "positive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets.map(lambda x: (':-)' in x) or (':)' in x) or (';)' in x)).value_counts() #also just smileys basicaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(twitter_df.main_length, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that the location names depend on the language of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_location'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also dirrectly see that often, the location is not very accurate, giving us only 'switzerland'.\n",
    "This will not be enough for our analysis as we want to see if we can find a difference between the röstigraben.\n",
    "But we could still work with this by using language as an inicator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_spam_probability'].value_counts() #none of them are suspected to be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['tags'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a münchen in there -> munich -> this indicated that geolocation may be off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['published'] = pd.to_datetime(twitter_df['published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['published'].map(lambda x: (x.day, x.month)).value_counts() #all published on same day, same month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_user_interactions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df['source_following'].value_counts() just to see, not that usefull as values, make graph maybe if useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df['source_favorites'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df['source_followers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_likes'].value_counts() #no facebook likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.drop(twitter_df['lang'] == 'und')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.iloc[-11:, 3:]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't forget to remove all links\n",
    "check for swiss german! (some do get into dutch but not that many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all languages we don't care about\n",
    "mask = (twitter_df['lang'] != 'de') & (twitter_df['lang'] != 'fr') & (twitter_df['lang'] != 'en')\n",
    "twitter_df.drop(twitter_df[mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['sentiment'] == 'NEGATIVE']['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therap' in x) ]['main'] ##issue of adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'suic' in x) ]['main'] #issue of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.iloc[2105, :]['main'] #example depression tweet that was not recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.iloc[14146, :]['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.iloc[11539, :]['main'] #again, neutral when this is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.iloc[1128, :]['main'] #we should remove if news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying it to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import regexp_replace, col, lower, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('swiss-tweet/')\n",
    "# Displays the content of the DataFrame to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes # we only care about the inner nested types in ._source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('_source.*') #remove nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns we care about\n",
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.lang == 'en') | (df.lang == 'fr') | (df.lang == 'de')\n",
    "df = df.filter(mask)  #gets tweets with right language\n",
    "df = df.filter(df.sentiment != 'POSITIVE') #remove positive sentient tweets\n",
    "df = df.filter(df.source_spam_probability < 0.5) #remove spam\n",
    "df = df.withColumn('main', lower(df.main)) #get lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('main', regexp_replace(col('main'), 'pic.twitter\\S+', ' ')) #remove picture urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(~(df.main.contains('http'))).filter(~(df.main.contains('.com'))).filter(~(df.main.contains('.ch'))).filter(~(df.main.contains('www')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.main.isin('lol'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-e82d8dd0f2e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1020\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "df.select('main').map(dict_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(main='en espérant que 2016 soit meilleur que 2015 quand même'),\n",
       " Row(main='nice bmw ...he kills ant s!!!  '),\n",
       " Row(main='@madmenna ich hab nichts dagegen, wenn mir andere beim saufen zuschauen, solange sie pro minute 5.- zahlen. @deinwoelfchen'),\n",
       " Row(main='@ietsist dank je. heel veel lieve nieuwjaarswensen terug aan jullie allemaal. net terug na een leuk feest in belgië zonder internet.'),\n",
       " Row(main='happy new jear!!!! i am back from 2 weeks cuba! let s go into the 2016!  '),\n",
       " Row(main='@megadriver16 bonne année à toi je prends beaucoup de plaisir à suivre tes vidéos je suis un grand fande tour de france merci continue ainsi'),\n",
       " Row(main='@sweaggygirl oh la la'),\n",
       " Row(main=\"putain j'ai maaaal au crane la\"),\n",
       " Row(main=\"@claudiocolu alla fine gn simm #b'ccat ️\"),\n",
       " Row(main='so, das ist ein kleiner teil von bangkok  '),\n",
       " Row(main=\"gister was de trein reis van belgie naar nederland wel echt de allergrootste ramp ooit. :')\"),\n",
       " Row(main='beauty is around  '),\n",
       " Row(main='neujahrsmorgen, zehn uhr und ich habe bereits den küchenboden gewischt. was ist mit mir passiert?'),\n",
       " Row(main=\"@miss_r i got a bit blindsided, too ^_^'''\"),\n",
       " Row(main=\"tellement que j'ai une équipe banaf sur fut, quand je fais le draft c'est trop fluide pour moi j'arrive pas à jouer\")]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('main').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json('reduced_tweets') #write dataframe to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.json('reduced_tweets2') #write just one file instead of multiple! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.toPandas() #turns it into pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    3755\n",
       "fr    3026\n",
       "de    1950\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.lang.value_counts() #not that many german/germanic tweets, lots of spanish etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_filter(word):\n",
    "    found = False\n",
    "    for w in ['lol', 'depri', 'na']:\n",
    "        if( w in word):\n",
    "            found = True\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = df.select(regexp_replace(col('main'), 'http\\S+|www.\\S+', ' ').alias('filtered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = f.select(regexp_replace(col('filtered'), '.*[\\r\\n]', ' ').alias('filtered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = df.select('_source.main').withColumn(\"main\", regexp_replace('main', \"http\\S+|www.\\S+\", \" \"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
