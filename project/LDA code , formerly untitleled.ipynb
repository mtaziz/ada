{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('reduced_tweets.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary data processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different languages have different pattern but no difference betweet mental illness related tweets and unrelated tweets can be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this might be a dumb idea in retrospect. we need to keep èä to use stop word removal effectivley\n",
    "# df['main'] = df['main'].astype(str).str.lower().\\\n",
    "#                     apply(lambda tweet: unicodedata.normalize('NFD', tweet).\\\n",
    "#                     encode('ascii', 'ignore').decode('utf-8'))\n",
    "# df['main'] = df['main'].str.replace(r'[^\\w\\s]', '')\n",
    "# df['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we should use the tweets tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tknzr.tokenize('mir gahts so so lala lol!! merd:X :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'mir gahts so so lala lol!! :D'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['main'].map(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized.to_pickle('tokenized_strings.pkl')\n",
    "#save time and read from this pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['split'] = df['main'].str.split()\n",
    "df['tokenized'] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    lang_set = stopwords.words(language)\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stops('english', df, 'tokenized')\n",
    "remove_stops('french', df, 'tokenized')\n",
    "remove_stops('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_words('english', df, 'tokenized')\n",
    "stem_words('french', df, 'tokenized')\n",
    "stem_words('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokenized.to_pickle('tokenized_stemmed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_PATH = \"dictionary.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = pd.concat([dictionaries['german'].dropna(), dictionaries['swiss_german'].dropna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_cleaning(lang):\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_dict = lang_dict.astype(str).str.lower().\\\n",
    "                        apply(lambda expression: unicodedata.normalize('NFD', expression).\\\n",
    "                        encode('ascii', 'ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cleaning('en')\n",
    "dict_cleaning('fr')\n",
    "dict_cleaning('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "en_dict = en_dict.str.split()\n",
    "fr_dict = fr_dict.str.split()\n",
    "de_dict = de_dict.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    lang_dict = lang_dict.apply(lambda expression: [word for word in expression if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "dict_remove_stops('english')\n",
    "dict_remove_stops('french')\n",
    "dict_remove_stops('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    lang_dict = lang_dict.apply(lambda expression: [stemmer.stem(word) for word in expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the words\n",
    "dict_stem_words('english')\n",
    "dict_stem_words('french')\n",
    "dict_stem_words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merged'] = df.tokenized.map(lambda x:  ' '.join(x))\n",
    "de_dict = de_dict.map(lambda x:  ' '.join(x))\n",
    "en_dict = en_dict.map(lambda x:  ' '.join(x))\n",
    "fr_dict = fr_dict.map(lambda x:  ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatching(tweet_list, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict(tweet, dict_):\n",
    "    \"\"\"checks if dict entry matches tweet\"\"\"\n",
    "    match = [ w for w in dict_ if w in tweet] #find matching for each entry\n",
    "    return len(match) > 0 #at least one match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered = df[df.lang == 'en'][df[df.lang == 'en']['merged'].map(lambda x: check_dict(x, en_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered.main.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered = df[df.lang == 'fr'][df[df.lang == 'fr']['tokenized'].map(lambda x: check_dict(x, fr_dict))]\n",
    "french_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(french_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered = df[df.lang == 'de'][df[df.lang == 'de']['tokenized'].map(lambda x: check_dict(x, de_dict))]\n",
    "german_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(german_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english_filtered.to_csv('english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english_filtered.sample(n=1000).to_csv('english_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#french_filtered.sample(n=1000).to_csv('french_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move this above\n",
    "import re\n",
    "s = \"string. With. Punctuation? ! àéè. : ; \"\n",
    "s = re.sub(r'[^\\w\\s]','',s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered_2 = english_filtered.tokenized.map(lambda x: [re.sub(r'[^\\w\\s]','',s) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(english_filtered_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=3, no_above=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = english_filtered_2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.LdaMulticore(corpus, id2word=dictionary, num_topics=3, workers=3, iterations=100, passes=5) #takes like 5minutes on leo's pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel =  models.LdaModel.load('lda.model') #retrive lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_to_cluster = list()\n",
    "for n,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        cluster = max(ldamodel[doc],key=lambda x:x[1])\n",
    "        sent_to_cluster.append(cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel.save('lda.model') #save lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for french etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered_2 = french_filtered.tokenized.map(lambda x: [re.sub(r'[^\\w\\s]','',s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dictionary = corpora.Dictionary(french_filtered_2)\n",
    "fr_dictionary.filter_extremes(no_below=5, no_above=.3)\n",
    "fr_texts = french_filtered_2.tolist()\n",
    "fr_corpus = [fr_dictionary.doc2bow(text) for text in fr_texts]\n",
    "\n",
    "ldamodel_2 = models.LdaMulticore(fr_corpus, id2word=fr_dictionary, num_topics=10, workers=3, iterations=100, passes=5)\n",
    "\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel_2, fr_corpus, fr_dictionary)\n",
    "\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for german:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered_2 = german_filtered.tokenized.map(lambda x: [re.sub(r'[^\\w\\s]','',s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dictionary = corpora.Dictionary(german_filtered_2)\n",
    "ger_dictionary.filter_extremes(no_below=3, no_above=.5)\n",
    "ger_texts = german_filtered_2.tolist()\n",
    "ger_corpus = [ger_dictionary.doc2bow(text) for text in ger_texts]\n",
    "\n",
    "ldamodel_2 = models.LdaMulticore(ger_corpus, id2word=ger_dictionary, num_topics=5, workers=3, iterations=100, passes=5)\n",
    "\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel_2, ger_corpus, ger_dictionary)\n",
    "\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
