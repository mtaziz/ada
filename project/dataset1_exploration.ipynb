{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analysis for Dataset 1\n",
    "\n",
    "This notebook contains an initial analysis of the swiss-tweets dataset that was provided on the cluster.\n",
    "\n",
    "We first preform the analysis on the sample set using pandas and then show how to use spark to scale up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields contained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the schema to see what fields are contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pd.read_table('twitter-swisscom/schema.txt', header=None, delim_whitespace=True, index_col=0,\n",
    "                       names=['name', 'type', 'specification', '??', 'format'] )\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have no exact information of what exactly each column contains, we can infer it form the column names.\n",
    "\n",
    "For out analysis, the following columns are potentialy useful:\n",
    "- userId: to see if some users occur frequently\n",
    "- createdAt: to see when the tweet was created, so we can look into seasonal changes\n",
    "- text: the actual tweet\n",
    "- longitude & latitude / placeLongitude & placeLatitude: giving us the exact location of the tweet\n",
    "- followersCount & friendCounts: to see how sociable or integrated in twitter a user is\n",
    "- userLocation: to give us the location of the user\n",
    "\n",
    "\n",
    "We see that we have no way of dirrectly seeing the language of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a look at the sample tsv provided, containing a smaller subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after previous reads we saw that \\N was used as NA value\n",
    "#tsv -> use \\t as separator\n",
    "#use schema names for column name\n",
    "twitter_df = pd.read_table('twitter-swisscom/sample.tsv', \n",
    "              sep='\\t', engine='c', encoding='utf-8', quoting=csv.QUOTE_NONE,\n",
    "              header=None, names=schema.name, na_values='\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly see the following things:\n",
    "\n",
    "- longitude/latitude are often nan, this is note the case for the place_ equivalent\n",
    "- userLocation contains places outside of Switzerland\n",
    "- there are a lot of different languages present in the dataset\n",
    "\n",
    "We look at bit more closely at the nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- longtiude/latitude contains a lot of nan values\n",
    "- this is not the case for placeLatitude/placeLongitude, but still 10% nan\n",
    "- we have Nans in every column, we would have to remove these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As natural language processing methods are at the heart of our project, knowing which language is used in a tweet is essential.\n",
    "\n",
    "We thus tried to find a way to detect language in a tweet. The langdetect library provides this functionality by making calls to the google translate api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try to apply this to the sample set and get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try to get the distribution of the languages in the tweets\n",
    "twitter_df['text'].map(detect).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that some tweets contain no language or language usable for the classification. We thus have to first remove the tweets containing these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df['text'] = twitter_df['text'].astype(str)\n",
    "twitter_df.text.dropna() #remove NaNs\n",
    "twitter_df.text = twitter_df.text.str.replace('http\\S+|www.\\S+', '', case=False)#remove website\n",
    "twitter_df.text = twitter_df.text.str.replace('@\\S+|via', '', case=False)#remove @ and via\n",
    "twitter_df.text = twitter_df.text.str.replace('\\((.+?)\\)', '', case=False)#remove content in ()\n",
    "twitter_df.text = twitter_df.text.str.replace('([^\\s\\w]|_)+','', case=False)#remove non alphanumeric (needed for language dec)\n",
    "twitter_df = twitter_df[twitter_df.text.map(lambda x: len(x)) > 0 ] #remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df.text = twitter_df.text[twitter_df.text.map(lambda x:any(c.isalpha() for c in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['text'].map(detect).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of the overall distribution of languages in the dataset.\n",
    "We see that the majority of tweets is either english, french or german.\n",
    "\n",
    "We note that there is a small issue with the accuracy of detection, sometimes swiss german gets categorized as something other than german:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect('wie gahts dir') #How are you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this sentence is categorized as afrikaans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moving to spark\n",
    "\n",
    "We also looked into how we could read this in spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local', 'pyspark')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv')\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .options(header='false')\n",
    "    .load('twitter-swisscom/sample.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative dataset was later povided we did not preform any more tasks on spark, as we will be using the other dataset provided."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
