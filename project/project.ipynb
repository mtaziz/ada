{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "@oth: Describe getting the dataset from the cluster..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. dataset selection & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first analysis is to familiarize ourselves with the dataset inorder the know if we need to adapt our research question or enrich the dataset with external information in order to preform our analysis.\n",
    "\n",
    "We were provided two separate datasets containing swiss tweets. They were formated differently and contained different fields and while one was over the duration of multiple years the other only covers a span of 10 months.\n",
    "We performed an analysis of both in order to be able to decide which one should be used in out project.\n",
    "After the analysis both sets we decided to use **dataset 2** for our project.\n",
    "\n",
    "While dataset 1 containes more precise location information in the form of longitude and latitude, dataset 2 contains a sentiment analysis field as well as a language field.\n",
    "\n",
    "As trying to categorize the language of each tweet in dataset 1 was quite computationaly expensive –having to deal with network latency of api requests– and a lot of preprocessing was necessary to get it to work, dataset 2 containing this field puts it at a clear advantage.\n",
    "\n",
    "We now provide a quick overview of dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 1 (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of what each column in the *tsv file* containing the tweets represents. A sample file was given, but we optained the complete set of tweets (5gb) via a .zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following usefull columns:\n",
    "\n",
    "- userId : id identifying user\n",
    "- createdAt : time of posting tweet\n",
    "- text : content of tweet\n",
    "- placeLatitude : latitude of tweet\n",
    "- placeLongitude : longitude of tweet\n",
    "- sourceName : username\n",
    "- sourceUrl : url of tweet\n",
    "- followersCount : number of followers\n",
    "- friendsCount : number of mutuals\n",
    "- statusesCount : number of statuses of user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sample dataset contains a lot of nan values, and each column contains at least 1% or more nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set 2 (from Spinner)\n",
    "\n",
    "This dataset has a elaborate description of each field available at the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema).\n",
    "Unlike the previous dataset, this dataset is given in json format.\n",
    "\n",
    "To deal with the amount of data present in the cluster we look at one day to perform our first analysis and then show how to scale up.\n",
    "\n",
    "The format of this dataset is a nested json that we could not find how to extract dirrectly using the read json funtion provided. We thus use a json normalizer contained in the pandas libary to extract it. We will later see that spark deals better with nested json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [ column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these columns, the one we can use are:\n",
    "- main: contains the content of the tweet\n",
    "- published: gives the time on which the content was posted\n",
    "- source_spam_probability: probability of tweet being spam\n",
    "- source_location: location of tweet\n",
    "- tags: tags associated with tweet, as provided by spinn3r\n",
    "- lang: language of tweet\n",
    "- sentiment: sentiment score of tweet -POSITIVE, NEGATIVE, NEUTRAL-\n",
    "- author_gender: gender of author -MALE, FEMALE, UNKNOWN-\n",
    "- source_followers: followers of user who tweeted\n",
    "- source_following: number of mutual followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at general distributions in this dataset. While this example isn't representative when it comes to the tweets –especially given it contains tweets of the 1th of january– it can still give us insights on the other fields.\n",
    "\n",
    "We assume that roughtly the same categories of users were active on that day, so we can draw conclusions on the distribution of language, gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language distribution is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that english, french and german are most frequent. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the distribution of gender in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['author_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not seem to contain this information.  But there are still a lot that do, so we could use the ones that do to look at differences between gender, although it would not give use an unbiased set, as the type of user declaring their gender on twitter may be different than those who chose not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the sentiment column, to see how the tweets were labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of tweets was labeled as neutral, and only a very small number are labeled ad negative. We will this look at both neutral and negatively labeled tweets.\n",
    "Under the assumption that the positives are not false positives, a tweet showing signs of mental distress will not be labeled as POSITIVE, hence we can safely exclude these tweets from further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probabality we see that not a single tweet was labeled as spam. This puts into question the accuracy of the labeling, as the set of tweets on that day most certainly contains spam. We will still use it, as we assume the chanse of false positives is is low, so we lose nothing by using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['source_spam_probability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine the locations provided by the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we only look at the locations for the languages we care about, as location seems to be language dependent\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- there are a lot of locations that are the same but in a different language, such as Switzerland and Schweiz\n",
    "- the names of the locations are not just in the languages we are interessted in (see สวิตเซอร์แลนด์)\n",
    "- a vast majority of the dataset is just labeled as 'switzerland'\n",
    "- but as opposed to dataset 1, they are all located in switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count() #give us number of NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking at the tweets\n",
    "\n",
    "while this set of tweets is not representative we can still use it to find potential issues we might have with the tweet content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly see that the tweets containing links are not relevant to our research question, as they are mostly news or adds. We make the assumption that this would be the case anytime of the year.\n",
    "\n",
    "We look at the tweets containing links and confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now preform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suic' in x) ]['main'] #news instead of personal reference\n",
    "#removing nres would be good\n",
    "#we also see that we should remove pic.twit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main'] #adds instead of personal reference\n",
    "#all contain links..reason to remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. datset cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain that we began with pandas (local proof of concept) and scaled up using spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 unnesting the json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 column selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 language filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 spam removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 time format encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 text treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lowercase, normalize (unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url removal, RT removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 dictionary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. labeling the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 construcing features TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 train SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 relabel training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. final data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to find similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://github.com/master/spark-stemming preforming stemming with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03 example dataprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] https://spark.apache.org/docs/2.1.0/ml-features.html ml methods we can use with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ best ways to do text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] https://www.rand.org/content/dam/rand/pubs/rgs_dissertations/RGSD300/RGSD391/RAND_RGSD391.pdf dissertation containing dict 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2880/3264 public health paper, LDA usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] https://docs.google.com/spreadsheets/d/1WwI9crZk36pcTOQ1g_5dumMd11OlkpFRNHsEvpkwLMk/edit?usp=sharing our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] https://getd.libs.uga.edu/pdfs/kale_sayali_s_201512_ms.pdf second thesis containing dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
