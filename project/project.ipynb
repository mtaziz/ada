{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "@oth: Describe getting the dataset from the cluster..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. dataset selection & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first analysis is to familiarize ourselves with the dataset inorder the know if we need to adapt our research question or enrich the dataset with external information in order to preform our analysis.\n",
    "\n",
    "We were provided two separate datasets containing swiss tweets. They were formated differently and contained different fields and while one was over the duration of multiple years the other only covers a span of 10 months.\n",
    "We performed an analysis of both in order to be able to decide which one should be used in out project.\n",
    "After the analysis both sets we decided to use **dataset 2** for our project.\n",
    "\n",
    "While dataset 1 containes more precise location information in the form of longitude and latitude, dataset 2 contains a sentiment analysis field as well as a language field.\n",
    "\n",
    "As trying to categorize the language of each tweet in dataset 1 was quite computationaly expensive –having to deal with network latency of api requests– and a lot of preprocessing was necessary to get it to work, dataset 2 containing this field puts it at a clear advantage.\n",
    "\n",
    "We now provide a quick overview of dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 1 (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of what each column in the *tsv file* containing the tweets represents. A sample file was given, but we optained the complete set of tweets (5gb) via a .zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following usefull columns:\n",
    "\n",
    "- userId : id identifying user\n",
    "- createdAt : time of posting tweet\n",
    "- text : content of tweet\n",
    "- placeLatitude : latitude of tweet\n",
    "- placeLongitude : longitude of tweet\n",
    "- sourceName : username\n",
    "- sourceUrl : url of tweet\n",
    "- followersCount : number of followers\n",
    "- friendsCount : number of mutuals\n",
    "- statusesCount : number of statuses of user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sample dataset contains a lot of nan values, and each column contains at least 1% or more nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set 2 (from Spinner)\n",
    "\n",
    "This dataset has a elaborate description of each field available at the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema).\n",
    "Unlike the previous dataset, this dataset is given in json format.\n",
    "\n",
    "To deal with the amount of data present in the cluster we look at one day to perform our first analysis and then show how to scale up.\n",
    "\n",
    "The format of this dataset is a nested json that we could not find how to extract dirrectly using the read json funtion provided. We thus use a json normalizer contained in the pandas libary to extract it. We will later see that spark deals better with nested json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [ column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these columns, the one we can use are:\n",
    "- main: contains the content of the tweet\n",
    "- published: gives the time on which the content was posted\n",
    "- source_spam_probability: probability of tweet being spam\n",
    "- source_location: location of tweet\n",
    "- tags: tags associated with tweet, as provided by spinn3r\n",
    "- lang: language of tweet\n",
    "- sentiment: sentiment score of tweet -POSITIVE, NEGATIVE, NEUTRAL-\n",
    "- author_gender: gender of author -MALE, FEMALE, UNKNOWN-\n",
    "- source_followers: followers of user who tweeted\n",
    "- source_following: number of mutual followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at general distributions in this dataset. While this example isn't representative when it comes to the tweets –especially given it contains tweets of the 1th of january– it can still give us insights on the other fields.\n",
    "\n",
    "We assume that roughtly the same categories of users were active on that day, so we can draw conclusions on the distribution of language, gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language distribution is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that english, french and german are most frequent. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the distribution of gender in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df['author_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not seem to contain this information.  But there are still a lot that do, so we could use the ones that do to look at differences between gender, although it would not give use an unbiased set, as the type of user declaring their gender on twitter may be different than those who chose not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the sentiment column, to see how the tweets were labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of tweets was labeled as neutral, and only a very small number are labeled ad negative. We will this look at both neutral and negatively labeled tweets.\n",
    "Under the assumption that the positives are not false positives, a tweet showing signs of mental distress will not be labeled as POSITIVE, hence we can safely exclude these tweets from further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probabality we see that not a single tweet was labeled as spam. This puts into question the accuracy of the labeling, as the set of tweets on that day most certainly contains spam. We will still use it, as we assume the chanse of false positives is is low, so we lose nothing by using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df['source_spam_probability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine the locations provided by the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we only look at the locations for the languages we care about, as location seems to be language dependent\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- there are a lot of locations that are the same but in a different language, such as Switzerland and Schweiz\n",
    "- the names of the locations are not just in the languages we are interessted in (see สวิตเซอร์แลนด์)\n",
    "- a vast majority of the dataset is just labeled as 'switzerland'\n",
    "- but as opposed to dataset 1, they are all located in switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df.count() #give us number of NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking at the tweets\n",
    "\n",
    "while this set of tweets is not representative we can still use it to find potential issues we might have with the tweet content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly see that the tweets containing links are not relevant to our research question, as they are mostly news or adds. We make the assumption that this would be the case anytime of the year.\n",
    "\n",
    "We look at the tweets containing links and confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now preform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main'] #news instead of personal reference\n",
    "#removing nres would be good\n",
    "#we also see that we should not remove pic.twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a lot of these tweets contains news, we should remove the links in order to get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at therapy (in german), again this confirms the issues with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main'] #adds instead of personal reference\n",
    "#all contain links..reason to remove links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we look at the tweets that are retweets, to get an idea if they could be useful.\n",
    "From what we see, and what others [8] have seen they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this analysis we apply the gained knowledge and start to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all of the data exploring, we have to clean the data to be able to use it correctly for the rest of the project.\n",
    "\n",
    "First, we use Pandas on a small subset of the dataset and locally, so as to be able to make sure all functions work as expected and to test them out. Later on, we use spark to be able to do the same operations on a bigger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unnesting the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is for us to have an unnested JSON, which basically means having a Pandas DataFrame, thus having the correct columns and the data for each of them. To do so,m we simply use the **json_normalize** function in Pandas. We also choose to take out all \"_source\" in the strings to have clearer and smaller column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    example = json.load(data_file)\n",
    "cleaned = json_normalize(example)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a Dataframe, we choose the columns we think will be useful for the rest of the proejct. Thus, we found only ten columns that seem interesting to us.\n",
    "\n",
    "Those ten are:\n",
    "\n",
    "- **main**: contains the tweet (text) itself.\n",
    "- **published**: time when the tweet was published.\n",
    "- **source_spam_probability**: permits to determine if tweet is spam or not.\n",
    "- **source_location**:  place where the tweet was published.\n",
    "- **tags**: hashtags given in the tweet.\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment analysis of the tweet.\n",
    "- **author_gender**: gender of the author.\n",
    "- **source_followers**: number of followers.\n",
    "- **source_following**: number of people they are following.\n",
    "\n",
    "Some of them will bring us interesting insights., For example: are men or women more prone to depression? Is someone with more followers happy? There are quite a few interesting questions that can thus be asked and later on answered with the information we choose to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only keeping the necessary columns\n",
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Language Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as our project is mainly focused on Switzerland, we will make sure we only keep the tweets that are in languages spoken by the majority of Switzerland, which would mean French, German and Italian. Unfortunatley, as no one in the group speaks Italian, we decided to forfeit the language in favor of English. This has two reasons: first, a lot of people speak English on the Internet, as it is a global language; second, there is a large English-speaking community in Switzerland.\n",
    "\n",
    "Noite also that Swiss German is counted in the German part, even if some of the words are a little bit more peculiar. This will be taken into account in our dictionnaries later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_mask = (cleaned['lang'] != 'de') & (cleaned['lang'] != 'fr') & (cleaned['lang'] != 'en')\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as we were provided with a dataset that has a sentiment analysis, we use it to be able to drop all tweets that have a **positive** result. Seeing as we want to find depression and other mental illnesses, we are mostly searching for negative or neutral tweets. \n",
    "\n",
    "It could be that the sentiment analysis does have a few false positives (for example, the use of a smiling face could be used saracastically and thus making the result of the analysis be positive), but we decide to trust the data that was given and not double-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Spam Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam is not relevant to any of our objectives, thus, we decide to drop all the tweets having a greater value than 0.5 in the **source_spam_probability** column. Once again, we decide to trust the algorithm giving us this data.\n",
    "\n",
    "We chose a 50% threshold as it follows the maximum likelihood rule: it is likely to be spam if it has more than 50% chance to be spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time Format Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the time in some analysis (to find seasonal patterns in depression for example), we also need the dates to all be properly formatted. Thus, we decided to reformat all the dates in the column **published** to be certain to be able to use them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Text Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the tweets that interest us remaining, we need to do a little bit of cleaning in the **main** column. Indeed, to be able to process the text and find patterns or any other information that we could seek.\n",
    "\n",
    "First we put everything in lowercase to be able to make comapraisons between words without having problems with different representation between the same letter in uppercase and lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].astype(str).str.lower().\\\n",
    "                    apply(lambda tweet: unicodedata.normalize('NFD', tweet).\\\n",
    "                    encode('ascii', 'ignore').decode('utf-8'))\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove all URLs and images. As we do not have any image processing, the pictures are not useful. As for the links, some information might be relevant (for example the title of an article if it is to be found in the URL) but most of it isn't.\n",
    "\n",
    "We also remove all non-alphanumeric characters, as they are will not give more information and might get in the way of the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(\"www\\S+\", '').str.replace(\"http\\S+\", '').\\\n",
    "                    str.replace(\"pic.twitter\\S+\", '').str.replace('[^\\w\\s]', '')\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we will use usual functions for text treatment. Thus we the **NLP library** to be able to remove stopwords (*the*, *and*, ... in English for example) from a given language. We also use the stemmer, which gives us the root of any word (ending would become end, beginning would become begin) thus making it easier to make comparaisons with the dictionnaries we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].str.split().\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set]).\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_words('english')\n",
    "process_words('french')\n",
    "process_words('german')\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 dictionary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. labeling the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 construcing features TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 train SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 relabel training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. final data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to find similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://github.com/master/spark-stemming preforming stemming with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03 example dataprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] https://spark.apache.org/docs/2.1.0/ml-features.html ml methods we can use with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ best ways to do text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] https://www.rand.org/content/dam/rand/pubs/rgs_dissertations/RGSD300/RGSD391/RAND_RGSD391.pdf dissertation containing dict 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2880/3264 public health paper, LDA usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] https://docs.google.com/spreadsheets/d/1WwI9crZk36pcTOQ1g_5dumMd11OlkpFRNHsEvpkwLMk/edit?usp=sharing our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] https://getd.libs.uga.edu/pdfs/kale_sayali_s_201512_ms.pdf second thesis containing dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
