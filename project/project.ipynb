{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we describe the multiple steps of our preprocessing pipeline and the work done on the twitter dataset to answer our research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "0. **Data Retrival:** how we access the dataset\n",
    "1. **Dataset Selection and Analysis:** a first look at the dataset, exploring potential issues\n",
    "2. **Dataset Cleaning:** cleaning based on the results found in _Part 1_\n",
    "3. **NLP methods:** applying NLP methods to our data to retrieve relevant data\n",
    "4. **Machine Learning:** using Machine Learning to further clean our dataset\n",
    "5. **Analysis:** trying to find meaning in the final dataset we retrieve\n",
    "6. **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Datasets Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not retrieve both datasets using the same method first because they did not have the same weight, but also because they were not provided on the same platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset (_**twitter-swisscom**_) was retrieved from a .zip file. Thus, we had access to the entire dataset quickly allowing us to have an overview of all the tweets when analyzing it (cf _Part 1_ below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset (_**Spinn3r**_), however, was retrieved from the cluster. We first tried to retrieve the whole dataset but quickly realized it would be impossible to do so (it was very heavy and took a long time to be downloaded). Thus, we only extracted the first JSON file to perform our analysis. To do this, we used the following methods:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets. They were formated differently and contained different fields. On top of this, while the first dataset contained data collectd over multiple years, the other only covers a span of 10 months.\n",
    "\n",
    "The goal of our first analysis was thus to decide which dataset would be more useful but also to familiarize ourselves with the chosen dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "After the analysis (presented below), we decided to use **dataset 2** for our project. While dataset 1 contains more precise location information (longitude and latitude), dataset 2 contains a sentiment analysis field as well as a language field. As trying to categorize the language of each tweet in the first dataset was quite expensive in terms of computation – we needed to deal with the network latency of our API requests – and a lot of preprocessing was necessary to get it to work, dataset 2 was clearly better suited for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 (from Spinn3r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata analysis\n",
    "\n",
    "This dataset has an elaborate description of each field available on the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema). Given the amount of data present in the cluster, we only look at one day to perform our first analysis (we will later show how we scale our operations using Spark).\n",
    "\n",
    "Unlike the previous dataset, this dataset is given in JSON format (with nested elemtns). As we could not find how to extract all the data directly using the *read_json* function provided, we use a JSON normalizer (provided in the *Pandas* libary). \n",
    "\n",
    "_Note: We will later see that Spark deals better with nested JSON._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "TRUE_PATH = 'reduced_tweets.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these columns, we find the ones below to be the most useful for our analysis:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of the tweet.\n",
    "- **tags**: tags associated with the tweet (provided by Spinn3r).\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of the user who tweeted.\n",
    "- **source_following**: number of people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider these tags to be the most useful as:\n",
    "\n",
    "- **Main** stands at the center of our analysis. We plan to perform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "- **Published** can be used to map the tweets over the duration of the year and look for seasonal changes.\n",
    "- **Source_location** can be used to look at the geographical distribution of the tweets.\n",
    "- **Lang** will be used to filter out unwanted languages, which we need to do in order to perform our NLP tasks.\n",
    "- **Author_gender** will be used to identify the gender and look at the differences between both genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Distribution\n",
    "\n",
    "While this example isn't representative (especially given that it contains the tweets posted on January, 1st), it can still give us insights on other fields. We assume that roughly the same categories of users were active on that day, meaning we can draw conclusions on the distribution of language and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 20])\n",
    "explode = (0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, \n",
    "           0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "twitter_df['lang'].value_counts().plot(kind='pie', explode=explode)\n",
    "plt.legend(labels=twitter_df['lang'])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that English, French and German are the most frequent languages. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['author_gender'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, we have a significant number of profiles where the gender is documented meaning we could use these to look at the differences in mental distress between the genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "explode = (0, 0.2, 0.4)\n",
    "twitter_df['sentiment'].value_counts().plot(kind='pie', explode=explode)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that the vast majority of tweets are labeled as neutral while only a very small number is labeled as negative. Given our subject will look at both the **'NEUTRAL'** and **'NEGATIVE'** tweets.\n",
    "\n",
    "_Note: we make the assumption that there are no false positives, meaning that a tweet showing signs of mental distress will not be labeled as **'POSITIVE'**. This means we can safely exclude these tweets from further analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "twitter_df['source_spam_probability'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probability of the tweet set, we see that not a single tweet was labeled as spam. This makes us question the accuracy of the labeling as the set of tweets on that day most certainly contains spam. However, we will still use it as we assume the chance of having false positives to be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10])\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts().plot(kind='pie')\n",
    "plt.legend(labels=twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the location seems to be language dependent, we only care about tweets written in the 3 languages we need. We see that:\n",
    "- A lot of locations only differ in language (e.g. as Switzerland and Schweiz)\n",
    "- The name of the locations are not always given languages we are interested in (e.g. สวิตเซอร์แลนด์)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- As opposed to dataset 1, all tweets are located in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values in the dataset, we see that for most tweets, the tags are missing. This may indicate that the tags won't be usefull for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information. As we will filter out such tweets, the remaining set should contain sentiment. Even if doesn't, this field is not central to our analysis, it is merely used to help us filter our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6, 5])\n",
    "twitter_df[~(twitter_df.sentiment.isin(['POSITIVE', 'NEGATIVE', 'NEUTRAL']))]['lang'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure this won't be an issue, we quickly look at all the uncategorized tweets and see that the language is unkown as well, meaning they will all be removed anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential issues\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we immediatly see that the tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more intently at the tweets containing links, we can make the assumption that the relationship between URLs and spam is a general rule (at any time of the year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Preform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing a simplistic dictionary matching using the occurences of the word *'suicide'* in our tweet set, we see that a lot of these tweets contain news. This further comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word *'therapy'* (in German), we can confirm once again the issue there is with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to get an idea on how useful they could be. From what we see, and what others have seen [8], they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this analysis, we can apply this gained knowledge to start cleaning our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean our dataset to be able to use it correctly for our research.\n",
    "\n",
    "First, we use Pandas locally on a small subset of the tweets to explore different cleaning methods and make sure the functions we chose work as expected. After proving that our concept works, we use Spark to scale up our operations and be able to perform them on the 320 files provided on the cluster.\n",
    "\n",
    "_Note: The Spark version of this code can be found in the [run.py](run.py) file._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unnesting the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in _Part 1_, the provided dataset uses a nested json format. Thus, to be able to work with it, we need to unnest it using the _normalize_ function.\n",
    "\n",
    "_Note: we normalize the **data** DF as we use it in **Part 1** to import our datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = json_normalize(data)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a functional Dataframe, we choose the useful columns determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned in _Part 1_, we chose this columns for the possible insights they can give us. Some of the question we want to answer in our research are:\n",
    "- _Are men or women more prone to depression?_ \n",
    "- _Is someone with more followers happy?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Language Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided contains tweets in a lot of languages. However, as our project is focused on the Swiss Population, we only keep tweets written in one of the official Swiss languages (i.e. French, German and Italian).\n",
    "\n",
    "However, we had 2 hurdles (pushing us to do slight changes):\n",
    "- Nobody in the group is Italian-Speaking\n",
    "- Most of the tweets are written in English _(this is due to the large English-Speaking community in Switzerland but also due to the fact English is a _de-facto_ choice when communicating through the Internet)_\n",
    "\n",
    "Moreover, we should not forget the existence of Swiss-German dialects which also complicates the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work our way through these problems, we decided to keep 3 languages: English, French and German. Note that we mean \"German\" in an extended manner as we also include Swiss-German dialects in the lot (this will be taken into account when creating our dictionaries as some words slightly change between the two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_mask = ~cleaned.lang.isin(['de', 'en', 'fr'])\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use a dataset with sentiment labeling, we use this opportunity to drop all tweets marked as **POSITIVE** given the nature of our subject (we are looking for signs of mental distress). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this labeling is largely based on emojis, we expect some false positives in the **POSITIVE** category. However, it is too difficult to consider the sarcastic use of the _:)_ emoji (text analysis is not refined enough yet to detect such subtleties). Thus, we knowingly choose this method as we are only interested in having an overview of tweets showcasing signs of mental illnesses and not listing all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Spam Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam is not relevant to any of our objectives. Thus, we decide to drop all tweets having a probability of being a spam greater value than 0.5 (given in the **source_spam_probability** field). Once again, we decide to trust the system that treated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: our choice to have a 50% threshold follows the maximum likelihood rule and was considered to be a sufficiently robust value._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time Format Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the time in our analysis (especially to find seasonal patterns in depression), we need the dates to be properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Text Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have relevant tweets (following a mild cleaning), we need to work on the main subject of our analysis: the content of the tweets itself. This is necessary to ease the _Part 3_ of our research allowing us to process the text and find patterns: Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our treatment is to transform all the caracters to lowercase to be able to compare tweets easily without being bothered with case-sensitive searches. Following the same idea, we get rid of all the special characters complicating our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cleaning = lambda data: unicodedata.normalize('NFD', data.lower()).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].map(data_cleaning)\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking to pictures). As we image processing is not in the scope of this project, we remove all \"pic.twitter\" URLS. After that, we take out all tweets containing a URL format. We also remove retweets, as they would "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_mask = cleaned['main'].str.contains(\"www\\S+\") | cleaned['main'].str.contains(\"http\\S+\")\n",
    "cleaned['main'] = cleaned['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "cleaned.drop(cleaned[url_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove mentions to remove the usernames, which are meanling less and should not be considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(\"@\\S+\", '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we remove all non-alphanumeric characters, as they are will not give more information and might get in the way of the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(r'[^\\w\\s]', '')\n",
    "cleaned.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing all these cleaning steps, the size of the set is significantly reduced (the new file only weights 2.4MB instead of the 55MB of the original file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Natural Language Processing (NLP) is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). Unlike the previous part, we only used local functions instead of a scaled up version using Spark.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important step in NLP is to divide the words of every tweet in a table to be able to easily treat them. It is easily done using the _'split'_ method provided in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['tweets'] = cleaned['main'] #keep tweets for later\n",
    "cleaned['main'] = cleaned['main'].str.split()\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stop words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have relevant tweets, it is necessary to remove useless words which would interfere with our analysis. We assumed stop words were the only important words to remove (as we already dealt with special characters and urls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stops(language, df, *col):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    if col:\n",
    "        df.loc[df['lang'] == lang, col] = df.loc[df['lang'] == lang, col].\\\n",
    "            apply(lambda data: [word for word in data if word not in lang_set])\n",
    "    else: \n",
    "        df = df.apply(lambda data: [word for word in data if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stops('english', cleaned, 'main')\n",
    "remove_stops('french', cleaned, 'main')\n",
    "remove_stops('german', cleaned, 'main')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of our NLP treatement pipeline is stemming. The idea is that words appearing in multiple forms (such as have, having, had, …) should only be considered once using their radical (e.g. \"hav\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(language, df, *col):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    if col:\n",
    "        df.loc[df['lang'] == lang, col] = df.loc[df['lang'] == lang, col].\\\n",
    "            apply(lambda data: [[stemmer.stem(word) for word in expression] for expression in data])\n",
    "    else:\n",
    "        df = df.apply(lambda data: [[stemmer.stem(word) for word in expression] for expression in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_words('english', cleaned, 'main')\n",
    "stem_words('french', cleaned, 'main')\n",
    "stem_words('german', cleaned, 'main')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dictionary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we applied all of these methods to our dataset, it is only natural that we do the same for the dictionary we will use. The first step of this part explains how we built the dictionary (and cleaned it as we did before) while the second part focuses on applying the previous NLP methods to the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems, such as can be seen in [2], [3]. Starting from this, we built our own dictionary by expanding the examples (as we expanded our subject to take into account multiple mental disorders instead of simply determining tweets of users diagnosed with clinical illnesses). As we treat tweets in multiple languages, we also translated all the terms and tried adding words that specifically target mood (or eating) disorders in french and german.\n",
    "\n",
    "_Note: you can take a look at this dictionary by clicking on the 4th reference (at the end of the file)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DICT_PATH = \"dictionary.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = pd.concat([dictionaries['german'].dropna(), dictionaries['swiss_german'].dropna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching our 3 datasets, we clean them as we did for the tweets in _Part 2_. However, as we created them ourselves, we only have to lowercase them and treat the special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dict = en_dict.apply(data_cleaning).str.split()\n",
    "fr_dict = fr_dict.apply(data_cleaning).str.split()\n",
    "de_dict = de_dict.apply(data_cleaning).str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries. \n",
    "\n",
    "_Note: As all methods were explained above, we will not dwell on each step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stops('english', en_dict)\n",
    "remove_stops('french', fr_dict)\n",
    "remove_stops('german', de_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_words('english', en_dict)\n",
    "stem_words('french', fr_dict)\n",
    "stem_words('german', de_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the soundness of our method, we display the head of each dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we cross the dataset with our dictionaries to retrieve the tweets exhibiting mental distress. This is the first (less naive) step of our analysis before running the Machine Learning algorithms on our data.\n",
    "\n",
    "We first transfrom the lists back to string sentences, for efficacy and code readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned.main = cleaned.main.map(lambda x:  ' '.join(x))\n",
    "de_dict = de_dict.map(lambda x:  ' '.join(x))\n",
    "en_dict = en_dict.map(lambda x:  ' '.join(x))\n",
    "fr_dict = fr_dict.map(lambda x:  ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we can find a dict entry for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_dict(tweet, dict_):\n",
    "    \"\"\"checks if dict entry matches tweet\"\"\"\n",
    "    match = [w for w in dict_ if w in tweet] #find matching for each entry\n",
    "    return len(match) > 0 #at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language.\n",
    "\n",
    "\n",
    "For english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered = cleaned[cleaned.lang == 'en']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'en']['main'].map(lambda x: check_dict(x, en_dict))]\n",
    "english_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see sadness an lonelyness or fear in this tweets, but some of them do not actualy express distress.\n",
    "We also note that the number of tweets has greatly been reduces through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the french tweets and again find similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered = cleaned[cleaned.lang == 'fr']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'fr']['main'].map(lambda x: check_dict(x, fr_dict))]\n",
    "french_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for german we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered = cleaned[cleaned.lang == 'de']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'de']['main'].map(lambda x: check_dict(x, de_dict))]\n",
    "german_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do all of this tweets reference the person, but they also talk about sadness or dissapointment.\n",
    "The remaining number is quite small, compared to the initial 4000 german tweets in the dataset, but it is a number we migth expect.\n",
    "\n",
    "Still those results could be better, which is why we decide to introduce a step 4 into our analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML (to be done in milestone 3)\n",
    "\n",
    "The final part of our data processing involves training a supervised Machine Learning classifier to get better results, as missclasifications are still somewhat frequent. Similar projects routinely use this [2] [3] [8].\n",
    "\n",
    "All these methods are implemented in scikit-learn and have an equivalent in Spark in case our computers cannot handle the size of the data, allowing us to perform these steps on the cluster.\n",
    "\n",
    "In order to do this, we go through the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Labeling the tweets\n",
    "\n",
    "We need to label a subset of tweets as 'mentaly distressed' or not. Ideally around 6000 per language [7], but due to limited time and manpower, we will limit ourself to 1000 tweets each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Construcing features TF-IDF\n",
    "\n",
    "Once we have the labeled set, we transfrom it in order to have features to use for the ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train SVM classifier\n",
    "\n",
    "According to [7], SVM along with TF-IDF tends to preform very well, especially with limited training set size (which is ideal in our case). We perform a binary classification on the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Relabel the training set\n",
    "\n",
    "Using the previously attained classifier, we relabel the set and discard tweets that are labeled as 'undistressed'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Data Analysis\n",
    "\n",
    "In the final analysis, we try to analyze and visualize the results in order to answer our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 General analysis\n",
    "\n",
    "We compare the set found using Machine Learning to the general cleaned set and the overall set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Finding related indicatior of mental health issues using LDA\n",
    "\n",
    "As discussed in [8], LDA one of the best ways to find relevant related words in tweets. Again, we plan on using the implemented scikit-learn version to preform this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Lookint at gender differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tweet frequency and type over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Comparing our results to census data\n",
    "\n",
    "This will be approximative since we could not find a dataset containing this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Comparing the languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Example dataprocessing pipeline](http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] [Dissertation containing a first dict](https://www.rand.org/content/dam/rand/pubs/rgs_dissertations/RGSD300/RGSD391/RAND_RGSD391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] [Second thesis containing dict](https://getd.libs.uga.edu/pdfs/kale_sayali_s_201512_ms.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] [Our own dictionary](https://docs.google.com/spreadsheets/d/1WwI9crZk36pcTOQ1g_5dumMd11OlkpFRNHsEvpkwLMk/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] [Stemming with Spark](https://github.com/master/spark-stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] [ML methods in Spark](https://spark.apache.org/docs/2.1.0/ml-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] [Public health paper using LDA](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2880/3264)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] [Best ways to do Text Classification](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
