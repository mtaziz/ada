{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we describe our pipeline and work done on the twitter datset to answer our research question(s).\n",
    "\n",
    "Overview:\n",
    "0. **Dataretrival:** how we access the dataset\n",
    "1. **Dataset selection and analysis:** a first look at the dataset, exploring potential issues\n",
    "2. **Dataset cleaning:** cleaning based on the results found in 1\n",
    "3. **NLP methods:** applying NLP methods to our data to retrieve relevant data\n",
    "4. **Machine Learning:** using machine learning to further clean\n",
    "5. **Analysis:** analysis preformed on our cleaned dataset\n",
    "6. **Conclusion**\n",
    "\n",
    "\n",
    "An implemention cleaning the data on spark can be found in run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords #Part 3\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Datasets retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not retrieve both datasets using the same method first because they did not have the same weight, but also because they were not provided on the same platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset (_**twitter-swisscom**_) was retrieved from a .zip file. Thus, we had access to the entire dataset quickly allowing us to have an overview of all the tweets when analyzing it (cf _Part 1_ below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset, however, was retrieved from the cluster. We first tried to retrieve the whole dataset but quickly realized it would be impossible to do so (it was very heavy and took a long time to be downloaded). Thus, we only extracted the first JSON file to perform our analysis. To do this, we used the following methods:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset selection & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first analysis is to familiarize ourselves with the dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "We were provided with two separate datasets containing swiss tweets. They were formated differently and contained different fields. While contained data collectd over the duration of multiple years, the other only covers a span of 10 months.\n",
    "We performed an analysis on both in order to be able to decide which one should be used in out project. After this analysis, we decided to use **dataset 2** for our project.\n",
    "\n",
    "While dataset 1 containes more precise location information, in the form of longitude and latitude, dataset 2 contains a sentiment analysis field, as well as a language field.\n",
    "As trying to categorize the language of each tweet in dataset 1 was quite expensive – having to deal with network latency of API requests – and a lot of preprocessing was necessary to get it to work, dataset 2 containing this field puts it at a clear advantage.\n",
    "\n",
    "We now provide a quick overview of dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of what each column in the *tsv file* containing the tweets represents. A sample file was given, but we optained the complete set of tweets (5gb) via a .zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following usefull columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted on.\n",
    "- **text** : content of tweet.\n",
    "- **placeLatitude** : latitude of tweet.\n",
    "- **placeLongitude** : longitude of tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 (from Spinn3r)\n",
    "\n",
    "This dataset has an elaborate description of each field available at the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema).\n",
    "Unlike the previous dataset, this dataset is given in JSON format.\n",
    "\n",
    "To deal with the amount of data present in the cluster we look at one day to perform our first analysis and then show how to scale up.\n",
    "\n",
    "The format of this dataset is a nested json that we could not find how to extract directly using the read JSON funtion provided. We thus use a JSON normalizer contained in the Pandas libary to extract it. We will later see that spark deals better with nested JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', '_index', '_score', 'author_avatar_img', 'author_gender',\n",
       "       'author_link', 'author_name', 'bucket', 'canonical', 'date_found',\n",
       "       'domain', 'hashcode', 'index_method', 'lang', 'links', 'main',\n",
       "       'main_checksum', 'main_format', 'main_length', 'mentions', 'permalink',\n",
       "       'published', 'resource', 'sentiment', 'sequence', 'sequence_range',\n",
       "       'site', 'source_content_checksum', 'source_content_length',\n",
       "       'source_created', 'source_date_found', 'source_description',\n",
       "       'source_favicon_height', 'source_favicon_width', 'source_favorites',\n",
       "       'source_followers', 'source_following', 'source_handle',\n",
       "       'source_hashcode', 'source_http_status', 'source_image_height',\n",
       "       'source_image_src', 'source_image_width', 'source_last_posted',\n",
       "       'source_last_published', 'source_last_updated', 'source_likes',\n",
       "       'source_link', 'source_location', 'source_parsed_posts',\n",
       "       'source_parsed_posts_max', 'source_profiles', 'source_publisher_type',\n",
       "       'source_resource', 'source_setting_author_policy',\n",
       "       'source_setting_index_strategy', 'source_setting_update_strategy',\n",
       "       'source_spam_probability', 'source_title', 'source_update_interval',\n",
       "       'source_user_interactions', 'source_verified', 'tags', 'type',\n",
       "       'version', '_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [ column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these columns, the one we can use are:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of tweet.\n",
    "- **tags**: tags associated with tweet, as provided by spinn3r.\n",
    "- **lang**: language of tweet.\n",
    "- **sentiment**: sentiment score of tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of user who tweeted.\n",
    "- **source_following**: number of people the user follows.\n",
    "\n",
    "\n",
    "We quickly discuss the usage of the most importan tags:\n",
    "\n",
    "**Main** stands at the center of our analysis, we plan to preform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "**Published** can be used to map the tweets over the duration of the year, and look at seasonal changes.\n",
    "**Source_location** can be used look at the geographical distribution of the tweets.\n",
    "**Lang** will be used to filter out unwanted languages, which we need to do in order to preform the nlp tasks.\n",
    "**Author_gender** will be used to identify the gender and look at the differenc between genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns we care about\n",
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at general distributions in this dataset. While this example isn't representative when it comes to the tweets – especially given it contains tweets of the 1th of january – it can still give us insights on the other fields.\n",
    "\n",
    "We assume that roughly the same categories of users were active on that day, so we can draw conclusions on the distribution of language and gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language distribution is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     7350\n",
       "fr     4421\n",
       "de     4174\n",
       "pt     2139\n",
       "es     1463\n",
       "und     893\n",
       "it      266\n",
       "in      125\n",
       "pl      124\n",
       "tr      124\n",
       "ar      116\n",
       "ja      111\n",
       "ht       84\n",
       "nl       71\n",
       "tl       60\n",
       "et       43\n",
       "da       39\n",
       "sv       38\n",
       "zh       30\n",
       "no       25\n",
       "ko       25\n",
       "fi       15\n",
       "lt       14\n",
       "ru        9\n",
       "hi        7\n",
       "is        7\n",
       "sl        5\n",
       "hu        5\n",
       "lv        4\n",
       "ta        3\n",
       "bg        2\n",
       "th        1\n",
       "vi        1\n",
       "el        1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that English, French and German are most frequent. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the distribution of gender in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNKNOWN    14424\n",
       "FEMALE      4044\n",
       "MALE        3327\n",
       "Name: author_gender, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['author_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not seem to contain this information.  But there are still a lot that do, so we could use the ones that do to look at differences between gender, although it would not give use an unbiased set, as the type of user declaring their gender on twitter may be different than those who chose not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the sentiment column, to see how the tweets were labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEUTRAL     20357\n",
       "POSITIVE      482\n",
       "NEGATIVE       63\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of tweets was labeled as neutral, and only a very small number are labeled as negative. We will look at both neutral and negatively labeled tweets.\n",
    "\n",
    "Under the assumption that the positives are not false positives, a tweet showing signs of mental distress will not be labeled as POSITIVE, hence we can safely exclude these tweets from further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probabality we see that not a single tweet was labeled as spam. This puts into question the accuracy of the labeling, as the set of tweets on that day most certainly contains spam. We will still use it, as we assume the chance of false positives is low, so we lose nothing by using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    21795\n",
       "Name: source_spam_probability, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['source_spam_probability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine the locations provided by the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Switzerland           6613\n",
       "Schweiz               1977\n",
       "Suisse                1762\n",
       "Genève                1094\n",
       "Zürich                 961\n",
       "Geneva                 676\n",
       "Zurich                 544\n",
       "Basel                  433\n",
       "Bern                   363\n",
       "Lausanne               287\n",
       "CH                     154\n",
       "Swiss                  124\n",
       "Lugano                  93\n",
       "St. Gallen              88\n",
       "Geneve                  67\n",
       "Schaffhausen            64\n",
       "Fribourg                59\n",
       "Svizzera                50\n",
       "Luzern                  44\n",
       "Winterthur              42\n",
       "Baden                   37\n",
       "Chur                    26\n",
       "Lenzburg                23\n",
       "Waldenburg              21\n",
       "Interlaken              21\n",
       "Biel                    19\n",
       "Marly                   19\n",
       "Genf                    19\n",
       "Sankt Gallen            18\n",
       "Neuchâtel               14\n",
       "                      ... \n",
       "Visp                     2\n",
       "Frauenfeld               2\n",
       "Bâle                     2\n",
       "Horgen                   2\n",
       "Glarus                   2\n",
       "Hinwil                   2\n",
       "Herisau                  1\n",
       "Suica                    1\n",
       "Meilen                   1\n",
       "Schwyz                   1\n",
       "Le Locle                 1\n",
       "Conthey                  1\n",
       "Samedan                  1\n",
       "Zofingen                 1\n",
       "Frutigen                 1\n",
       "Andelfingen              1\n",
       "Rheinfelden              1\n",
       "Langnau                  1\n",
       "Zwitserland              1\n",
       "Suíça                    1\n",
       "Sursee                   1\n",
       "Payerne                  1\n",
       "Pfäffikon                1\n",
       "Poschiavo                1\n",
       "Lauterbrunnen            1\n",
       "สวิตเซอร์แลนด์           1\n",
       "Renens                   1\n",
       "Affoltern am Albis       1\n",
       "İsviçre                  1\n",
       "Laufen                   1\n",
       "Name: source_location, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we only look at the locations for the languages we care about, as location seems to be language dependent\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- there are a lot of locations that are the same but in a different language, such as Switzerland and Schweiz\n",
    "- the names of the locations are not just in the languages we are interessted in (see สวิตเซอร์แลนด์)\n",
    "- a vast majority of the dataset is just labeled as 'switzerland'\n",
    "- but as opposed to dataset 1, they are all located in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main                       21795\n",
       "published                  21795\n",
       "source_spam_probability    21795\n",
       "source_location            21795\n",
       "tags                        3865\n",
       "lang                       21795\n",
       "sentiment                  20902\n",
       "author_gender              21795\n",
       "source_followers           21795\n",
       "source_following           21795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.count() #give us number of NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for most tweets the tags are missing, this may indicatie that te taggs wont be usefull for analysis, but this does not influence our research at this stage.\n",
    "We also note that for some tweets the sentiment is missing. As noted on spinn3er, this may be due to the tweets not containing enough linguistic information. As we ourself will filter out such tweets, the remaining set should contain sentiment. Even if not, this field is not central to our analysis.\n",
    "\n",
    "To be sure this wont be an issue we quickly look at tweets and note that for all those uncategorized tweets the language is unkown as well, so they will all be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "und    893\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[~(twitter_df.sentiment.isin(['POSITIVE', 'NEGATIVE', 'NEUTRAL']))]['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the Tweets\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweet content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582     Chez Mohawk's Cycles on a toujours une place pour la Velosophe Beer! Merci. http://fb.me/6Qi6lrkSZ                                                                                                                                                                                                                                   \n",
       "7053     @StopHatinRussia ich habe dieses Jahr zu jeder Mitternacht in jeder Weltregion einen Tweet rausgelassen… morgen früh kommt Amerika dran…                                                                                                                                                                                             \n",
       "4169     attention la kaïra des bacs à sable est parmi nous                                                                                                                                                                                                                                                                                   \n",
       "8612     Psicologo em Curvelo, terapeuta de depressao, autismo, adolescentes http://psicologosetelagoas.com.br  PSI00126                                                                                                                                                                                                                      \n",
       "15520    @ommoody04 من فضلك أنا مش عاوزة أتكلم مع حد مفترضة أنه هيقوللي كلام مش عاوزة أسمعه. تحياتي لك ويسعد مساك.                                                                                                                                                                                                                            \n",
       "20914    @RoadTrip3000 Don't forget to DM me babes #NewBoyband2016                                                                                                                                                                                                                                                                            \n",
       "13008    St-Sylvestre: Une grosse rixe a mobilisé la police zurichoise http://bit.ly/1R4E9nI  pic.twitter.com/5rNu8WLWtx                                                                                                                                                                                                                      \n",
       "15245    #fireworks #Silvester #Silvester2015 #Silvesternacht #Feuerwerk #photography #photo #photooftheday #newyear #swiss pic.twitter.com/CBK49xLqO6                                                                                                                                                                                        \n",
       "7545     Söyleyen RTE carpitan biz.. Yine Biz suclu ciktik, Adamlar Zeytinyagi gibi.. https://twitter.com/yeniakit/status/682966893439905792 … Cumhurbaşkanlığı'ndan ''Hitler Almanyası' açıklaması! http://www.yeniakit.com.tr/haber/cumhurbaskanligindan-hitler-almanyasi-aciklamasi-kabul-edilemez-117785.html … pic.twitter.com/NhOUjgh31t\n",
       "10467    Créer sa société offshore au Royaume Uni, une... http://www.fidusuisse-offshore.com/societe-offshore-royaume-uni-opportunite-a-saisir/ …                                                                                                                                                                                             \n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly see that the tweets containing links are not relevant to our research question, as they are mostly news or adds. We make the assumption that this would be the case anytime of the year.\n",
    "\n",
    "We look at the tweets containing links and confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     Alpinultras. Sello, circuito y estudio médico para potenciar las travesías de ultrafondo de al… http://wp.me/p9GIl-b3O  vía @CarrerasMontana \n",
       "7     5 people unfollowed me today tracked by http://unfollowspy.com                                                                               \n",
       "9     Wir wünschen euch von Herzen ein gesundes 2016! http://fb.me/7SfcGRpCP                                                                       \n",
       "17    Aktuellste Stellenangebote http://tinyurl.com/bugpgs8                                                                                        \n",
       "23    Naa geierst Du wieder und willst wichsen..Na gut aber Du musst 2 mal spritzen.Du musst… http://dlvr.it/D8jjgL  pic.twitter.com/LBV2QcdDIF    \n",
       "25    AZEALIA BANKS - 212 FT. LAZY JAY http://fb.me/3GqunbPRp                                                                                      \n",
       "26    Alles Gute fürs Neue Jahr! Sehen Sie eine Videobotschaft des CEO #Bringhen Group: https://vimeo.com/148510641  http://fb.me/4SuzAOUz0        \n",
       "27    Diverse Angebote http://tinyurl.com/bto4bbc                                                                                                  \n",
       "32    Mürted demiş birisi birisine http://www.zaman.com/yazarlar/ahmet-kurucan/murted-demis-birisi-birisine_2335698.html … @zamancomtr aracılığıyla\n",
       "33    Wie wichtig ist die Schule ? - Investapedia Finanzschule - Financial Coaching: http://youtu.be/21GbY1UQKPM?a  über @YouTube                  \n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now preform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1731     Suicide bombing kills 1 at Kabul restaurant - http://news.bridgeward.com/suicide-bombing-kills-1...\n",
       "3624                                              IS suicide attack planned in Munich http://bit.ly/1Owgpls \n",
       "5228     Afghanistan: attentat suicide dans un restaurant français de Kaboul - Europe1 http://dlvr.it/D8q...\n",
       "6896     Ian Murdock's last night alive #debian http://sanfrancisco.cbslocal.com/2015/12/31/prominent-pro...\n",
       "9479     Ipad Music Making Daily is out! http://paper.li/suicidesurfer77/1315555100?edition_id=bb78ee60-b...\n",
       "10145     L'Etat islamique soupçonné d'avoir planifié un attentat suicide en Allemagne http://rss.ch/662786 \n",
       "10446                           Afghanistan: attentat suicide dans le centre de Kaboul http://rss.ch/662843 \n",
       "11096    Kaboul : les talibans revendiquent l'attentat suicide de ce 1er janvier contre un restaurant fra...\n",
       "11377     L'Etat islamique soupçonné d'avoir planifié un attentat suicide en Allemagne http://rss.ch/662783 \n",
       "14771                                                                @ModestLord suicide is an option Modest\n",
       "16065    Bon je vais arrêter de me plaindre car je sens les gens venir me dire \"suicide-toi.\" Ou \"ta gueu...\n",
       "17093    can't wait for my computer to die for real so I can have a real reason to commit suicide pic.twi...\n",
       "18348    Armed officers guard evacuated Germany train stations http://www.dailymail.co.uk/news/article-33...\n",
       "20506    Hamas Announces Return to Suicide Bombings Inside Israel | PJ Media https://pjmedia.com/trending...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main'] #news instead of personal reference\n",
    "#removing news would be good\n",
    "#we also see that we should not remove pic.twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a lot of these tweets contain news, we should remove the links in order to get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at therapy (in german), again this confirms the issue with links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11567    #Arbeit #Jobs #CH Chefarzt Psychiatrie/Psychotherapie 100 (w/m): Herisau, Appenzell Ausserrhoden...\n",
       "12508    Aktuelle Firmensuche \"Physiotherapie Praxis Bücheli\" Ostermundigen (BE) #Gesundheitsberatung #Su...\n",
       "13610    Natürlich Gesund: Mistel und Myrrhe in der Phytotherapie http://pure-natur.blogspot.ch/2014/06/t...\n",
       "13720    Aktuelle Suchabfrage \"Kunsttherapie\" auf @Help_ch #Kunsttherapie #Suchportal #Schweiz #Suche htt...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main'] #adds instead of personal reference\n",
    "#all contain links..reason to remove links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we look at the tweets that are retweets, to get an idea if they could be useful.\n",
    "From what we see, and what others [8] have seen they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this analysis we apply the gained knowledge and start to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean the data to be able to use it correctly for the rest of the project.\n",
    "\n",
    "First, we use Pandas on a small subset of the dataset and locally, so as to be able to make sure all functions work as expected and to test them out. Later on, we use spark to be able to do the same operations on a bigger scale.\n",
    "The spark version of this code can be found in [run.py](run.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unnesting the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in 1, the dataset is given in nested json format, we thus have to unnest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = json_normalize(data)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a Dataframe, we choose the columns we think will be useful for the rest of the proejct, as described in 1. Thus, we select the columns we previously found to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only keeping the necessary columns\n",
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them will bring us interesting insights. For example: are men or women more prone to depression? Is someone with more followers happy? There are quite a few interesting questions that can thus be asked and later on answered with the information we choose to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Language Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as our project is mainly focused on Switzerland, we will make sure we only keep the tweets that are in languages spoken by the majority of Switzerland, which would mean French, German and Italian. Unfortunatley, as no one in the group speaks Italian, we decided to forfeit the language in favor of English. This has two reasons: first, a lot of people speak English on the Internet, as it is a global language; second, there is a large English-speaking community in Switzerland.\n",
    "\n",
    "Additionaly, note that Swiss German is counted in the German part, even if some of the words are a little bit more peculiar. This will be taken into account in our dictionnaries later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_mask = ~cleaned.lang.isin('de', 'en', 'fr')\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as we were provided with a dataset that has a sentiment analysis, we use it to be able to drop all tweets that have a **positive** result. Seeing as we want to find depression and other mental illnesses, we are mostly searching for negative or neutral tweets. \n",
    "\n",
    "It could be that the sentiment analysis does have a few false positives (for example, the use of a smiling face could be used saracastically and thus making the result of the analysis be positive), but we decide to use the data as it is, since we are interessted in an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Spam Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam is not relevant to any of our objectives, thus, we decide to drop all the tweets having a greater value than 0.5 in the **source_spam_probability** column. Once again, we decide to trust the algorithm giving us this data.\n",
    "\n",
    "We chose a 50% threshold as it follows the maximum likelihood rule: it is likely to be spam if it has more than 50% chance to be spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time Format Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the time in some analysis (to find seasonal patterns in depression for example), we also need the dates to be properly formatted. Thus, we decided to reformat all the dates in the column **published** to be certain to be able to use them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Text Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the tweets that interest us remaining, we need to do a little bit of cleaning in the **main** column. Indeed, to be able to process the text and find patterns or any other information that we could seek.\n",
    "\n",
    "First we put everything in lowercase to be able to make comapraisons between words without having problems with different representation between the same letter in uppercase and lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].astype(str).str.lower().\\\n",
    "                    apply(lambda tweet: unicodedata.normalize('NFD', tweet).\\\n",
    "                    encode('ascii', 'ignore').decode('utf-8'))\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove all URLs and images. As we do not have any image processing, the pictures are not useful. As for the links, some information might be relevant (for example the title of an article if it is to be found in the URL) but most of it isn't.\n",
    "\n",
    "We also remove all non-alphanumeric characters, as they are will not give more information and might get in the way of the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(\"www\\S+\", '').str.replace(\"http\\S+\", '').\\\n",
    "                    str.replace(\"pic.twitter\\S+\", '').str.replace('[^\\w\\s]', '')\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Natural Language Processing (NLP) is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). Unlike the previous part, we only used local functions instead of a scaled up version using Spark.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].str.split().\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set]).\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important step in NLP is to divide the words of every tweet in a table to be able to easily treat them. It is easily done using the _'split'_ method provided in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.split()\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stop words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have relevant tweets, it is necessary to remove useless words which would interfere with our analysis. We assumed stop words were the only important words to remove (as we already dealt with special characters and urls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_stops('english')\n",
    "remove_stops('french')\n",
    "remove_stops('german')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of our NLP treatement pipeline is stemming. The idea is that words appearing in multiple forms (such as have, having, had, …) should only be considered once using their radical (e.g. \"hav\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_words('english')\n",
    "stem_words('french')\n",
    "stem_words('german')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dictionary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we applied all of these methods to our dataset, it is only natural that we do the same for the dictionary we will use. The first step of this part explains how we built the dictionary (and cleaned it as we did before) while the second part focuses on applying the previous NLP methods to the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Building the Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems. Starting from this, we built our own dictionary by expanding the examples (as we expanded our subject to take into account multiple mental disorders instead of simply determining tweets of users diagnosed with clinical depression). As we treat tweets in multiple languages, we also translated all the terms and tried adding words that specifically target mood (or eating) disorders in french and german.\n",
    "\n",
    "_Note: you can take a look at this dictionary by clicking on the 4th reference (at the end of the file)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. labeling the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 construcing features TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 train SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 relabel training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to find similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Example dataprocessing pipeline](http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] [Dissertation containing a first dict](https://www.rand.org/content/dam/rand/pubs/rgs_dissertations/RGSD300/RGSD391/RAND_RGSD391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] [Second thesis containing dict](https://getd.libs.uga.edu/pdfs/kale_sayali_s_201512_ms.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] [Our own dictionary](https://docs.google.com/spreadsheets/d/1WwI9crZk36pcTOQ1g_5dumMd11OlkpFRNHsEvpkwLMk/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] [Stemming with Spark](https://github.com/master/spark-stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] [ML methods in Spark](https://spark.apache.org/docs/2.1.0/ml-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] [Best ways to do Text Classification](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] [Public health paper using LDA](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2880/3264)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
