{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we describe the multiple steps of our preprocessing pipeline and the work done on the twitter dataset to answer our research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "0. **Data Retrival:** how we access the dataset\n",
    "1. **Dataset Selection and Analysis:** a first look at the dataset, exploring potential issues\n",
    "2. **Dataset Cleaning:** cleaning based on the results found in _Part 1_\n",
    "3. **NLP methods:** applying NLP methods to our data to retrieve relevant data\n",
    "4. **Machine Learning:** using Machine Learning to further clean our dataset\n",
    "5. **Analysis:** trying to find meaning in the final dataset we retrieve\n",
    "6. **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Datasets Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not retrieve both datasets using the same method first because they did not have the same weight, but also because they were not provided on the same platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset (_**twitter-swisscom**_) was retrieved from a .zip file. Thus, we had access to the entire dataset quickly allowing us to have an overview of all the tweets when analyzing it (cf _Part 1_ below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset (_**Spinn3r**_), however, was retrieved from the cluster. We first tried to retrieve the whole dataset but quickly realized it would be impossible to do so (it was very heavy and took a long time to be downloaded). Thus, we only extracted the first JSON file to perform our analysis. To do this, we used the following methods:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets. They were formated differently and contained different fields. On top of this, while the first dataset contained data collectd over multiple years, the other only covers a span of 10 months.\n",
    "\n",
    "The goal of our first analysis was thus to decide which dataset would be more useful but also to familiarize ourselves with the chosen dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "After the analysis (presented below), we decided to use **dataset 2** for our project. While dataset 1 contains more precise location information (longitude and latitude), dataset 2 contains a sentiment analysis field as well as a language field. As trying to categorize the language of each tweet in the first dataset was quite expensive in terms of computation – we needed to deal with the network latency of our API requests – and a lot of preprocessing was necessary to get it to work, dataset 2 was clearly better suited for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 (from Spinn3r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata analysis\n",
    "\n",
    "This dataset has an elaborate description of each field available on the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema). Given the amount of data present in the cluster, we only look at one day to perform our first analysis (we will later show how we scale our operations using Spark).\n",
    "\n",
    "Unlike the previous dataset, this dataset is given in JSON format (with nested elemtns). As we could not find how to extract all the data directly using the *read_json* function provided, we use a JSON normalizer (provided in the *Pandas* libary). \n",
    "\n",
    "_Note: We will later see that Spark deals better with nested JSON._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', '_index', '_score', 'author_avatar_img', 'author_gender',\n",
       "       'author_link', 'author_name', 'bucket', 'canonical', 'date_found',\n",
       "       'domain', 'hashcode', 'index_method', 'lang', 'links', 'main',\n",
       "       'main_checksum', 'main_format', 'main_length', 'mentions', 'permalink',\n",
       "       'published', 'resource', 'sentiment', 'sequence', 'sequence_range',\n",
       "       'site', 'source_content_checksum', 'source_content_length',\n",
       "       'source_created', 'source_date_found', 'source_description',\n",
       "       'source_favicon_height', 'source_favicon_width', 'source_favorites',\n",
       "       'source_followers', 'source_following', 'source_handle',\n",
       "       'source_hashcode', 'source_http_status', 'source_image_height',\n",
       "       'source_image_src', 'source_image_width', 'source_last_posted',\n",
       "       'source_last_published', 'source_last_updated', 'source_likes',\n",
       "       'source_link', 'source_location', 'source_parsed_posts',\n",
       "       'source_parsed_posts_max', 'source_profiles', 'source_publisher_type',\n",
       "       'source_resource', 'source_setting_author_policy',\n",
       "       'source_setting_index_strategy', 'source_setting_update_strategy',\n",
       "       'source_spam_probability', 'source_title', 'source_update_interval',\n",
       "       'source_user_interactions', 'source_verified', 'tags', 'type',\n",
       "       'version', '_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these columns, we find the ones below to be the most useful for our analysis:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of the tweet.\n",
    "- **tags**: tags associated with the tweet (provided by Spinn3r).\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of the user who tweeted.\n",
    "- **source_following**: number of people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider these tags to be the most useful as:\n",
    "\n",
    "- **Main** stands at the center of our analysis. We plan to perform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "- **Published** can be used to map the tweets over the duration of the year and look for seasonal changes.\n",
    "- **Source_location** can be used to look at the geographical distribution of the tweets.\n",
    "- **Lang** will be used to filter out unwanted languages, which we need to do in order to perform our NLP tasks.\n",
    "- **Author_gender** will be used to identify the gender and look at the differences between both genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Distribution\n",
    "\n",
    "While this example isn't representative (especially given that it contains the tweets posted on January, 1st), it can still give us insights on other fields. We assume that roughly the same categories of users were active on that day, meaning we can draw conclusions on the distribution of language and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     7350\n",
       "fr     4421\n",
       "de     4174\n",
       "pt     2139\n",
       "es     1463\n",
       "und     893\n",
       "it      266\n",
       "in      125\n",
       "tr      124\n",
       "pl      124\n",
       "ar      116\n",
       "ja      111\n",
       "ht       84\n",
       "nl       71\n",
       "tl       60\n",
       "et       43\n",
       "da       39\n",
       "sv       38\n",
       "zh       30\n",
       "ko       25\n",
       "no       25\n",
       "fi       15\n",
       "lt       14\n",
       "ru        9\n",
       "hi        7\n",
       "is        7\n",
       "hu        5\n",
       "sl        5\n",
       "lv        4\n",
       "ta        3\n",
       "bg        2\n",
       "el        1\n",
       "th        1\n",
       "vi        1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that English, French and German are the most frequent languages. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNKNOWN    14424\n",
       "FEMALE      4044\n",
       "MALE        3327\n",
       "Name: author_gender, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['author_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, we have a significant number of profiles where the gender is documented meaning we could use these to look at the differences in mental distress between the genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEUTRAL     20357\n",
       "POSITIVE      482\n",
       "NEGATIVE       63\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that the vast majority of tweets are labeled as neutral while only a very small number is labeled as negative. Given our subject will look at both the **'NEUTRAL'** and **'NEGATIVE'** tweets.\n",
    "\n",
    "_Note: we make the assumption that there are no false positives, meaning that a tweet showing signs of mental distress will not be labeled as **'POSITIVE'**. This means we can safely exclude these tweets from further analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    21795\n",
       "Name: source_spam_probability, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['source_spam_probability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probability of the tweet set, we see that not a single tweet was labeled as spam. This makes us question the accuracy of the labeling as the set of tweets on that day most certainly contains spam. However, we will still use it as we assume the chance of having false positives to be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Switzerland           6613\n",
       "Schweiz               1977\n",
       "Suisse                1762\n",
       "Genève                1094\n",
       "Zürich                 961\n",
       "Geneva                 676\n",
       "Zurich                 544\n",
       "Basel                  433\n",
       "Bern                   363\n",
       "Lausanne               287\n",
       "CH                     154\n",
       "Swiss                  124\n",
       "Lugano                  93\n",
       "St. Gallen              88\n",
       "Geneve                  67\n",
       "Schaffhausen            64\n",
       "Fribourg                59\n",
       "Svizzera                50\n",
       "Luzern                  44\n",
       "Winterthur              42\n",
       "Baden                   37\n",
       "Chur                    26\n",
       "Lenzburg                23\n",
       "Interlaken              21\n",
       "Waldenburg              21\n",
       "Biel                    19\n",
       "Genf                    19\n",
       "Marly                   19\n",
       "Sankt Gallen            18\n",
       "Solothurn               14\n",
       "                      ... \n",
       "Hinwil                   2\n",
       "Bâle                     2\n",
       "Frauenfeld               2\n",
       "Horgen                   2\n",
       "Visp                     2\n",
       "Glarus                   2\n",
       "Langnau                  1\n",
       "Andelfingen              1\n",
       "Suica                    1\n",
       "Frutigen                 1\n",
       "Payerne                  1\n",
       "Schwyz                   1\n",
       "Affoltern am Albis       1\n",
       "Zwitserland              1\n",
       "Poschiavo                1\n",
       "Meilen                   1\n",
       "สวิตเซอร์แลนด์           1\n",
       "Rheinfelden              1\n",
       "Zofingen                 1\n",
       "Herisau                  1\n",
       "Pfäffikon                1\n",
       "Lauterbrunnen            1\n",
       "İsviçre                  1\n",
       "Conthey                  1\n",
       "Samedan                  1\n",
       "Sursee                   1\n",
       "Renens                   1\n",
       "Laufen                   1\n",
       "Suíça                    1\n",
       "Le Locle                 1\n",
       "Name: source_location, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the location seems to be language dependent, we only care about tweets written in the 3 languages we need. We see that:\n",
    "- A lot of locations only differ in language (e.g. as Switzerland and Schweiz)\n",
    "- The name of the locations are not always given languages we are interested in (e.g. สวิตเซอร์แลนด์)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- As opposed to dataset 1, all tweets are located in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main                       21795\n",
       "published                  21795\n",
       "source_spam_probability    21795\n",
       "source_location            21795\n",
       "tags                        3865\n",
       "lang                       21795\n",
       "sentiment                  20902\n",
       "author_gender              21795\n",
       "source_followers           21795\n",
       "source_following           21795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values in the dataset, we see that for most tweets, the tags are missing. This may indicate that the tags won't be usefull for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information. As we will filter out such tweets, the remaining set should contain sentiment. Even if doesn't, this field is not central to our analysis, it is merely used to help us filter our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "und    893\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[~(twitter_df.sentiment.isin(['POSITIVE', 'NEGATIVE', 'NEUTRAL']))]['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure this won't be an issue, we quickly look at all the uncategorized tweets and see that the language is unkown as well, meaning they will all be removed anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential issues\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7002     Hello everyone, have a great Friday! Looking forward to reading your tweets :) (insight by http://commun.it )                                  \n",
       "16140    anime fight between Nico and his bad luck                                                                                                      \n",
       "3148     \"\"\"\"Tratamento da doenca do Panico, Sindrome do Panico, Ansiedades Sete Lagoas http://psicologosetelagoas.com.br  313773-9185 OH7366\"\"\"\"       \n",
       "18324    2002 @AudemarsPiguet Royal Oak Concept Watch is one of the very first Super watches. Click http://bit.ly/1D9TojO  pic.twitter.com/aJ80gjWTj3   \n",
       "1196     Feliz inicio de año! Todo lo bueno y mejor para todos!                                                                                         \n",
       "5412     ouais comme toute les années ahhahah https://twitter.com/lxviss/status/682736832581337088 … J'aurai fini 2015 célibataire mdr                  \n",
       "734      What if Star Wars had been realised by Fred Astaire? Perfect #alternatehistory for @drahsturgis \"The Tap Awakens\" https://youtu.be/tZcbwz3N2eU \n",
       "21742    @MattDaddarioFan @McNamaraFans @EmeraudeDaily @HarryShumDaily @DomSherwoodFan @Alberto_Rosende i forgot to follow Dom'a account                \n",
       "9498     Publiqué una nueva foto en Facebook http://fb.me/2zue6s3A8                                                                                     \n",
       "18710    New Year, new beginnings. Counting on spending a lot more time in this beautiful place in 2016 @montreuxriviera #ch pic.twitter.com/syTU4qR8fK \n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we immediatly see that the tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     Alpinultras. Sello, circuito y estudio médico para potenciar las travesías de ultrafondo de al… http://wp.me/p9GIl-b3O  vía @CarrerasMontana \n",
       "7     5 people unfollowed me today tracked by http://unfollowspy.com                                                                               \n",
       "9     Wir wünschen euch von Herzen ein gesundes 2016! http://fb.me/7SfcGRpCP                                                                       \n",
       "17    Aktuellste Stellenangebote http://tinyurl.com/bugpgs8                                                                                        \n",
       "23    Naa geierst Du wieder und willst wichsen..Na gut aber Du musst 2 mal spritzen.Du musst… http://dlvr.it/D8jjgL  pic.twitter.com/LBV2QcdDIF    \n",
       "25    AZEALIA BANKS - 212 FT. LAZY JAY http://fb.me/3GqunbPRp                                                                                      \n",
       "26    Alles Gute fürs Neue Jahr! Sehen Sie eine Videobotschaft des CEO #Bringhen Group: https://vimeo.com/148510641  http://fb.me/4SuzAOUz0        \n",
       "27    Diverse Angebote http://tinyurl.com/bto4bbc                                                                                                  \n",
       "32    Mürted demiş birisi birisine http://www.zaman.com/yazarlar/ahmet-kurucan/murted-demis-birisi-birisine_2335698.html … @zamancomtr aracılığıyla\n",
       "33    Wie wichtig ist die Schule ? - Investapedia Finanzschule - Financial Coaching: http://youtu.be/21GbY1UQKPM?a  über @YouTube                  \n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more intently at the tweets containing links, we can make the assumption that the relationship between URLs and spam is a general rule (at any time of the year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Preform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1731     Suicide bombing kills 1 at Kabul restaurant - http://news.bridgeward.com/suicide-bombing-kills-1...\n",
       "3624                                              IS suicide attack planned in Munich http://bit.ly/1Owgpls \n",
       "5228     Afghanistan: attentat suicide dans un restaurant français de Kaboul - Europe1 http://dlvr.it/D8q...\n",
       "6896     Ian Murdock's last night alive #debian http://sanfrancisco.cbslocal.com/2015/12/31/prominent-pro...\n",
       "9479     Ipad Music Making Daily is out! http://paper.li/suicidesurfer77/1315555100?edition_id=bb78ee60-b...\n",
       "10145     L'Etat islamique soupçonné d'avoir planifié un attentat suicide en Allemagne http://rss.ch/662786 \n",
       "10446                           Afghanistan: attentat suicide dans le centre de Kaboul http://rss.ch/662843 \n",
       "11096    Kaboul : les talibans revendiquent l'attentat suicide de ce 1er janvier contre un restaurant fra...\n",
       "11377     L'Etat islamique soupçonné d'avoir planifié un attentat suicide en Allemagne http://rss.ch/662783 \n",
       "14771                                                                @ModestLord suicide is an option Modest\n",
       "16065    Bon je vais arrêter de me plaindre car je sens les gens venir me dire \"suicide-toi.\" Ou \"ta gueu...\n",
       "17093    can't wait for my computer to die for real so I can have a real reason to commit suicide pic.twi...\n",
       "18348    Armed officers guard evacuated Germany train stations http://www.dailymail.co.uk/news/article-33...\n",
       "20506    Hamas Announces Return to Suicide Bombings Inside Israel | PJ Media https://pjmedia.com/trending...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing a simplistic dictionary matching using the occurences of the word *'suicide'* in our tweet set, we see that a lot of these tweets contain news. This further comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11567    #Arbeit #Jobs #CH Chefarzt Psychiatrie/Psychotherapie 100 (w/m): Herisau, Appenzell Ausserrhoden...\n",
       "12508    Aktuelle Firmensuche \"Physiotherapie Praxis Bücheli\" Ostermundigen (BE) #Gesundheitsberatung #Su...\n",
       "13610    Natürlich Gesund: Mistel und Myrrhe in der Phytotherapie http://pure-natur.blogspot.ch/2014/06/t...\n",
       "13720    Aktuelle Suchabfrage \"Kunsttherapie\" auf @Help_ch #Kunsttherapie #Suchportal #Schweiz #Suche htt...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word *'therapy'* (in German), we can confirm once again the issue there is with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124     RT @TimeOutSwiss : Discover some of the best ski resorts in #Switzerland #Swissalps #SwissSki #s...\n",
       "3472    RT @SkafarPierre: General differences in use of social media for health care #hcsm #digitial #so...\n",
       "3697                           RT @Boehringer: We wish you a happy new year! #Hcsm #hcsmeu #socmed #newyear\n",
       "3751    RT @Paul_Sonnier: Tech That Will Change Your Life in 2016 http://stfi.re/xzzdya  #IoT #Whealth #...\n",
       "3786    touche.... https://twitter.com/JamesRobertWebb/status/682951862929080322 … And #SometimesWhenIFe...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to get an idea on how useful they could be. From what we see, and what others have seen [8], they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this analysis, we can apply this gained knowledge to start cleaning our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean our dataset to be able to use it correctly for our research.\n",
    "\n",
    "First, we use Pandas locally on a small subset of the tweets to explore different cleaning methods and make sure the functions we chose work as expected. After proving that our concept works, we use Spark to scale up our operations and be able to perform them on the 320 files provided on the cluster.\n",
    "\n",
    "_Note: The Spark version of this code can be found in the [run.py](run.py) file._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unnesting the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in _Part 1_, the provided dataset uses a nested json format. Thus, to be able to work with it, we need to unnest it using the _normalize_ function.\n",
    "\n",
    "_Note: we normalize the **data** DF as we use it in **Part 1** to import our datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = json_normalize(data)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a functional Dataframe, we choose the useful columns determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned in _Part 1_, we chose this columns for the possible insights they can give us. Some of the question we want to answer in our research are:\n",
    "- _Are men or women more prone to depression?_ \n",
    "- _Is someone with more followers happy?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Language Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided contains tweets in a lot of languages. However, as our project is focused on the Swiss Population, we only keep tweets written in one of the official Swiss languages (i.e. French, German and Italian).\n",
    "\n",
    "However, we had 2 hurdles (pushing us to do slight changes):\n",
    "- Nobody in the group is Italian-Speaking\n",
    "- Most of the tweets are written in English _(this is due to the large English-Speaking community in Switzerland but also due to the fact English is a _de-facto_ choice when communicating through the Internet)_\n",
    "\n",
    "Moreover, we should not forget the existence of Swiss-German dialects which also complicates the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work our way through these problems, we decided to keep 3 languages: English, French and German. Note that we mean \"German\" in an extended manner as we also include Swiss-German dialects in the lot (this will be taken into account when creating our dictionaries as some words slightly change between the two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_mask = ~cleaned.lang.isin(['de', 'en', 'fr'])\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use a dataset with sentiment labeling, we use this opportunity to drop all tweets marked as **POSITIVE** given the nature of our subject (we are looking for signs of mental distress). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this labeling is largely based on emojis, we expect some false positives in the **POSITIVE** category. However, it is too difficult to consider the sarcastic use of the _:)_ emoji (text analysis is not refined enough yet to detect such subtleties). Thus, we knowingly choose this method as we are only interested in having an overview of tweets showcasing signs of mental illnesses and not listing all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Spam Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam is not relevant to any of our objectives. Thus, we decide to drop all tweets having a probability of being a spam greater value than 0.5 (given in the **source_spam_probability** field). Once again, we decide to trust the system that treated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: our choice to have a 50% threshold follows the maximum likelihood rule and was considered to be a sufficiently robust value._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Time Format Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the time in our analysis (especially to find seasonal patterns in depression), we need the dates to be properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2016-01-01 00:30:04\n",
       "1   2016-01-01 12:12:33\n",
       "2   2016-01-01 19:52:15\n",
       "3   2016-01-01 11:58:03\n",
       "4   2016-01-01 06:17:28\n",
       "Name: published, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Text Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have relevant tweets (following a mild cleaning), we need to work on the main subject of our analysis: the content of the tweets itself. This is necessary to ease the _Part 3_ of our research allowing us to process the text and find patterns: Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our treatment is to transform all the caracters to lowercase to be able to compare tweets easily without being bothered with case-sensitive searches. Following the same idea, we get rid of all the special characters complicating our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 en esperant que 2016 soit meilleur que 2015 quand meme\n",
       "1                                               nice bmw ...he kills ant s!!! pic.twitter.com/5zt5v1mljk\n",
       "2    @madmenna ich hab nichts dagegen, wenn mir andere beim saufen zuschauen, solange sie pro minute ...\n",
       "3     happy new jear!!!! i am back from 2 weeks cuba! let s go into the 2016! pic.twitter.com/bu0lbpfql3\n",
       "4    @megadriver16 bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un gra...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned['main'] = cleaned['main'].astype(str).str.lower().\\\n",
    "                    apply(lambda tweet: unicodedata.normalize('NFD', tweet).\\\n",
    "                    encode('ascii', 'ignore').decode('utf-8'))\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking to pictures). As we image processing is not in the scope of this project, we remove all \"pic.twitter\" URLS. After that, we take out all tweets containing a URL format. We also remove retweets, as they would "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 en esperant que 2016 soit meilleur que 2015 quand meme\n",
       "1                                                                         nice bmw ...he kills ant s!!! \n",
       "2    @madmenna ich hab nichts dagegen, wenn mir andere beim saufen zuschauen, solange sie pro minute ...\n",
       "3                               happy new jear!!!! i am back from 2 weeks cuba! let s go into the 2016! \n",
       "4    @megadriver16 bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un gra...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_mask = cleaned['main'].str.contains(\"www\\S+\") | cleaned['main'].str.contains(\"http\\S+\")\n",
    "cleaned['main'] = cleaned['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "cleaned.drop(cleaned[url_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove mentions to remove the usernames, which are meanling less and should not be considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(\"@\\S+\", '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we remove all non-alphanumeric characters, as they are will not give more information and might get in the way of the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 en esperant que 2016 soit meilleur que 2015 quand meme\n",
       "1                                                                               nice bmw he kills ant s \n",
       "2          ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen \n",
       "3                                     happy new jear i am back from 2 weeks cuba let s go into the 2016 \n",
       "4     bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...\n",
       "Name: main, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned['main'] = cleaned['main'].str.replace(r'[^\\w\\s]', '')\n",
    "cleaned.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing all these cleaning steps, the size of the set is significantly reduced (the new file only weights 2.4MB instead of the 55MB of the original file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Natural Language Processing (NLP) is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). Unlike the previous part, we only used local functions instead of a scaled up version using Spark.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important step in NLP is to divide the words of every tweet in a table to be able to easily treat them. It is easily done using the _'split'_ method provided in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main</th>\n",
       "      <th>published</th>\n",
       "      <th>source_spam_probability</th>\n",
       "      <th>source_location</th>\n",
       "      <th>tags</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author_gender</th>\n",
       "      <th>source_followers</th>\n",
       "      <th>source_following</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en, esperant, que, 2016, soit, meilleur, que, 2015, quand, meme]</td>\n",
       "      <td>2016-01-01 00:30:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saint-Maurice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>80</td>\n",
       "      <td>140</td>\n",
       "      <td>en esperant que 2016 soit meilleur que 2015 quand meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nice, bmw, he, kills, ant, s]</td>\n",
       "      <td>2016-01-01 12:12:33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>nice bmw he kills ant s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ich, hab, nichts, dagegen, wenn, mir, andere, beim, saufen, zuschauen, solange, sie, pro, minut...</td>\n",
       "      <td>2016-01-01 19:52:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Schenkon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>MALE</td>\n",
       "      <td>479</td>\n",
       "      <td>783</td>\n",
       "      <td>ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[happy, new, jear, i, am, back, from, 2, weeks, cuba, let, s, go, into, the, 2016]</td>\n",
       "      <td>2016-01-01 11:58:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>happy new jear i am back from 2 weeks cuba let s go into the 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bonne, annee, a, toi, je, prends, beaucoup, de, plaisir, a, suivre, tes, videos, je, suis, un, ...</td>\n",
       "      <td>2016-01-01 06:17:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bâle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>75</td>\n",
       "      <td>984</td>\n",
       "      <td>bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  main  \\\n",
       "0                                    [en, esperant, que, 2016, soit, meilleur, que, 2015, quand, meme]   \n",
       "1                                                                       [nice, bmw, he, kills, ant, s]   \n",
       "2  [ich, hab, nichts, dagegen, wenn, mir, andere, beim, saufen, zuschauen, solange, sie, pro, minut...   \n",
       "3                   [happy, new, jear, i, am, back, from, 2, weeks, cuba, let, s, go, into, the, 2016]   \n",
       "4  [bonne, annee, a, toi, je, prends, beaucoup, de, plaisir, a, suivre, tes, videos, je, suis, un, ...   \n",
       "\n",
       "            published  source_spam_probability source_location tags lang  \\\n",
       "0 2016-01-01 00:30:04                      0.0   Saint-Maurice  NaN   fr   \n",
       "1 2016-01-01 12:12:33                      0.0          Hinwil  NaN   en   \n",
       "2 2016-01-01 19:52:15                      0.0        Schenkon  NaN   de   \n",
       "3 2016-01-01 11:58:03                      0.0          Hinwil  NaN   en   \n",
       "4 2016-01-01 06:17:28                      0.0            Bâle  NaN   fr   \n",
       "\n",
       "  sentiment author_gender  source_followers  source_following  \\\n",
       "0   NEUTRAL        FEMALE                80               140   \n",
       "1   NEUTRAL       UNKNOWN                79               196   \n",
       "2   NEUTRAL          MALE               479               783   \n",
       "3   NEUTRAL       UNKNOWN                79               196   \n",
       "4   NEUTRAL       UNKNOWN                75               984   \n",
       "\n",
       "                                                                                                tweets  \n",
       "0                                               en esperant que 2016 soit meilleur que 2015 quand meme  \n",
       "1                                                                             nice bmw he kills ant s   \n",
       "2        ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen   \n",
       "3                                   happy new jear i am back from 2 weeks cuba let s go into the 2016   \n",
       "4   bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned['tweets'] = cleaned['main'] #keep tweets for later\n",
    "cleaned['main'] = cleaned['main'].str.split()\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stop words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have relevant tweets, it is necessary to remove useless words which would interfere with our analysis. We assumed stop words were the only important words to remove (as we already dealt with special characters and urls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main</th>\n",
       "      <th>published</th>\n",
       "      <th>source_spam_probability</th>\n",
       "      <th>source_location</th>\n",
       "      <th>tags</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author_gender</th>\n",
       "      <th>source_followers</th>\n",
       "      <th>source_following</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[esperant, 2016, meilleur, 2015, quand, meme]</td>\n",
       "      <td>2016-01-01 00:30:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saint-Maurice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>80</td>\n",
       "      <td>140</td>\n",
       "      <td>en esperant que 2016 soit meilleur que 2015 quand meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nice, bmw, kills, ant]</td>\n",
       "      <td>2016-01-01 12:12:33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>nice bmw he kills ant s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dagegen, beim, saufen, zuschauen, solange, pro, minute, 5, zahlen]</td>\n",
       "      <td>2016-01-01 19:52:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Schenkon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>MALE</td>\n",
       "      <td>479</td>\n",
       "      <td>783</td>\n",
       "      <td>ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[happy, new, jear, back, 2, weeks, cuba, let, go, 2016]</td>\n",
       "      <td>2016-01-01 11:58:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>happy new jear i am back from 2 weeks cuba let s go into the 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bonne, annee, a, prends, beaucoup, plaisir, a, suivre, videos, grand, fande, tour, france, merc...</td>\n",
       "      <td>2016-01-01 06:17:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bâle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>75</td>\n",
       "      <td>984</td>\n",
       "      <td>bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  main  \\\n",
       "0                                                        [esperant, 2016, meilleur, 2015, quand, meme]   \n",
       "1                                                                              [nice, bmw, kills, ant]   \n",
       "2                                  [dagegen, beim, saufen, zuschauen, solange, pro, minute, 5, zahlen]   \n",
       "3                                              [happy, new, jear, back, 2, weeks, cuba, let, go, 2016]   \n",
       "4  [bonne, annee, a, prends, beaucoup, plaisir, a, suivre, videos, grand, fande, tour, france, merc...   \n",
       "\n",
       "            published  source_spam_probability source_location tags lang  \\\n",
       "0 2016-01-01 00:30:04                      0.0   Saint-Maurice  NaN   fr   \n",
       "1 2016-01-01 12:12:33                      0.0          Hinwil  NaN   en   \n",
       "2 2016-01-01 19:52:15                      0.0        Schenkon  NaN   de   \n",
       "3 2016-01-01 11:58:03                      0.0          Hinwil  NaN   en   \n",
       "4 2016-01-01 06:17:28                      0.0            Bâle  NaN   fr   \n",
       "\n",
       "  sentiment author_gender  source_followers  source_following  \\\n",
       "0   NEUTRAL        FEMALE                80               140   \n",
       "1   NEUTRAL       UNKNOWN                79               196   \n",
       "2   NEUTRAL          MALE               479               783   \n",
       "3   NEUTRAL       UNKNOWN                79               196   \n",
       "4   NEUTRAL       UNKNOWN                75               984   \n",
       "\n",
       "                                                                                                tweets  \n",
       "0                                               en esperant que 2016 soit meilleur que 2015 quand meme  \n",
       "1                                                                             nice bmw he kills ant s   \n",
       "2        ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen   \n",
       "3                                   happy new jear i am back from 2 weeks cuba let s go into the 2016   \n",
       "4   bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stops('english')\n",
    "remove_stops('french')\n",
    "remove_stops('german')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of our NLP treatement pipeline is stemming. The idea is that words appearing in multiple forms (such as have, having, had, …) should only be considered once using their radical (e.g. \"hav\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    cleaned.loc[cleaned['lang'] == lang, 'main'] = cleaned.loc[cleaned['lang'] == lang, 'main'].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main</th>\n",
       "      <th>published</th>\n",
       "      <th>source_spam_probability</th>\n",
       "      <th>source_location</th>\n",
       "      <th>tags</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author_gender</th>\n",
       "      <th>source_followers</th>\n",
       "      <th>source_following</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[esper, 2016, meilleur, 2015, quand, mem]</td>\n",
       "      <td>2016-01-01 00:30:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saint-Maurice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>80</td>\n",
       "      <td>140</td>\n",
       "      <td>en esperant que 2016 soit meilleur que 2015 quand meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nice, bmw, kill, ant]</td>\n",
       "      <td>2016-01-01 12:12:33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>nice bmw he kills ant s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dageg, beim, sauf, zuschau, solang, pro, minut, 5, zahl]</td>\n",
       "      <td>2016-01-01 19:52:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Schenkon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>MALE</td>\n",
       "      <td>479</td>\n",
       "      <td>783</td>\n",
       "      <td>ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[happi, new, jear, back, 2, week, cuba, let, go, 2016]</td>\n",
       "      <td>2016-01-01 11:58:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hinwil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>79</td>\n",
       "      <td>196</td>\n",
       "      <td>happy new jear i am back from 2 weeks cuba let s go into the 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bon, anne, a, prend, beaucoup, plais, a, suivr, videos, grand, fand, tour, franc, merc, continu...</td>\n",
       "      <td>2016-01-01 06:17:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bâle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>75</td>\n",
       "      <td>984</td>\n",
       "      <td>bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  main  \\\n",
       "0                                                            [esper, 2016, meilleur, 2015, quand, mem]   \n",
       "1                                                                               [nice, bmw, kill, ant]   \n",
       "2                                            [dageg, beim, sauf, zuschau, solang, pro, minut, 5, zahl]   \n",
       "3                                               [happi, new, jear, back, 2, week, cuba, let, go, 2016]   \n",
       "4  [bon, anne, a, prend, beaucoup, plais, a, suivr, videos, grand, fand, tour, franc, merc, continu...   \n",
       "\n",
       "            published  source_spam_probability source_location tags lang  \\\n",
       "0 2016-01-01 00:30:04                      0.0   Saint-Maurice  NaN   fr   \n",
       "1 2016-01-01 12:12:33                      0.0          Hinwil  NaN   en   \n",
       "2 2016-01-01 19:52:15                      0.0        Schenkon  NaN   de   \n",
       "3 2016-01-01 11:58:03                      0.0          Hinwil  NaN   en   \n",
       "4 2016-01-01 06:17:28                      0.0            Bâle  NaN   fr   \n",
       "\n",
       "  sentiment author_gender  source_followers  source_following  \\\n",
       "0   NEUTRAL        FEMALE                80               140   \n",
       "1   NEUTRAL       UNKNOWN                79               196   \n",
       "2   NEUTRAL          MALE               479               783   \n",
       "3   NEUTRAL       UNKNOWN                79               196   \n",
       "4   NEUTRAL       UNKNOWN                75               984   \n",
       "\n",
       "                                                                                                tweets  \n",
       "0                                               en esperant que 2016 soit meilleur que 2015 quand meme  \n",
       "1                                                                             nice bmw he kills ant s   \n",
       "2        ich hab nichts dagegen wenn mir andere beim saufen zuschauen solange sie pro minute 5 zahlen   \n",
       "3                                   happy new jear i am back from 2 weeks cuba let s go into the 2016   \n",
       "4   bonne annee a toi je prends beaucoup de plaisir a suivre tes videos je suis un grand fande tour...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words('english')\n",
    "stem_words('french')\n",
    "stem_words('german')\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dictionary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we applied all of these methods to our dataset, it is only natural that we do the same for the dictionary we will use. The first step of this part explains how we built the dictionary (and cleaned it as we did before) while the second part focuses on applying the previous NLP methods to the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems, such as can be seen in [2], [3]. Starting from this, we built our own dictionary by expanding the examples (as we expanded our subject to take into account multiple mental disorders instead of simply determining tweets of users diagnosed with clinical illnesses). As we treat tweets in multiple languages, we also translated all the terms and tried adding words that specifically target mood (or eating) disorders in french and german.\n",
    "\n",
    "_Note: you can take a look at this dictionary by clicking on the 4th reference (at the end of the file)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>german</th>\n",
       "      <th>swiss_german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abusive</td>\n",
       "      <td>abusif</td>\n",
       "      <td>missbrauch</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>addict</td>\n",
       "      <td>accro</td>\n",
       "      <td>sucht</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afraid</td>\n",
       "      <td>peur</td>\n",
       "      <td>angst</td>\n",
       "      <td>angscht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agoraphobe</td>\n",
       "      <td>agoraphobe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agoraphobia</td>\n",
       "      <td>agoraphobie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       english       french      german swiss_german\n",
       "0      abusive       abusif  missbrauch          NaN\n",
       "1       addict        accro       sucht          NaN\n",
       "2       afraid         peur       angst      angscht\n",
       "3   agoraphobe   agoraphobe         NaN          NaN\n",
       "4  agoraphobia  agoraphobie         NaN          NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT_PATH = \"dictionary.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = pd.concat([dictionaries['german'].dropna(), dictionaries['swiss_german'].dropna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching our 3 datasets, we clean them as we did for the tweets in _Part 2_. However, as we created them ourselves, we only have to lowercase them and treat the special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_cleaning(lang):\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_dict = lang_dict.astype(str).str.lower().\\\n",
    "                        apply(lambda expression: unicodedata.normalize('NFD', expression).\\\n",
    "                        encode('ascii', 'ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cleaning('en')\n",
    "dict_cleaning('fr')\n",
    "dict_cleaning('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries. \n",
    "\n",
    "_Note: As all methods were explained above, we will not dwell on each step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "en_dict = en_dict.str.split()\n",
    "fr_dict = fr_dict.str.split()\n",
    "de_dict = de_dict.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_set = stopwords.words(language)\n",
    "        \n",
    "    lang_dict = lang_dict.apply(lambda expression: [word for word in expression if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "dict_remove_stops('english')\n",
    "dict_remove_stops('french')\n",
    "dict_remove_stops('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    \n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    lang_dict = lang_dict.apply(lambda expression: [stemmer.stem(word) for word in expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the words\n",
    "dict_stem_words('english')\n",
    "dict_stem_words('french')\n",
    "dict_stem_words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the soundness of our method, we display the head of each dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [abusive]\n",
       "1         [addict]\n",
       "2         [afraid]\n",
       "3     [agoraphobe]\n",
       "4    [agoraphobia]\n",
       "Name: english, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [abusif]\n",
       "1          [accro]\n",
       "2           [peur]\n",
       "3     [agoraphobe]\n",
       "4    [agoraphobie]\n",
       "Name: french, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [missbrauch]\n",
       "1           [sucht]\n",
       "2           [angst]\n",
       "5    [alcoholismus]\n",
       "6           [alein]\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we cross the dataset with our dictionaries to retrieve the tweets exhibiting mental distress. This is the first (less naive) step of our analysis before running the Machine Learning algorithms on our data.\n",
    "\n",
    "We first transfrom the lists back to string sentences, for efficacy and code readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.main = cleaned.main.map(lambda x:  ' '.join(x))\n",
    "de_dict = de_dict.map(lambda x:  ' '.join(x))\n",
    "en_dict = en_dict.map(lambda x:  ' '.join(x))\n",
    "fr_dict = fr_dict.map(lambda x:  ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we can find a dict entry for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict(tweet, dict_):\n",
    "    \"\"\"checks if dict entry matches tweet\"\"\"\n",
    "    match = [ w for w in dict_ if w in tweet] #find matching for each entry\n",
    "    return len(match) > 0 #at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language.\n",
    "\n",
    "\n",
    "For english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210     i feel sad that they have to hide and probably wont ever be able to go out and have fun like a ...\n",
       "279     it become a scandal like if dating someone is wrongand they may loose fansor getting hate from ...\n",
       "314                                                 i hope he will take care of my girl dont hurt her huh \n",
       "316                                   i hope people will react positively but im afraid about his fangirls\n",
       "349    but hanis dating news makes me feel sad because its not normal that idols have to hide their rel...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_filtered = cleaned[cleaned.lang == 'en']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'en']['main'].map(lambda x: check_dict(x, en_dict))]\n",
    "english_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see sadness an lonelyness or fear in this tweets, but some of them do not actualy express distress.\n",
    "We also note that the number of tweets has greatly been reduces through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the french tweets and again find similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310     je te comprends tellement cest vraiment se foutre de la gueule des lecteurs milady sont les roi...\n",
       "330                                                                                suis je le seul debout \n",
       "343                         je crois je vais allez regarder les feu dartifices toute seule au bord de leau\n",
       "411                jai ajoute 60 livres dans ma boutique priceminister cest ezila4 fouillez il y a de tout\n",
       "588               trouve le temps de passer sur le mumble et le voit avec personnes de present tristesse d\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_filtered = cleaned[cleaned.lang == 'fr']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'fr']['main'].map(lambda x: check_dict(x, fr_dict))]\n",
    "french_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for german we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65     rtl bringt das kunststuck fertig eine klassische pleitenpechundpannensendung in den sand zu setz...\n",
       "426                                                            das war er nun der schlechteste tatort ever\n",
       "458    angekundigte katastrophen bleiben in der regel aus wem nutzt die angstmache eigentlich mehr den ...\n",
       "548                        nur so wenn du schlaftst die zeit auch vergeht und das meist sogar schmerzfrei \n",
       "558                                                         nei nei keine angst ich reise nicht mehr gerne\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_filtered = cleaned[cleaned.lang == 'de']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'de']['main'].map(lambda x: check_dict(x, de_dict))]\n",
    "german_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do all of this tweets reference the person, but they also talk about sadness or dissapointment.\n",
    "The remaining number is quite small, compared to the initial 4000 german tweets in the dataset, but it is a number we migth expect.\n",
    "\n",
    "Still those results could be better, which is why we decide to introduce a step 4 into our analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML (to be done in milestone 3)\n",
    "\n",
    "The final part of our data processing involves training a supervised Machine Learning classifier to get better results, as missclasifications are still somewhat frequent. Similar projects routinely use this [2] [3] [8].\n",
    "\n",
    "All these methods are implemented in scikit-learn and have an equivalent in Spark in case our computers cannot handle the size of the data, allowing us to perform these steps on the cluster.\n",
    "\n",
    "In order to do this, we go through the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Labeling the tweets\n",
    "\n",
    "We need to label a subset of tweets as 'mentaly distressed' or not. Ideally around 6000 per language [7], but due to limited time and manpower, we will limit ourself to 1000 tweets each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Construcing features TF-IDF\n",
    "\n",
    "Once we have the labeled set, we transfrom it in order to have features to use for the ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train SVM classifier\n",
    "\n",
    "According to [7], SVM along with TF-IDF tends to preform very well, especially with limited training set size (which is ideal in our case). We perform a binary classification on the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Relabel the training set\n",
    "\n",
    "Using the previously attained classifier, we relabel the set and discard tweets that are labeled as 'undistressed'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Data Analysis\n",
    "\n",
    "In the final analysis, we try to analyze and visualize the results in order to answer our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 General analysis\n",
    "\n",
    "We compare the set found using Machine Learning to the general cleaned set and the overall set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Finding related indicatior of mental health issues using LDA\n",
    "\n",
    "As discussed in [8], LDA one of the best ways to find relevant related words in tweets. Again, we plan on using the implemented scikit-learn version to preform this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Lookint at gender differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tweet frequency and type over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Comparing our results to census data\n",
    "\n",
    "This will be approximative since we could not find a dataset containing this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Comparing the languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Example dataprocessing pipeline](http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] [Dissertation containing a first dict](https://www.rand.org/content/dam/rand/pubs/rgs_dissertations/RGSD300/RGSD391/RAND_RGSD391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] [Second thesis containing dict](https://getd.libs.uga.edu/pdfs/kale_sayali_s_201512_ms.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] [Our own dictionary](https://docs.google.com/spreadsheets/d/1WwI9crZk36pcTOQ1g_5dumMd11OlkpFRNHsEvpkwLMk/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] [Stemming with Spark](https://github.com/master/spark-stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] [ML methods in Spark](https://spark.apache.org/docs/2.1.0/ml-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] [Public health paper using LDA](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2880/3264)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] [Best ways to do Text Classification](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
