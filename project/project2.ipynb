{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we describe the multiple steps of our preprocessing pipeline and the work done on the Twitter dataset to answer our research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li><b>Data Collection & Preliminary Analysis:</b> how we accessed the datasets and why we use them</li>\n",
    "  <li><b>Data Treatment:</b> cleaning and NLP on the first dataset</li>\n",
    "  <li><b>Datasets Description for Spinn3r:</b> a first look at the Spinn3r Dataset</li>\n",
    "  <li><b>Dictionary Construction:</b> creation of the dictionary we are going to us on the datasets</li>\n",
    "  <li><b>Dictionary Update:</b> updating the dictionnary with what we learned thanks to the dataset</li>\n",
    "  <li><b>Analysis of First Dataset:</b> anaysis of the smaller Spinn3r set for a proof of concept</li>\n",
    "  <li><b>Processing Pipeline on the Second Dataset:</b> apply what we learned on the previous dataset on the second one</li>\n",
    "  <li><b>Analysis of the Second Dataset:</b> applying what we learned on the smaller dataset on the swisscom-twitter dataset</li>\n",
    "  <li><b>Conclusion</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Collection & Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete twitter data for both datsets was collected from the cluster provided in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example dataset of (_**twitter-swisscom**_) was retrieved from a .zip file provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset (_**Spinn3r**_), however, did not provide a specific example datset. As a standing we extracted the first JSON file to perform our analysis. To do this, we used the following comands after connecting to the cluster:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis\n",
    "\n",
    "This dataset has an elaborate description of each field available on the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema). Given the amount of data present in the cluster, we only look at one day to perform our first analysis (we will later show how we scale our operations using Spark).\n",
    "\n",
    "Unlike the previous dataset, this dataset is given in JSON format (with nested elements). As we could not find how to extract all the data directly using the *read_json* function provided, we use a JSON normalizer (provided in the *Pandas* libary). \n",
    "\n",
    "_Note: We will later see that Spark deals better with nested JSON._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these columns, we find the ones below to be the most useful for our analysis:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of the tweet.\n",
    "- **tags**: tags associated with the tweet (provided by Spinn3r).\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of the user who tweeted.\n",
    "- **source_following**: number of people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider these tags to be the most useful as:\n",
    "\n",
    "- **Main** stands at the center of our analysis. We plan to perform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "- **Published** can be used to map the tweets over the duration of the year and look for seasonal changes.\n",
    "- **Source_location** can be used to look at the geographical distribution of the tweets.\n",
    "- **Lang** will be used to filter out unwanted languages, which we need to do in order to perform our NLP tasks.\n",
    "- **Author_gender** will be used to identify the gender and look at the differences between both gend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ that on the cluster we find additional fields, out of which we decide to use **'author_user_id'**, as means to uniquely identify an user and **'geo_point'**, containing latitude and longitude, as a way to identify user location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "twitter_df['source_spam_probability'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probability of the tweet set, we see that not a single tweet was labeled as spam. This makes us question the accuracy of the labeling as the set of tweets on that day most certainly contains spam. However, we will still use it as we assume the chance of having false positives to be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10])\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the location seems to be language dependent, we only care about tweets written in the 3 languages we need. We see that:\n",
    "- A lot of locations only differ in language (e.g. as Switzerland and Schweiz)\n",
    "- The name of the locations are not always given languages we are interested in (e.g. สวิตเซอร์แลนด์)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- As opposed to dataset 1, all tweets are located in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values in the dataset, we see that for most tweets, the tags are missing. This may indicate that the tags won't be usefull for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information. As we will filter out such tweets, the remaining set should contain sentiment. Even if doesn't, this field is not central to our analysis, it is merely used to help us filter our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying potential issues\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we immediatly see that the tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more intently at the tweets containing links, we can make the assumption that the relationship between URLs and spam is a general rule (at any time of the year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing a simplistic dictionary matching using the occurences of the word *'suicide'* in our tweet set, we see that a lot of these tweets contain news. This further comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word *'therapy'* (in German), we can confirm once again the issue there is with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to get an idea on how useful they could be. From what we see, and what others have seen [8], they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean our dataset to be able to use it correctly for our research.\n",
    "\n",
    "First, we use Pandas locally on a small subset of the tweets to explore different cleaning methods and make sure the functions we chose work as expected. After proving that our concept works, we use Spark to scale up our operations and be able to perform them on all the files provided on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet filtering\n",
    "\n",
    "Given the context of our research, there are many useless tweets. \n",
    "- First, we would like to get rid of any tweets with a spam probability greater than 0.5 (an arbitrary value we found sound). \n",
    "- Finally, as we are focused on the Swiss population, we only keep tweets in the official languages (except for Italian as no one in our group speaks the language, meaning we won't be able to extract useful information) and in English (as most tweets in the dataset, and more generally on Twitter, are written in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mask = (twitter_df['source_spam_probability'] >= 0.5)\n",
    "twitter_df.drop(twitter_df[spam_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_mask = ~twitter_df.lang.isin(['de', 'en', 'fr'])\n",
    "twitter_df.drop(twitter_df[lang_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have relevant tweets (following a mild cleaning), we need to work on the main subject of our analysis: the content of the tweets itself. This is necessary to ease the _3rd Part_ of our research (processing the text to find patterns using NLP, _Natural Language Processing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make later temporal analysis easier we transform the column containing the dates into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['published'] = pd.to_datetime(twitter_df['published'])\n",
    "twitter_df['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our treatment is to lower all the caracters (allows comparing the tweets without case-sensitive searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['main'] = twitter_df['main'].str.lower()\n",
    "twitter_df['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking pictures to the tweet). As image processing is not in the scope of this project, we remove all \"pic.twitter\" URLS and mentions of the format \"@user\". After that, we remove all tweets containing a URL and retweets as they do not give us any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_mask = twitter_df['main'].str.contains(\"www\\S+\") | twitter_df['main'].str.contains(\"http\\S+\")\n",
    "twitter_df['main'] = twitter_df['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "twitter_df.drop(twitter_df[url_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)\n",
    "twitter_df['main'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanum_filter = lambda data: re.sub(r'[^\\w\\s]', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['main'] = twitter_df['main'].map(alphanum_filter)\n",
    "twitter_df['main'] = twitter_df['main'].str.replace('_', '') #not removed by following regex\n",
    "twitter_df.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark version\n",
    "\n",
    "After performing all these cleaning steps, the size of the set is **significantly reduced** (the new file only weights **2.4MB** instead of the **55MB** of the original file, allowing us to reduce the set by **30x**). This is particularly helpful as the whole dataset represents around **30GB**.\n",
    "\n",
    "_Note:_ the code was defined as _**Raw NBConvert**_ as it is not meant to be run on the notebook but on the cluster. The code is only here as information for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary elements and defining Spark environment variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lower, explode\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json('/datasets/swiss-tweet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import our data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#remove nesting\n",
    "df = df.select('_source.*')\n",
    "\n",
    "#list and keep relevant columns\n",
    "columns = ['main', 'author_user_id','published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following', 'geo_point']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter tweets using the same method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = df.filter(df.source_spam_probability < 0.5) #remove spam\n",
    "df = df.filter(df.lang.isin('en', 'de', 'fr'))  #gets tweets with right language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text treatment is a little longer as it is more thorough in Spark."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#lower the characters\n",
    "df = df.withColumn('main', lower(df.main))\n",
    "\n",
    "#replace picture urls and mentions\n",
    "df = df.withColumn('main', regexp_replace(col('main'), 'pic.twitter\\S+', ' '))\n",
    "df = df.withColumn('main', regexp_replace(col('main'), '@\\S+', ' '))\n",
    "\n",
    "#remove tweets mentioning websites and retweets\n",
    "df = df.where(~df.main.like(\"%http%\"))\n",
    "df = df.where(~df.main.like(\"%.com%\"))\n",
    "df = df.where(~df.main.like(\"%.ch%\"))\n",
    "df = df.where(~df.main.like(\"%www%\"))\n",
    "df = df.where(~df.main.like(\"%rt%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the data in the _JSON_ we will be using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.write.json('tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description for Spinn3r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets. They were formated differently and contained different fields. On top of this, while the first dataset contained data collected over multiple years (twitter-swisscom, refered to as second dataset), the other only covers a span of 10 months (Spinn3r, refered to as first dataset).\n",
    "\n",
    "The goal of our first analysis was thus to decide which dataset would be more useful but also to familiarize ourselves with the chosen dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "After the analysis (presented below), we decided to use **both datasets** for our project. While the twitter-swisscom contains more precise location information (longitude and latitude), the Spinn3r dataset contains a sentiment analysis field as well as a language field and has a lot less data. Thus, we decided to create our functions and methods using the smaller dataset (Spinn3r) and use it as a proof of concept before going on a larger scale and use the swisscom-twitter dataset to further our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pkl.load( open( \"processed_tweets.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at how many values we have for each field. The total number of tweets is $3936084$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['lang'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['author_gender'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, we have a significant number of profiles where the gender is documented meaning we could use these to look at the differences in mental distress between the genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "twitter_df['sentiment'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring functions\n",
    "#May not be needed\n",
    "\n",
    "def get_time_distribution(times):\n",
    "    publishing_time = times.map(lambda x: x.hour).value_counts() / times.count()\n",
    "    publishing_time.sort_index(inplace=True)\n",
    "    return publishing_time\n",
    "\n",
    "def plot_daily(depressed):\n",
    "    neg = get_time_distribution(depressed[depressed.sentiment == 'NEGATIVE']['published'])\n",
    "    neut = get_time_distribution(depressed[depressed.sentiment == 'NEUTRAL']['published'])\n",
    "    pos = get_time_distribution(depressed[depressed.sentiment == 'POSITIVE']['published'])\n",
    "\n",
    "    plt.plot(neg)\n",
    "    plt.plot(neut)\n",
    "    plt.plot(pos)\n",
    "    plt.legend(['negative', 'neutral', 'positive'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily(twitter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearly(times):\n",
    "    publishing_time = times.map(lambda x: x.month).value_counts() / times.count()\n",
    "#     publishing_time.sort_index(inplace=True)\n",
    "    return publishing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = get_yearly(twitter_df.published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(data=yearly_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP _(Natural Language Processing)_ is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). Unlike the previous part, we only used local functions.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "(cf. _Next part_ below for information)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def remove_stops(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    lang_set = stopwords.words(language)\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def stem_words(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is mainly based on treating the data to obtain an adequate format to work with following 3 steps:\n",
    "- Tokenizing: separating words \n",
    "- Stop-words removal\n",
    "- Stemming: keeping only the radical of a word (allows better comparison)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tknzr = TweetTokenizer()\n",
    "twitter_df['tokenized'] = twitter_df['main'].map(lambda x: tknzr.tokenize(x))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "remove_stops('english', df, 'tokenized')\n",
    "remove_stops('french', df, 'tokenized')\n",
    "remove_stops('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stem_words('english', df, 'tokenized')\n",
    "stem_words('french', df, 'tokenized')\n",
    "stem_words('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems, such as can be seen in [2], [3]. Starting from this, we built our own English dictionary _($4^{th}$ reference in the Bibliography at the end of the file)_ and carefully translated it in French and German.\n",
    "\n",
    "_Note_: a more complete explanation of our dictionary construction is included in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_PATH = \"dictionary.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = pd.concat([dictionaries['german'].dropna(), dictionaries['swiss_german'].dropna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching our 3 datasets, we clean them as we did for the tweets in _Part 2_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = en_dict.map(lambda x: tknzr.tokenize(x))\n",
    "fr_dict = fr_dict.map(lambda x: tknzr.tokenize(x))\n",
    "de_dict = de_dict.map(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries. \n",
    "\n",
    "_Note_: Special methods were written for the dictionaries as they do not follow the same format as tweets, but they are the same in essence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_cleaning(lang):\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_dict = lang_dict.astype(str).str.lower()\n",
    "\n",
    "def dict_remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_set = stopwords.words(language)\n",
    "    lang_dict = lang_dict.apply(lambda expression: [word for word in expression if word not in lang_set])\n",
    "\n",
    "def dict_stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    lang_dict = lang_dict.apply(lambda expression: [stemmer.stem(word) for word in expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_remove_stops('english')\n",
    "dict_remove_stops('french')\n",
    "dict_remove_stops('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_stem_words('english')\n",
    "dict_stem_words('french')\n",
    "dict_stem_words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the soundness of our method, we display the head of each dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we cross the dataset with our dictionaries to retrieve the tweets exhibiting mental distress. This is the first (less naive) step of our analysis before using Topic Modeling methods on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we can find a dict entry for each tweet, by using the following algorithm, that checks if a keyword occurs at least once in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, t):\n",
    "    return Counter(s) == Counter(t)\n",
    "\n",
    "def match_dict(tweet, dict_):\n",
    "    \"\"\"returns keywords that match in string\n",
    "    tweet: tweet to find keywords in\n",
    "    dict_: list of keywords\n",
    "    \"\"\"\n",
    "    doc = [sublist for sublist in dict_ if compare(list(filter(lambda x: x in tweet, sublist)), sublist) ]\n",
    "    return doc#at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language.\n",
    "\n",
    "\n",
    "For english:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "english_filtered = twitter_df[twitter_df.lang == 'en']['tokenized'].map(lambda x: match_dict(x, en_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from keywordfile and show head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see sadness an loneliness or fear in these tweets, but some of them do not actualy express distress.\n",
    "We also note that the number of tweets has greatly been reduces through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the French tweets and again find similar results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "french_filtered = df[df.lang == 'fr']['tokenized'].map(lambda x: match_dict(x, fr_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from keywordfile and show head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for German we get the following results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "german_filtered = df[df.lang == 'de']['tokenized'].map(lambda x: match_dict(x, de_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from keywordfile and show head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do all of this tweets reference the person, but they also talk about sadness or dissapointment.\n",
    "The remaining number is quite small, compared to the initial 4000 german tweets in the dataset, but it is a number we might expect.\n",
    "\n",
    "Still those results could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dictionary Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from Dataset 1 to Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting is as follows: **Column in Dataset 2** -> Column in Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **author_user_id**           -> userId\n",
    "- **geo_point**                -> longitude + latitude (or: placeLongitude + placeLatitude)\n",
    "- **main**                     -> text\n",
    "- **published**                -> createdAt\n",
    "- **source_followers**         -> followersCount\n",
    "- **source_following**         -> friendsCount (or is it folowwers that are following ?)\n",
    "- **source_location**          -> userLocation\n",
    "\n",
    "\n",
    "#### Dataset 1 having no equivalent : \n",
    "- id                                -> tweet ID\n",
    "- truncated                         -> no idea, only NaNs\n",
    "- placeId                           -> ID of where they are\n",
    "- inReplyTo                         -> If it is a tweet reply\n",
    "- source & sourceName, sourceUrl    -> what user used to send tweet (Android, website, and so on)\n",
    "- userName                          -> username\n",
    "- screenName                        -> name shown on tweets from username\n",
    "- statusesCount                     -> number of status of user\n",
    "\n",
    "#### Dataset 2 having no equivalent : \n",
    "- **author_gender**                 -> author's gender\n",
    "- **lang**                          -> language of tweet (given)\n",
    "- **sentiment**                     -> tweet's sentiment analysis (given)\n",
    "- **source_spam_probability**       -> spam probability of tweet (given)\n",
    "- **tags**                          -> hashtags contained in the tweet (in Dataset 1 stays in text)\n",
    "- **tokenized**                     -> tokenized text of tweet (done by us)\n",
    "- **keywords**                      -> keywords fund in tweets (done by us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processing Pipeline on the Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of the Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
