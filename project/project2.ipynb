{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we describe the multiple steps of our preprocessing pipeline and the work done on the Twitter dataset to answer our research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li><b>Data Collection:</b> how we accessed the datasets</li>\n",
    "  <li><b>Datasets Description:</b> a first look at the datasets</li>\n",
    "  <li><b>Data Treatment:</b> cleaning based on the results found in <i>Part 1</i> and applying NLP methods to our data</li>\n",
    "  <li><b>Dictionary Construction:</b> creation of the dictionary we are going to us on the datasets</li>\n",
    "  <li><b>Topic Modeling and Analysis of First Dataset:</b> anaysis of the smaller Spinn3r set for a proof of concept</li>\n",
    "  <li><b>Analysis of the Second Dataset:</b> applying what we learned on the smaller dataset on the swisscom-twitter dataset</li>\n",
    "  <li><b>Final Findings:</b> trying to find meaning in the final dataset we retrieve</li>\n",
    "  <li><b>Conclusion</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not retrieve both datasets using the same method, as they did not have the same weight, but also because they were not provided on the same platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset (_**twitter-swisscom**_) was retrieved from a .zip file. Thus, we had access to the entire dataset quickly allowing us to have an overview of all the tweets when analyzing it (cf _Part 1_ below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset (_**Spinn3r**_), however, was retrieved from the cluster. We first tried to retrieve the whole dataset but quickly realized it would be impossible to do so (it was very heavy and took a long time to be downloaded). Thus, we only extracted the first JSON file to perform our analysis. To do this, we used the following methods:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets. They were formated differently and contained different fields. On top of this, while the first dataset contained data collected over multiple years (twitter-swisscom, refered to as second dataset), the other only covers a span of 10 months (Spinn3r, refered to as first dataset).\n",
    "\n",
    "The goal of our first analysis was thus to decide which dataset would be more useful but also to familiarize ourselves with the chosen dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "After the analysis (presented below), we decided to use **both datasets** for our project. While the twitter-swisscom contains more precise location information (longitude and latitude), the Spinn3r dataset contains a sentiment analysis field as well as a language field and has a lot less data. Thus, we decided to create our functions and methods using the smaller dataset (Spinn3r) and use it as a proof of concept before going on a larger scale and use the swisscom-twitter dataset to further our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Dataset (Spinn3r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata analysis\n",
    "\n",
    "This dataset has an elaborate description of each field available on the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema). Given the amount of data present in the cluster, we only look at one day to perform our first analysis (we will later show how we scale our operations using Spark).\n",
    "\n",
    "Unlike the previous dataset, this dataset is given in JSON format (with nested elements). As we could not find how to extract all the data directly using the *read_json* function provided, we use a JSON normalizer (provided in the *Pandas* libary). \n",
    "\n",
    "_Note: We will later see that Spark deals better with nested JSON._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "TRUE_PATH = 'reduced_tweets.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these columns, we find the ones below to be the most useful for our analysis:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of the tweet.\n",
    "- **tags**: tags associated with the tweet (provided by Spinn3r).\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of the user who tweeted.\n",
    "- **source_following**: number of people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider these tags to be the most useful as:\n",
    "\n",
    "- **Main** stands at the center of our analysis. We plan to perform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "- **Published** can be used to map the tweets over the duration of the year and look for seasonal changes.\n",
    "- **Source_location** can be used to look at the geographical distribution of the tweets.\n",
    "- **Lang** will be used to filter out unwanted languages, which we need to do in order to perform our NLP tasks.\n",
    "- **Author_gender** will be used to identify the gender and look at the differences between both gend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Distribution\n",
    "\n",
    "While this example isn't representative (especially given that it contains the tweets posted on January, 1st), it can still give us insights on other fields. We assume that roughly the same categories of users were active on that day, meaning we can draw conclusions on the distribution of language and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[30, 20])\n",
    "explode = (0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, \n",
    "           0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "twitter_df['lang'].value_counts().plot(kind='pie', explode=explode, cmap=plt.cm.terrain)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that English, French and German are the most frequent languages. This is good as those are the languages we plan on using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['author_gender'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, we have a significant number of profiles where the gender is documented meaning we could use these to look at the differences in mental distress between the genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "explode = (0, 0.2, 0.4)\n",
    "twitter_df['sentiment'].value_counts().plot(kind='pie', explode=explode)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that the vast majority of tweets are labeled as neutral while only a very small number is labeled as negative. Given our subject will look at both the **'NEUTRAL'** and **'NEGATIVE'** tweets.\n",
    "\n",
    "_Note: we make the assumption that there are no false positives, meaning that a tweet showing signs of mental distress will not be labeled as **'POSITIVE'**. This means we can safely exclude these tweets from further analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "twitter_df['source_spam_probability'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probability of the tweet set, we see that not a single tweet was labeled as spam. This makes us question the accuracy of the labeling as the set of tweets on that day most certainly contains spam. However, we will still use it as we assume the chance of having false positives to be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10])\n",
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts().plot(kind='pie')\n",
    "plt.legend(labels=twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the location seems to be language dependent, we only care about tweets written in the 3 languages we need. We see that:\n",
    "- A lot of locations only differ in language (e.g. as Switzerland and Schweiz)\n",
    "- The name of the locations are not always given languages we are interested in (e.g. สวิตเซอร์แลนด์)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- As opposed to dataset 1, all tweets are located in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values in the dataset, we see that for most tweets, the tags are missing. This may indicate that the tags won't be usefull for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information. As we will filter out such tweets, the remaining set should contain sentiment. Even if doesn't, this field is not central to our analysis, it is merely used to help us filter our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6, 5])\n",
    "twitter_df[~(twitter_df.sentiment.isin(['POSITIVE', 'NEGATIVE', 'NEUTRAL']))]['lang'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure this won't be an issue, we quickly look at all the uncategorized tweets and see that the language is unkown as well, meaning they will all be removed anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential issues\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we immediatly see that the tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more intently at the tweets containing links, we can make the assumption that the relationship between URLs and spam is a general rule (at any time of the year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing a simplistic dictionary matching using the occurences of the word *'suicide'* in our tweet set, we see that a lot of these tweets contain news. This further comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word *'therapy'* (in German), we can confirm once again the issue there is with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to get an idea on how useful they could be. From what we see, and what others have seen [8], they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Dataset (twitter-swisscom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean our dataset to be able to use it correctly for our research.\n",
    "\n",
    "First, we use Pandas locally on a small subset of the tweets to explore different cleaning methods and make sure the functions we chose work as expected. After proving that our concept works, we use Spark to scale up our operations and be able to perform them on all the files provided on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data\n",
    "\n",
    "This part follows what exactly has already been done during Data Exploration, meaning normalizing the provided _JSON_ file and only keeping the columns we are interested in. The final step in importing the data is formatting the date of publication in the tweets for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = json_normalize(data)\n",
    "cleaned.columns = [column.replace('_source.','') for column in cleaned.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['published'] = pd.to_datetime(cleaned['published'])\n",
    "cleaned['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet filtering\n",
    "\n",
    "Given the context of our research, there are many useless tweets. \n",
    "- First, we would like to get rid of any tweets with a spam probability greater than 0.5 (an arbitrary value we found sound). \n",
    "- Moreover, we only want to keep tweets marked as **NEUTRAL** or **NEGATIVE** as we are conducting a study on mental distress in tweets. \n",
    "- Finally, as we are focused on the Swiss population, we only keep tweets in the official languages (except for Italian as no one in our group speaks the language, meaning we won't be able to extract useful information) and in English (as most tweets in the dataset, and more generally on Twitter, are written in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_mask = (cleaned['source_spam_probability'] >= 0.5)\n",
    "cleaned.drop(cleaned[spam_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_mask = (cleaned['sentiment'] == 'POSITIVE')\n",
    "cleaned.drop(cleaned[sent_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_mask = ~cleaned.lang.isin(['de', 'en', 'fr'])\n",
    "cleaned.drop(cleaned[lang_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have relevant tweets (following a mild cleaning), we need to work on the main subject of our analysis: the content of the tweets itself. This is necessary to ease the _3rd Part_ of our research (processing the text to find patterns using NLP, _Natural Language Processing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our treatment is to lower all the caracters (allows comparing the tweets without case-sensitive searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cleaning = lambda data: unicodedata.normalize('NFD', data.lower()).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].map(data_cleaning)\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking pictures to the tweet). As image processing is not in the scope of this project, we remove all \"pic.twitter\" URLS and mentions of the format \"@user\". After that, we remove all tweets containing a URL and retweets as they do not give us any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_mask = cleaned['main'].str.contains(\"www\\S+\") | cleaned['main'].str.contains(\"http\\S+\")\n",
    "cleaned['main'] = cleaned['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "cleaned.drop(cleaned[url_mask].index, inplace=True)\n",
    "cleaned.reset_index(drop=True, inplace=True)\n",
    "cleaned['main'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphanum_filter = lambda data: re.sub(r'[^\\w\\s]', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['main'] = cleaned['main'].map(alphanum_filter)\n",
    "cleaned.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark version\n",
    "\n",
    "After performing all these cleaning steps, the size of the set is **significantly reduced** (the new file only weights **2.4MB** instead of the **55MB** of the original file, allowing us to reduce the set by **30x**). This is particularly helpful as the whole dataset represents around **30GB**.\n",
    "\n",
    "_Note:_ the code was defined as _**Raw NBConvert**_ as it is not meant to be run on the notebook but on the cluster. The code is only here as information for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary elements and defining Spark environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lower, explode\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json('/datasets/swiss-tweet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove nesting\n",
    "df = df.select('_source.*')\n",
    "\n",
    "#list and keep relevant columns\n",
    "columns = ['main', 'author_user_id','published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following', 'geo_point']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter tweets using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.filter(df.source_spam_probability < 0.5) #remove spam\n",
    "df = df.filter(df.sentiment != 'POSITIVE') #remove positive sentient tweets\n",
    "df = df.filter(df.lang.isin('en', 'de', 'fr'))  #gets tweets with right language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text treatment is a little longer as it is more thorough in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lower the characters\n",
    "df = df.withColumn('main', lower(df.main))\n",
    "\n",
    "#replace picture urls and mentions\n",
    "df = df.withColumn('main', regexp_replace(col('main'), 'pic.twitter\\S+', ' '))\n",
    "df = df.withColumn('main', regexp_replace(col('main'), '@\\S+', ' '))\n",
    "\n",
    "#remove tweets mentioning websites and retweets\n",
    "df = df.where(~df.main.like(\"%http%\"))\n",
    "df = df.where(~df.main.like(\"%.com%\"))\n",
    "df = df.where(~df.main.like(\"%.ch%\"))\n",
    "df = df.where(~df.main.like(\"%www%\"))\n",
    "df = df.where(~df.main.like(\"%rt%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the data in the _JSON_ we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.write.json('reduced_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems, such as can be seen in [2], [3]. Starting from this, we built our own English dictionary _($4^{th}$ reference in the Bibliography at the end of the file)_ and carefully translated it in French and German.\n",
    "\n",
    "_Note_: a more complete explanation of our dictionary construction is included in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DICT_PATH = \"dictionary.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = pd.concat([dictionaries['german'].dropna(), dictionaries['swiss_german'].dropna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching our 3 datasets, we clean them as we did for the tweets in _Part 2_. However, as we created them ourselves, we only have to lowercase them and treat the special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dict = en_dict.apply(data_cleaning).str.split()\n",
    "fr_dict = fr_dict.apply(data_cleaning).str.split()\n",
    "de_dict = de_dict.apply(data_cleaning).str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries. \n",
    "\n",
    "_Note_: Special methods were written for the dictionaries as they do not follow the same format as tweets, but they are the same in essence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dict_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_remove_stops('english', en_dict)\n",
    "dict_remove_stops('french', fr_dict)\n",
    "dict_remove_stops('german', de_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_stem_words('english', en_dict)\n",
    "dict_stem_words('french', fr_dict)\n",
    "dict_stem_words('german', de_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the soundness of our method, we display the head of each dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling and Analysis of First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP _(Natural Language Processing)_ is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). Unlike the previous part, we only used local functions.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "(cf. _Next part_ below for information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stops(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    lang_set = stopwords.words(language)\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_words(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is mainly based on treating the data to obtain an adequate format to work with following 3 steps:\n",
    "- Tokenizing: separating words \n",
    "- Stop-words removal\n",
    "- Stemming: keeping only the radical of a word (allows better comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "cleaned['tweets'] = cleaned['main'] #keep tweets for qualitative study\n",
    "cleaned['main'] = cleaned['main'].map(lambda x: tknzr.tokenize(x))\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_stops('english', df, 'main')\n",
    "remove_stops('french', df, 'main')\n",
    "remove_stops('german', df, 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_words('english', df, 'main')\n",
    "stem_words('french', df, 'main')\n",
    "stem_words('german', df, 'main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we cross the dataset with our dictionaries to retrieve the tweets exhibiting mental distress. This is the first (less naive) step of our analysis before using Topic Modeling methods on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned.main = cleaned.main.map(lambda x:  ' '.join(x))\n",
    "de_dict = de_dict.map(lambda x:  ' '.join(x))\n",
    "en_dict = en_dict.map(lambda x:  ' '.join(x))\n",
    "fr_dict = fr_dict.map(lambda x:  ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we can find a dict entry for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_dict(tweet, dict_):\n",
    "    \"\"\"checks if dict entry matches tweet\"\"\"\n",
    "    match = [w for w in dict_ if w in tweet] #find matching for each entry\n",
    "    return len(match) > 0 #at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language.\n",
    "\n",
    "\n",
    "For english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_filtered = cleaned[cleaned.lang == 'en']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'en']['main'].map(lambda x: check_dict(x, en_dict))]\n",
    "english_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see sadness an loneliness or fear in these tweets, but some of them do not actualy express distress.\n",
    "We also note that the number of tweets has greatly been reduces through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the French tweets and again find similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "french_filtered = cleaned[cleaned.lang == 'fr']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'fr']['main'].map(lambda x: check_dict(x, fr_dict))]\n",
    "french_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "french_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for German we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_filtered = cleaned[cleaned.lang == 'de']['tweets']\\\n",
    "[cleaned[cleaned.lang == 'de']['main'].map(lambda x: check_dict(x, de_dict))]\n",
    "german_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do all of this tweets reference the person, but they also talk about sadness or dissapointment.\n",
    "The remaining number is quite small, compared to the initial 4000 german tweets in the dataset, but it is a number we might expect.\n",
    "\n",
    "Still those results could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from Dataset 1 to Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting is as follows: **Column in Dataset 2** -> Column in Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **author_user_id**           -> userId\n",
    "- **geo_point**                -> longitude + latitude (or: placeLongitude + placeLatitude)\n",
    "- **main**                     -> text\n",
    "- **published**                -> createdAt\n",
    "- **source_followers**         -> followersCount\n",
    "- **source_following**         -> friendsCount (or is it folowwers that are following ?)\n",
    "- **source_location**          -> userLocation\n",
    "\n",
    "\n",
    "#### Dataset 1 having no equivalent : \n",
    "- id                                -> tweet ID\n",
    "- truncated                         -> no idea, only NaNs\n",
    "- placeId                           -> ID of where they are\n",
    "- inReplyTo                         -> If it is a tweet reply\n",
    "- source & sourceName, sourceUrl    -> what user used to send tweet (Android, website, and so on)\n",
    "- userName                          -> username\n",
    "- screenName                        -> name shown on tweets from username\n",
    "- statusesCount                     -> number of status of user\n",
    "\n",
    "#### Dataset 2 having no equivalent : \n",
    "- **author_gender**                 -> author's gender\n",
    "- **lang**                          -> language of tweet (given)\n",
    "- **sentiment**                     -> tweet's sentiment analysis (given)\n",
    "- **source_spam_probability**       -> spam probability of tweet (given)\n",
    "- **tags**                          -> hashtags contained in the tweet (in Dataset 1 stays in text)\n",
    "- **tokenized**                     -> tokenized text of tweet (done by us)\n",
    "- **keywords**                      -> keywords fund in tweets (done by us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of the Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
