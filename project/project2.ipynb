{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we tweet about when we tweet about mental health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the methodology described in our report in order to answer our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li><b>Data Collection &amp; Preliminary Analysis:</b> description of our dataset retrieval and quick analysis of our datasets to familiarize ourselves with them</li>\n",
    "  <li><b><i>Spinn3r</i> Data Treatment:</b> \n",
    "      application of our cleaning and NLP pipeline to the $1^{st}$ dataset</li>\n",
    "  <li><b><i>Spinn3r</i> Dataset Description:</b> \n",
    "      a first (less naive) analysis of the dataset</li>\n",
    "  <li><b>Dictionary Construction</b> (using the <b><i>Spinn3r</i></b> dataset)<b>:</b> \n",
    "      creation (and refining) of the dictionary we are going to use on both datasets</li>\n",
    "  <li><b><i>Spinn3r</i> Dataset Analysis:</b> \n",
    "      first answer to our research questions using the <i>\"smaller\" <b>Spinn3r</b></i> set</li>\n",
    "  <li><b><i>twitter-swisscom</i> Data Processing Pipeline:</b> \n",
    "      after using the <b><i>Spinn3r</i></b> as a proof of concept, we apply the the cleaning, NLP and keyword-based selection methods to the our $2^{nd}$ dataset</li>\n",
    "  <li><b>Conclusion</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries we use in this Notebook (and define our constant values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "\n",
    "#Part 3\n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "#Maps display\n",
    "import folium\n",
    "import shapely\n",
    "import pysal as ps\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charts colors\n",
    "colors = plt.cm.RdYlBu(np.linspace(.2, .7, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE DATA\n",
    "EXAMPLE_PATH = 'data/example_data/'\n",
    "SP_EX_TWEETS = EXAMPLE_PATH + 'spinn3r_example.json'\n",
    "TS_EX_TWEETS = EXAMPLE_PATH + 'twitterswisscom_example.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPINN3R DATASETS\n",
    "SP_PATH = 'data/spinn3r_tweets/'\n",
    "SP_PROC_TWEETS = SP_PATH + 'processed_tweets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPINN3R KEYWORDED TWEETS SUBSETS\n",
    "KW_SP_PATH = SP_PATH + 'keyword_tweets/'\n",
    "SP_EN_KW = KW_SP_PATH + 'english_keyworded_tweets.pkl'\n",
    "SP_FR_KW = KW_SP_PATH + 'french_keyworded_tweets.pkl'\n",
    "SP_DE_KW = KW_SP_PATH + 'german_keyworded_tweets.pkl'\n",
    "SP_SMALL_EN_KW = KW_SP_PATH + 'small_english_keyworded_tweets.pkl'\n",
    "SP_SMALL_FR_KW = KW_SP_PATH + 'small_french_keyworded_tweets.pkl'\n",
    "SP_SMALL_DE_KW = KW_SP_PATH + 'small_german_keyworded_tweets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPINN3R LABELED TWEETS SUBSETS\n",
    "LBL_SP_PATH = SP_PATH + 'labeled_tweets/'\n",
    "SP_EN_LBL = LBL_SP_PATH + 'english_labeled.csv'\n",
    "SP_NEW_EN_LBL = LBL_SP_PATH + 'new_english_labeled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DICTIONARIES\n",
    "DICT_1 = \"data/dictionaries/dict_1.csv\"\n",
    "DICT_2 = \"data/dictionaries/dict_2.0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWITTER-SWISSCOM DATASETS\n",
    "SP_PATH = 'data/twitter-swisscom/'\n",
    "TWITTERSWISSCOM_PROCESSED_TWEETS = 'data/twitter-swisscom/dataset2_english.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Collection & Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets formated differently and containing different fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataset (_**Spinn3r**_) did not have any example subset. As a stand-in, we extracted the first JSON file to perform our preliminary analysis. To do this, we used the following commands after connecting to the cluster:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example dataset as well as the complet dataset of (_**twitter-swisscom**_) was retrieved from a provided _'.zip'_ file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Preliminary analysis\n",
    "\n",
    "The goal of this preliminary analysis is to familiarize ourselves with our datasets in order to know if we need to adapt our research questions or enrich the dataset with external information. We will see (below) that the main difference between the two datasets is that the $1^{st}$ dataset (_**Spinn3r**_) only covers a span of 10 months while the data of the $2^{nd}$ dataset (_**twitter-swisscom**_) was collected over multiple years and contains more precise location information. Another difference between the datasets was the fact that the $1^{st}$ contained a column expliciting the _language_, _gender of the user_, and the _predominent sentiment_ of the tweet.\n",
    "\n",
    "After our analysis, we decided to use **both datasets**. For this reason, we use the $1^{st}$ (smaller) dataset to write our functions and methods. On top of having results, we use this as a proof of concept of our analysis pipeline to be applied to the bigger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.1. Dataset 1: _Spinn3r_\n",
    "\n",
    "The [Spinn3r website](http://docs.spinn3r.com/?Example#content-schema) offers an elaborate description of each available field. Given the amount of data present in the cluster, we only look at a single file (corresponding to a day) to perform our preliminary analysis. This is a nested _JSON_ files. As the *read_json* function does not work well with nested _JSON_ data, we use a _JSON_ normalizer (provided in the _**Pandas.io**_ libary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SP_EX_TWEETS) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "ex_sp_df = json_normalize(data)\n",
    "\n",
    "#rename columns for convenience\n",
    "ex_sp_df.columns = [column.replace('_source.','') for column in ex_sp_df.columns]\n",
    "ex_sp_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these fields, we see that the most useful columns for our analysis are:\n",
    "- **main**: contains the content of the tweet (the center of our analysis).\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_location**: location of the tweet.\n",
    "- **lang**: language of the tweet.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL- (we will later see that this field is useless).\n",
    "\n",
    "_Note :_ On the cluster we find additional fields, out of which we decide to use **'author_user_id'**, as means to uniquely identify an user and **'geo_point'**, containing latitude and longitude, as a way to identify user location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the columns to keep\n",
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "ex_sp_df = ex_sp_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at categorical data :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "ex_sp_df['source_spam_probability'].value_counts().plot(kind='pie', colors=colors)\n",
    "plt.title('Spam probability in tweets')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spam probability of the tweet set seemed very promising to clean our data, but we see that not a single tweet was labeled as spam which makes us question the accuracy of the labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the values contained in the **source_location** column (which seem to be language dependent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location_occurences = ex_sp_df[ex_sp_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts()\n",
    "print('Most frequent:\\n' , location_occurences.head(), '\\n')\n",
    "print('Least frequent:\\n' , location_occurences.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that:\n",
    "- A lot of locations only differ in language (e.g. Switzerland, Schweiz and Suisse)\n",
    "- The locations' names are not always written in languages we are interested in (e.g. สวิตเซอร์แลนด์, a location name in our list)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- Some of the tweets are not located in Switzerland\n",
    "\n",
    "To deal with these different locations (and display them on a map) we use the **geo_point** field instead.\n",
    "\n",
    "_Note :_ we choose to not deal with the last point mentioned under the assumption that we were provided with a dataset only containing tweets written in Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sp_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the number of values of the dataset, we see that most tags associated to tweets are missing. This may indicate that tags won't be useful for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information to identify the sentiment (we will filter out such tweet). However, this field is not central to our analysis, it is merely used to help us evaluate the treatment applyied to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying potential issues :**\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content. We will later use this analysis to decide which values to filter out of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start be inspecting a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 140)\n",
    "ex_sp_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatly see that tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sp_df.main[ex_sp_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we generalize this rule and make the assumption that URLs are sign that a tweet is a spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a very simplified matching research, we see that some words are problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "ex_sp_df[ex_sp_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the occurences of the word _**'suicide'**_ in our set are often linked to news. This also comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sp_df[ex_sp_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word _**'therapie'**_ (_therapy_ in German), we can confirm once again the issue with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sp_df[ex_sp_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to see if they can be useful. Using these results (and the findings other researchers such as the ones presented in [8]), they are not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. _Spinn3r_ Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familiarized with our data, we have to treat our Spinn3r dataset to be able to use it correctly. To do this, we first apply our functions locally to a small subset to make sure they work as expected. After this, we use Spark to scale up our operations and apply them to all dataset on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tweet filtering\n",
    "\n",
    "- First, we would like to get rid of any tweets with a spam probability greater than $0.5$ (tweets more likely to be spam than ham). \n",
    "- As we are only focused on the Swiss population, we only keep tweets written in the country's official languages (except for Italian as no one in our group speaks the language, meaning we won't be able to extract useful information) and in English (as most tweets in the dataset, and more generally on Twitter, are written in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing spam\n",
    "spam_mask = (ex_sp_df['source_spam_probability'] >= 0.5)\n",
    "ex_sp_df.drop(ex_sp_df[spam_mask].index, inplace=True)\n",
    "ex_sp_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering by language\n",
    "lang_mask = ~ex_sp_df.lang.isin(['de', 'en', 'fr'])\n",
    "ex_sp_df.drop(ex_sp_df[lang_mask].index, inplace=True)\n",
    "ex_sp_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Column formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we format the column containing the dates to make the temporal analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to datetime format\n",
    "ex_sp_df['published'] = pd.to_datetime(ex_sp_df['published'])\n",
    "ex_sp_df['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the geo_point data is not standardised, we need to treat both types of geolocatio encoding seperately.\n",
    "\n",
    "As the example dataset does not contain the 'geo_point' field, we just present the function we will apply to do this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location_data(df):\n",
    "    \"\"\"preprocesses geo data in format as found on cluster\n",
    "    df: dataframe, locations should be in 'geo_point' column\n",
    "    \"\"\"\n",
    "    #get lines containing geopoint\n",
    "    non_null = df[~df.geo_point.isnull()]\n",
    "    \n",
    "    #location separated by comma format: [longitude, latitude ]\n",
    "    comma = non_null[non_null.geo_point.str.contains(',')].geo_point.str.split(pat=',', expand=True).applymap(float)\n",
    "    \n",
    "    #locations just next to each other [longitude latitude]\n",
    "    no_comma = non_null[~non_null.geo_point.str.contains(',')].geo_point.str.split(expand=True).applymap(float)\n",
    "    return pd.concat([comma, no_comma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Text treatment\n",
    "\n",
    "Following this mild cleaning, we need to work on the main subject of our analysis: the content of the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all caracters in lowercase (which will allow us to compare the tweets more easily without any case-sensitive-related issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sp_df['main'] = ex_sp_df['main'].str.lower()\n",
    "ex_sp_df['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking pictures to the tweet). Thus, we remove all _pic.twitter.com_ URLS and delete all mentions of the format _\"@user\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_mask = ex_sp_df['main'].str.contains(\"www\") | ex_sp_df['main'].str.contains(\"http\")\n",
    "ex_sp_df['main'] = ex_sp_df['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "ex_sp_df.drop(ex_sp_df[url_mask].index, inplace=True)\n",
    "ex_sp_df.reset_index(drop=True, inplace=True)\n",
    "ex_sp_df['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the text treatment, we delete all non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanum_filter = lambda data: re.sub(r'[^\\w\\s]', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_sp_df['main'] = ex_sp_df['main'].map(alphanum_filter)\n",
    "ex_sp_df['main'] = ex_sp_df['main'].str.replace('_', '') #not removed by previous regex\n",
    "ex_sp_df.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Spark version\n",
    "\n",
    "After performing all these filtering and cleaning steps, the size of the set is **significantly reduced** (the new file only weights **2.4MB** instead of the **55MB** of the original file, allowing us to reduce the set by **30 times**). This is particularly helpful as the whole dataset represents around **30GB**.\n",
    "\n",
    "_Note:_ the code was written as _**Raw NBConvert**_ as it is not meant to be run on the notebook but on the cluster. The code is only here as information for the reader, it's executable can be found in the run.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary elements and defining Spark environment variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lower, explode\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json('/datasets/swiss-tweet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import our data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#remove nesting\n",
    "df = df.select('_source.*')\n",
    "\n",
    "#list and keep relevant columns\n",
    "columns = ['main', 'author_user_id','published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following', 'geo_point']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter tweets using the same method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = df.filter(df.source_spam_probability < 0.5) #remove spam\n",
    "df = df.filter(df.lang.isin('en', 'de', 'fr'))  #gets tweets with right language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text treatment is a little longer as it is more thorough in Spark."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#lower the characters\n",
    "df = df.withColumn('main', lower(df.main))\n",
    "\n",
    "#replace picture urls and mentions\n",
    "df = df.withColumn('main', regexp_replace(col('main'), 'pic.twitter\\S+', ' '))\n",
    "df = df.withColumn('main', regexp_replace(col('main'), '@\\S+', ' '))\n",
    "\n",
    "#remove tweets mentioning websites and retweets\n",
    "df = df.where(~df.main.like(\"%http%\"))\n",
    "df = df.where(~df.main.like(\"%.com%\"))\n",
    "df = df.where(~df.main.like(\"%.ch%\"))\n",
    "df = df.where(~df.main.like(\"%www%\"))\n",
    "df = df.where(~df.main.like(\"%rt%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the data in the _JSON_ we will be using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.write.json('tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using  Natural Language Processing _(NLP)_ is necessary given that we work with tweets. We only perform a simple processing pipeline (tokenization, stop words removal and stemming) using the goto NLP python library _**nltk**_. This is useful because we work with tweets from different languages.\n",
    "\n",
    "_Note:_ once againt, the code was written as _**Raw NBConvert**_ as it takes a long time to run on the dataset ($5+$ minutes). The code is only here as information for the reader, it's executable can be found in the processing_pipeline.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1. Helper functions\n",
    "(cf. _Next part_ below for information)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def remove_stops(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    lang_set = stopwords.words(language)\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def stem_words(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2. Tweet formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is mainly based on treating the data to obtain an adequate format to work with. It usually follows the 3 steps explicited below:\n",
    "- Tokenizing: separating words, we use a special tweettokenizer to work onthe tweets\n",
    "- Stop-words removal: removes words with little semantic meaning\n",
    "- Stemming: keeping only the radical of a word (allows better comparison)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tknzr = TweetTokenizer()\n",
    "sp_df['tokenized'] = sp_df['main'].map(lambda x: tknzr.tokenize(x))\n",
    "sp_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "remove_stops('english', df, 'tokenized')\n",
    "remove_stops('french', df, 'tokenized')\n",
    "remove_stops('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stem_words('english', df, 'tokenized')\n",
    "stem_words('french', df, 'tokenized')\n",
    "stem_words('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description for Spinn3r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to provide sound answers to our research questions, we need to analyze our entire dataset to compare it to our final results.\n",
    "\n",
    "_Note :_ we used the processing_pipeline.py for speed and convenience. The results were save to the processed_tweets pkl file which can be found in the _'data/spinn3_tweets'_ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp_df = pkl.load(open(SP_PROC_TWEETS, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. General statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of $3936084$ tweets but we note that less than half of them have *geo\\_point* values attached to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Distribution of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7, 5])\n",
    "sp_df['lang'].value_counts().plot(kind='pie', colors=colors, autopct='%.2f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Distribution of language on tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost half of all tweets are in English, French and German being the next most popular languages. This is very interesting because it is quite unexpected given we are looking at tweets which were posted in Switzerland where german is the the most videly used official language with $66\\%$ of the country using german, which french speakers making up $22.7\\%$. [source](https://en.wikipedia.org/wiki/Languages_of_Switzerland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7, 5])\n",
    "sp_df['author_gender'].value_counts().plot(kind='pie', colors=colors, autopct='%.2f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Distribution of gender on tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, the dataset still provides the gender of over a third of profiles (even though male and female users are not equally represented), possibly allowing to get some insights on the differences in mental distress expression between the genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "sp_df['sentiment'].value_counts().plot(kind='pie', colors=colors,autopct='%.2f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Distribution of sentiment analysis results on tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around $75\\%$ of tweets are labeled as being _**NEUTRAL**_, and only $8\\%$ are labeled as negative. Taking these labeles for face value, this could already indicate that people on twitter do not discuss negative topics –mental health being such a topic – a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Distribution of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def get_time_distribution(times):\n",
    "    publishing_time = times.map(lambda x: x.hour).value_counts() / times.count()\n",
    "    publishing_time.sort_index(inplace=True)\n",
    "    return publishing_time\n",
    "\n",
    "def plot_daily(df):\n",
    "    times = get_time_distribution(df['published'])\n",
    "    return plt.plot(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily(sp_df)\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('Distribution of tweets over 24 hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the daily pattern of data. The highest number of posted tweets is around 8pm (a number decreasing through the night with the lowest pic attained at 3am). Another low point is around lunch time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = sp_df.published.map(lambda x: x.month).value_counts()\n",
    "\n",
    "sns.tsplot(data=yearly_distribution, time=yearly_distribution.index)\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('Distribution of tweets over a 10 months period')\n",
    "plt.xticks(range(1, 11),['jan', 'feb', 'mar', 'apr', 'mai', 'jun', 'jul', 'aug', 'sep', 'oct'])\n",
    "plt.xlabel('month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the number of tweets posted throughout the 10 months, we see that the dataset is not balanced (a plausible explanation is the rapidly-increasing number of Twitter users). This could also be linked to the Spinn3r's tweets retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Distribution of location data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously noted, only $\\sim40\\%$ of tweets are a geo-localised. However, we can still look at their overall distribution. We use [this tutorial](http://andrewgaidus.com/leaflet_webmaps_python/) (and [the geodata](https://github.com/interactivethings/swiss-maps) provided in the $3^{rd}$ homework) to create our map. We then convert our data into a shapefile using [an online converter](https://mygeodata.cloud/converter/shp-to-geojson).\n",
    "\n",
    "_Note:_ some points are not located in Switzerland but we do not filter out the attached tweets (we suppose the dataset is correct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first match the shapefile formatting with our location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = process_location_data(sp_df)\n",
    "location_geo = gpd.GeoSeries(locations.apply(lambda z: Point(z[1], z[0]), 1), crs={'init': 'epsg:4326'})\n",
    "location = gpd.GeoDataFrame(sp_df[~sp_df.geo_point.isnull()], geometry=location_geo) #merged data –useful later on–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities = gpd.read_file('data/geodata/ch-municipalities.shp') #load data\n",
    "municipalities.geometry = municipalities.geometry.to_crs(location_geo.crs) #change format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using _**gdp**_'s library, we find occurences per municipality and _'beautify'_ the data display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join between region and location\n",
    "counts = gpd.tools.sjoin(location, municipalities).groupby('id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add counts to the regions\n",
    "municipalities.set_index('id', inplace=True)\n",
    "municipalities['counts'] = counts\n",
    "municipalities.fillna(0, inplace=True) #if we can't find match we have 0 tweets\n",
    "municipalities.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display using 2 layers to get more granularity (we plot the municipalities with less than 10 tweets on a different layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities_0 = municipalities[municipalities.counts < 10]\n",
    "municipalities_1 = municipalities[municipalities.counts >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we create our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWISS_COORD = [46.85, 8.23]\n",
    "swiss_map = folium.Map(location = SWISS_COORD, zoom_start = 8, tiles = 'cartodbpositron')\n",
    "\n",
    "#layer with municipalities containing more than 10 tweets\n",
    "swiss_map.choropleth(municipalities_1.to_json(), data = municipalities_1,\n",
    "                     key_on = 'feature.properties.{}'.format('id'),\n",
    "                    columns = ['id', 'counts'], fill_color = 'PuRd',\n",
    "                     threshold_scale=[11, 500, 1000, 2000, 5000, 10000],\n",
    "                     legend_name='Number of tweets', fill_opacity=0.9)\n",
    "#layer with municipalities containing less than 10 tweets in total\n",
    "swiss_map.choropleth(municipalities_0.to_json(), data = municipalities_0, \n",
    "                     key_on = 'feature.properties.{}'.format('id'),\n",
    "                     columns = ['id', 'counts'], fill_color = 'PuBu', \n",
    "                     threshold_scale=[0, 10], legend_name='Area of little to no data',\n",
    "                     fill_opacity=0.1, line_opacity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cannot display folium maps in an _'.ipynb'_ file on GitHub, in addition it being quite large, we save our map to an HTML file so it an be viewd independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "swiss_map.save('municipalities_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width='900' height=\"500\" src=\"municipalities_map.html\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that tweets are strongly concentrated in urban centers. This will have to be taken into consideration when looking at the Röstigraben and urban/rural differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in building our dictionary was research (we use dictionnaries shown in [2] and [3] as theye were used for similar problems). As work with tweets in multiple languages, we translate them carefuly into French and German. We decided to add multiple appropriate synonyms in the translation for words that have many synonymes (as is the case with german). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 = pd.read_csv(DICT_1)\n",
    "dict_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_2 = pd.read_csv(DICT_2)\n",
    "dict_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our cleaning pipeline to the termes in the dictionary so that words in our dictionary can be matched to our processed tweets.\n",
    "\n",
    "_Note :_ we only show the pipeline application to the $1^{st}$ dictionary as an example. The actual cleaning (and matching of keywords) is done in the dict_filtering.py script and saved to a pkl file (stored in the *'data/spinn3r_tweets/keyworded_tweets'* folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dict_1['english'].dropna()\n",
    "fr_dict = dict_1['french'].dropna()\n",
    "de_dict = dict_1['german'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "en_dict = en_dict.map(lambda x: tknzr.tokenize(x))\n",
    "fr_dict = fr_dict.map(lambda x: tknzr.tokenize(x))\n",
    "de_dict = de_dict.map(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning and tokenizing, the dictionnaries look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_remove_stops(dictionary, language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    lang_set = stopwords.words(language)\n",
    "    return dictionary.apply(lambda expression: [word for word in expression if word not in lang_set])\n",
    "\n",
    "def dict_stem_words(dictionary, language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    return dictionary.apply(lambda expression: [stemmer.stem(word) for word in expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict = dict_remove_stops(fr_dict, 'french')\n",
    "en_dict = dict_remove_stops(en_dict, 'english')\n",
    "de_dict = dict_remove_stops(de_dict, 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dict_stem_words(en_dict, 'english')\n",
    "fr_dict =  dict_stem_words(fr_dict, 'french')\n",
    "de_dict =  dict_stem_words(de_dict, 'german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stemming and stop word removals, the dictionnaries look like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Finding keyword matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define to following helper functions to find matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, t):\n",
    "    \"\"\"compares to lists, and returns \n",
    "    true for all possible permutations of same list\n",
    "    s, t: lists to compare\n",
    "    \"\"\"\n",
    "    return Counter(s) == Counter(t)\n",
    "\n",
    "def match_dict(tweet, dict_):\n",
    "    \"\"\"returns keywords that match in string\n",
    "    tweet: tweet to find keywords in\n",
    "    dict_: list of keywords\n",
    "    \"\"\"\n",
    "    #should we explain this more?\n",
    "    doc = [sublist for sublist in dict_ if compare(list(filter(lambda x: x in tweet, sublist)), sublist) ]\n",
    "    return doc #at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language. We perform the actual algorithm in a script and save the results into a pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dictionaries, we filter the tweets to keep the ones where keyword occurs at least once in the tweets.\n",
    "\n",
    "_Note:_ the actual algorithm was performed using the dict_filtering_small.py script and the results were saved to pickle files in the *'data/spinn3r_tweets/keyworded_tweets'* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Dictonary 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**English results :**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "english_filtered = sp_df['tokenized'].map(lambda x: match_dict(x, en_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_SMALL_EN_KW, \"rb\" ))\n",
    "\n",
    "small_english_filtered = df[(df.keywords.map(lambda x: x != [] ))]\n",
    "print('number of matching tweets: ', small_english_filtered.main.count())\n",
    "print('percentage of matching tweets: ', small_english_filtered.main.count() / df.main.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_english_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get very frew results using this dictionary. Less than $0.2\\%$ of all tweets match at least one keyword. This is not surprising as these are quite specialized terms.\n",
    "Even though some tweets indeed show signs of mental health issues, some are completely unrelated to mental health. \n",
    "This discussion will be explicited in the Dictionary Evaluation (Parts _3.4_ and _3.5_).\n",
    "\n",
    "The fact that despite the few matches we get false positives indicates that the total number of true matching tweets is even smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_english_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.title('Keyword occurences')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 most frequent keywords are _depress_, _addict_ and _suicid_, which are often casually used compared to the 5 other words we see in the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**French results :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_SMALL_FR_KW, \"rb\" ))\n",
    "small_french_filtered = df[(df.keywords.map(lambda x: x != [] ))]\n",
    "print('number of matching tweets: ', small_french_filtered.main.count())\n",
    "print('percentage of matching tweets: ', small_french_filtered.main.count() / df.main.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are 3 times less matching tweets than in English, which can partially be explained by the fact that the ratio of matching tweets is not the same in both languages. Looking at the percentage of matching tweets we see that only $0.09\\%$ of all french tweets contain at least one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_french_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_french_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.title('Keyword occurences')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to French, the keyword _'suicid'_ is by far the most frequent, which all other words being used quite infrequently, having less than 100 matching tweets each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**German results :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_SMALL_DE_KW, \"rb\" ))\n",
    "small_german_filtered = df[(df.keywords.map(lambda x: x != [] ))]\n",
    "print('number of matching tweets: ', small_german_filtered.main.count())\n",
    "print('percentage of matching tweets: ', small_german_filtered.main.count() / df.main.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In German, the number of matching tweets is even lower, despite having used synonymes to compensate for the multitude of possible matching words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_german_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_german_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.title('Keyword occurences')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the keyword frequencies, we see that similar to english, addiction, depression and suicide ['abhang', 'depression', 'suizid', 'selbstmord'] are the most frequent keywords, which the other keywords in the dictionary having much fewer matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Dictionary 2\n",
    "\n",
    "\n",
    "We now look at the second dictionary retrived from previous research. This dictionary contains a broader range of words, and is a superset of the previous dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**English results :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_EN_KW, \"rb\" ))\n",
    "english_filtered = df[(df.keywords.map(lambda x: x != [] ))]\n",
    "print('number of matching tweets: ', english_filtered.main.count())\n",
    "print('percentage of matching tweets: ', english_filtered.main.count()/ df.main.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance we can see that this dictionary gives us many more matches, with about $1\\%$ of all english tweets containing at least one keyword. We also see that the matches tend to be more general about feelings and less about specific illnesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though not all tweets express distress (just as before), we can clearly see sadness, loneliness or fear. We also notice that the number of matching tweets has increased greatly (around x10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered[english_filtered.keywords.map(lambda x: len(x) > 1)].keywords.map(lambda x: str(x)).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**French results :**\n",
    "\n",
    "For french we find similar results as for english. We have more matched, and the matches are less mentall illness specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_FR_KW, \"rb\" ))\n",
    "french_filtered = df[(df.lang == 'fr') & (df.keywords.map(lambda x: x != [] ))]\n",
    "print('number of matching tweets: ', french_filtered.main.count())\n",
    "print('percentage of matching tweets: ', french_filtered.main.count() / df[df.lang == 'fr'].main.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the words we see that 'kill' or 'die' is by far the most frequent, followed by words like 'sadness' and 'hate'. It thus gives similar results to english. We also find that suicide is still comparetively quite frequent, even with the addition of other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**German results :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German also shows an increase in matches with the larger dict. However, due to the number of matches being small originally, the total number of matches is still quite small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load(open(SP_DE_KW, \"rb\" ))\n",
    "\n",
    "german_filtered = df[(df.lang == 'de') & (df.keywords.map(lambda x: x != [] ))]\n",
    "\n",
    "print('number of matching tweets: ', german_filtered.main.count())\n",
    "print('percentage of matching tweets: ', german_filtered.main.count() / df[df.lang == 'de'].main.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered[['main', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered.keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the most frequent words are again 'sad', followed by expressions of lonelyness and emptyness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Dictionary evaluation (using our sample labeling process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.0. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_search(df, word):\n",
    "    return df[df['main'].map(lambda x: word in x)]['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_search(df, word, col):\n",
    "    md_count = df[df[col].map(lambda x: word in x) & df['mental'] == 1][col].count()\n",
    "    print(\"Tweets showing mental distress and containing the word '\" + word + \"': \" , md_count)\n",
    "    count = df[df[col].map(lambda x: word in x)][col].count()\n",
    "    print(\"Total appearance of the word '\" + word + \"': \" , count)\n",
    "    print(\"Ratio between these 2 numbers : \", md_count/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blacklisted_search(df, word):\n",
    "    count = df[df['split'].map(lambda x: word in x)]['split'].count()\n",
    "    print(\"Total appearance of '\" + word + \"': \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After crossing the dataset with our second dictionary to retrieve tweets exhibiting _mental distress_, we labeled a sample to see if the tweets indeed show such signs. \n",
    "\n",
    "Note that this task is complex as it is very subjective (choosing tweets displaying mental health issues was done at the discretion of the person labeling them). Our main conclusion from this step was the necessity to \"expand\" our research to tweets showing signs of mental distress in general and not only mentioning mental illnesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_iter = pd.read_csv(SP_EN_LBL)\n",
    "first_iter.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Key _(qualitative)_ takeaways\n",
    "\n",
    "- Tweets were labeled \"losely\" using multiple signs of mental distress, mostly sadness. Tweets with the following specificities were labeled as showing mental distress: \n",
    "    - nostalgia (either for the past or simply for the end of a nice day)\n",
    "    - mention of sad activities (people who watched sad movies, listened to sad songs as this was most probably triggered by a previous feeling of sadness)\n",
    "    - mention of being mad (either over other people like haters or, more often, mad with their families)\n",
    " \n",
    "\n",
    "- Some tweets mentioned the mental distress of other people (either how they helped them or raising awareness over these issues)\n",
    "- Some tweets showed desperation (a form of mental distress) over the \"way the world is\", these were not included.\n",
    "\n",
    "- An unexpected insight (which we decided not to pursue further as it was too tenuous) was the link between summer and mental distress. Indeed, most of tweets mentioning summer showed some sadness due to the absence of friends (from university for example).\n",
    "- Some tweets only included _\"motivational\"_ quotes. We can suppose that these reflect bad times. Nonetheless, these were not included as our previous assumption is a little far-fetched. Moreover, this could have messed with our model (they did not include specific words of our dictionary nor specific mentions to mental distress).\n",
    "\n",
    "- A lot of mental distress tweets are related to unilateral feelings. They are often messages specifically targeted at someone who most certainly doesn't know of their existence (for example work collegues or college classmates).\n",
    "\n",
    "- Most tweets talking about depression or terrible low self-esteem show _« covert-humour »_, a coping technique used to hide ones self-deprecation. These tweets are very difficult to detect as they use sarcasm (which can not be perceived using current NLP techniques) and seldom use words from our dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. Key _(quantitative)_ takeaways\n",
    "\n",
    "After the labeling, we found the necessity of both changing our current dictionary (through additions but also removals) and creating a new dictionary (which we decided to call a _negative_ dictionary, but is simply a blacklist of words we did not want to appear anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words to include in our dictionary :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_search(first_iter, 'unfortunately')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_search(first_iter, 'therapy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_search(first_iter, 'overthinking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though these expressions do not appear very often, they are unique to tweets showing mental distress. Thus, we decide to include them in our new dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words to remove from our dictionary :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "md_search(first_iter, 'hate', 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_search(first_iter, 'pain', 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_search(first_iter, 'addict', 'split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we decided to exclude many words (_hate_, _hurt_, _pain_, _addict_ and _overdose_), we decided to demonstrate our method only on a few. When looking at the number, we see that more than half the tweets containing the (previously) specified words are false positives, which drives us to striked them from our dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words to blacklist :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_search(first_iter, 'overw')\n",
    "blacklisted_search(first_iter, 'overwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_search(first_iter, 'syria')\n",
    "blacklisted_search(first_iter, 'assad')\n",
    "blacklisted_search(first_iter, 'asylum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blacklisted_search(first_iter, 'trump')\n",
    "blacklisted_search(first_iter, 'clinton')\n",
    "blacklisted_search(first_iter, 'america')\n",
    "blacklisted_search(first_iter, 'usa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the words we want to strike represent major recurring themes (mostly having to do with the news) which do not have anything to do with mental distress (none of these tweets were labeled as showing signs of mental distress). Once again, we only showed our idea on a small subset of topics, the list of words we want to blacklist is : _gov_, _government_, _syria_, _assad_, _refugees_, _asylum_, _overwatch_, _vine_, _trump_, _hillary_, _america_, _usa_ and _india_. \n",
    "\n",
    "_Note:_ two other expressions are blacklisted, but that is due to our first research algorithm (which did not match perfectly the strings) : _suicidesquad_ (contains _suicide_) and _spain_ (contains _pain_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3. French and German labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When labeling the other languages, we saw that only $\\sim10\\%$ of tweets containing at least a keyword from a dictionary actually showed mental distress. Improving the dictionnaries would probably give better results, but as no research of this kind has been done in these languages, we can only improve our results through many iterations (and labeling steps). As this is too time-consuming and uncertain, we decided to drop both languages and concentrate on English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Dictionary evaluation (using LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we only work with tweets which were crossed with our second dictionary. \n",
    "\n",
    "As this is applied on a larger scale than our labeling, it allows us to show subjects which should be avoided (as showed when constructing our _negative_ dictionary).\n",
    "\n",
    "We confirm the results found in the labeling and add additional blacklisted words.\n",
    "\n",
    "_Note :_ we use a script we build our LDA models and save thems to the _'data/models'_ folder. We will quickly present these models were created but our focus will be on understanding then (and displaying our results using the _**pyLDAvis**_ library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1. Building an English LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our LDA model, we need to create a dictionary and define 'texts' (in our case the tweets). This model is then passed to gensim, which then implements the proper LDA model construction. The number of topics was choosen as to be most human understandable. The number of passes and iterations was choosen to be as large as possible, as the corpus we are working on is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords for more meaningful results\n",
    "english_stop_rem = dict_remove_stops(english_filtered.tokenized, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(english_stop_rem)\n",
    "dictionary.filter_extremes(no_below=2, no_above=.8) #removes too frequent / sparse words\n",
    "texts = english_stop_rem.tolist()\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Code used to form the LDA model\n",
    "ldamodel = models.LdaMulticore(corpus, id2word=dictionary, num_topics=6, workers=3, iterations=100, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading our LDA model, we evaluate it using the most frequent word distributions of each topic. We can already notice distinct words in Topic 3 (centers around words such as 'syria' and 'muslim')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.LdaModel.load('models/english.model')\n",
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the visualisation\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the $\\lambda$ slider to give understandable names to the topics:\n",
    "\n",
    "- 1 & 2 : general topics talking about everyday distress, contains specific words such as relationship, teacher, sunday, gloomy\n",
    "\n",
    "\n",
    "- 3 : wars & terrorism seems to be the main topic, we see keywords such as muslim, terroist, usa, syria, aleppo, kill, bomb\n",
    "\n",
    "\n",
    "- 4 & 5: mentions of suicide squad (a movie that came out in 2016 ), less easy to identify an overall topic, japan and whaling related tweets seem to be in this topic aswell\n",
    "\n",
    "\n",
    "- 6 : mentions of play, music (specificaly gnash and his song i hate u, i love u ft. olivia o'brien), this topic seems to have formed due to the many mentions of this song in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that common specific words in topic 3, 4, 5 and 6 should be excluded from our dictionary search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Dictionary update (using a second sample labeling)\n",
    "\n",
    "After everything that we found above, we updated our dictionary to refine our output. The following analysis follows the labeling of a new sample from the dataset evaluated using the new dictionary. We will use all these takeaways to update our dictionary one last time (without analysing it again). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_iter = pd.read_csv(SP_NEW_EN_LBL)\n",
    "second_iter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1. Key _(qualitative)_ takeaways\n",
    "\n",
    "Unlike the first pass on the data, this labeling does not include a thorough qualitative analysis. It only allowed us to improve our methodology and find plausible answers to some of the phenomena we observed. One of such phenomena is the difference in emotional expression between the genders. This may be due to the (very high) number of tweets by _\"fangirls\"_, female tweet authors mentioning their love for their music favorite bands (mostly Asian) or one of their favorite actors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2. Key _(quantitative)_ takeaways\n",
    "\n",
    "In this case, we only determine words to add both to our dictionary and to our _negative_ dictionary (no word in our dictionary was deemed superfluous or inducing too many false positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words to add to our dictionary :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_search(second_iter, 'breakdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_count = second_iter[second_iter['main'].map(lambda x: ('confused' in x or 'lonely' in x) and 'sad' in x)]['main']\n",
    "print(\"Total appearance of the expressions involving 'sad' : \", exp_count.count())\n",
    "exp_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not use our usual research function for the last demonstration as it involves 2 expressions we found the need to group. Once again, this is only a preview of our quantitative analysis. The whole list of expressions we want to add to our dictionary is : _panic attack_, _sleepless_, _problems falling asleep_, _lonely and sad_, _confused and sad_, _gambling_, _breakdown_, and _mental issue_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words to blacklist :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_search(second_iter, 'sadly', 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_count = second_iter[second_iter['main'].map(lambda x: 'sonic' in x and 'mania' in x)]['main']\n",
    "print(\"Total appearance of the expression 'Sonic Mania' : \", sm_count.count())\n",
    "sm_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we use a different format to show a special expression. We need to delete _'Sonic Mania'_ as it contains the word _'mania'_ (which couldn't be taken out even with our new search algorithm). However, we can see that affining our dictionary worked as we have a lot less words to blacklist : _sadly_, _not afraid_, _stress relief_ and words with the radical _\"addict\"_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis of First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We process the final dictionary using the dict_filtering_final.py script.\n",
    "\n",
    "\n",
    "We now discuss the final results from our dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_pickle('data/spinn3r_tweets/keyword_tweets/final_english_keyworded_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[~final_df.keywords.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_keyword = final_df.keywords.map(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[has_keyword].main.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[has_keyword].keywords.map(lambda x: ' '.join(x[0])).value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "(final_df[has_keyword].sentiment.value_counts()/final_df[has_keyword].sentiment.count()).plot(kind='bar')\n",
    "plt.subplot(2,2,2)\n",
    "(final_df.sentiment.value_counts()/final_df.sentiment.count()).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "(final_df[has_keyword].author_gender.value_counts()/final_df[has_keyword].author_gender.count()).plot(kind='bar')\n",
    "plt.subplot(2,2,2)\n",
    "(final_df.author_gender.value_counts()/final_df.author_gender.count()).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily(final_df[has_keyword])\n",
    "plot_daily(final_df)\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.legend(['filtered', 'all'])\n",
    "plt.title('Distribution of tweets over 24 hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = final_df[has_keyword].published.map(lambda x: x.month).value_counts() / final_df[has_keyword].published.count()\n",
    "yearly_distribution2 = final_df.published.map(lambda x: x.month).value_counts() /  final_df.published.count()\n",
    "\n",
    "sns.tsplot(data=yearly_distribution, time=yearly_distribution.index, color='orange')\n",
    "sns.tsplot(data=yearly_distribution2, time=yearly_distribution2.index)\n",
    "plt.legend(['filtered', 'overall'])\n",
    "plt.ylabel('relative percetage of tweets')\n",
    "plt.title('Distribution of tweets over a 10 months period')\n",
    "plt.xticks(range(1, 11),['jan', 'feb', 'mar', 'apr', 'mai', 'jun', 'jul', 'aug', 'sep', 'oct'])\n",
    "plt.xlabel('month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = process_location_data(final_df[has_keyword])\n",
    "location_geo = gpd.GeoSeries(locations.apply(lambda z: Point(z[1], z[0]), 1), crs={'init': 'epsg:4326'})\n",
    "location = gpd.GeoDataFrame(final_df[has_keyword][~final_df[has_keyword].geo_point.isnull()], geometry=location_geo) #merged data –useful later on–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities = gpd.read_file('data/geodata/ch-municipalities.shp') #load data\n",
    "municipalities.geometry = municipalities.geometry.to_crs(location_geo.crs) #change format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join between region and location\n",
    "counts = gpd.tools.sjoin(location, municipalities).groupby('id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add counts to the regions\n",
    "municipalities.set_index('id', inplace=True)\n",
    "municipalities['counts'] = counts\n",
    "municipalities.fillna(0, inplace=True) #if we can't find match we have 0 tweets\n",
    "municipalities.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities_0 = municipalities[municipalities.counts < 1]\n",
    "municipalities_1 = municipalities[municipalities.counts >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWISS_COORD = [46.85, 8.23]\n",
    "swiss_map = folium.Map(location = SWISS_COORD, zoom_start = 8, tiles = 'cartodbpositron')\n",
    "\n",
    "#layer with municipalities containing more than 10 tweets\n",
    "swiss_map.choropleth(municipalities_1.to_json(), data = municipalities_1,\n",
    "                     key_on = 'feature.properties.{}'.format('id'),\n",
    "                    columns = ['id', 'counts'], fill_color = 'PuRd',\n",
    "                     legend_name='Number of tweets', fill_opacity=0.9)\n",
    "#layer with municipalities containing less than 10 tweets in total\n",
    "swiss_map.choropleth(municipalities_0.to_json(), data = municipalities_0, \n",
    "                     key_on = 'feature.properties.{}'.format('id'),\n",
    "                     columns = ['id', 'counts'], fill_color = 'PuBu', \n",
    "                     threshold_scale=[0, 1], legend_name='Area of little to no data',\n",
    "                     fill_opacity=0.1, line_opacity=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_map.save('dist.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width='900' height=\"500\" src=\"dist.html\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing Pipeline on the Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from Dataset 1 to Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting is as follows: **Column in Dataset 2** -> Column in Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **author_user_id**           -> userId\n",
    "- **geo_point**                -> longitude + latitude (or: placeLongitude + placeLatitude)\n",
    "- **main**                     -> text\n",
    "- **published**                -> createdAt\n",
    "- **source_followers**         -> followersCount\n",
    "- **source_following**         -> friendsCount (or is it folowwers that are following ?)\n",
    "- **source_location**          -> userLocation\n",
    "\n",
    "\n",
    "#### Dataset 1 having no equivalent : \n",
    "- id                                -> tweet ID\n",
    "- truncated                         -> no idea, only NaNs\n",
    "- placeId                           -> ID of where they are\n",
    "- inReplyTo                         -> If it is a tweet reply\n",
    "- source & sourceName, sourceUrl    -> what user used to send tweet (Android, website, and so on)\n",
    "- userName                          -> username\n",
    "- screenName                        -> name shown on tweets from username\n",
    "- statusesCount                     -> number of status of user\n",
    "\n",
    "#### Dataset 2 having no equivalent : \n",
    "- **author_gender**                 -> author's gender\n",
    "- **lang**                          -> language of tweet (given)\n",
    "- **sentiment**                     -> tweet's sentiment analysis (given)\n",
    "- **source_spam_probability**       -> spam probability of tweet (given)\n",
    "- **tags**                          -> hashtags contained in the tweet (in Dataset 1 stays in text)\n",
    "- **tokenized**                     -> tokenized text of tweet (done by us)\n",
    "- **keywords**                      -> keywords fund in tweets (done by us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of the Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Due to issue with the cluster, we decide to use the dataset in local and preform the necessary processing steps locally.\n",
    "\n",
    "By removing unnecessary columns,  tweets containing websites, retweets, and general giberish, that is set of characters mixed with numbers, we can reduce the size of the dataset from  5.6 GB down to 1.2 GB. The code for this process can found in the run_d1.py script. We save this dataset in a csv file for further processing.\n",
    "\n",
    "As mentioned previously, this set is not annotated with the language of the tweet. We thus needed to use an external language classification library to label the tweets and only keep english language tweets. For reasons of speed we use the guess_language function from sprit. Using this function, the labeling of all 14million tweets took about 2 hours.\n",
    "After this we save the tweets that were labeled as english to an other csv file.\n",
    "\n",
    "We now give a quick analysis of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df2 = pd.read_csv('data/twitter-swisscom/dataset2_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preform some simple dataprocessing that is needed after retrieving the file from csv.\n",
    "\n",
    "Then we visualize the overal distribution of the dataset with functions shown in section 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df2.rename(columns={'createdAt':'published'}, inplace=True)\n",
    "twitter_df2 = twitter_df2[~twitter_df2.published.isnull()]\n",
    "twitter_df2['published'] = twitter_df2['published'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove deformed columns\n",
    "tw = twitter_df2[twitter_df2.published.map(lambda x: len(x) == 19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform to date time\n",
    "tw.published = pd.to_datetime(tw['published'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at how many tweets we have remaining: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 1 million tweets remaining, which is quite few given the initial size of the dataset.\n",
    "\n",
    "From the previous dataset we know that keywords are quite sparse. So despite this dataset spanning a longer amount of time, we still find that only very little usable tweets remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily(tw)\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('Distribution of tweets over 24 hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temporal distribution over one day matches the distrubution we observed in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = tw.published.map(lambda x: x.month).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(data=yearly_distribution, time=yearly_distribution.index)\n",
    "plt.ylabel('number of tweets')\n",
    "plt.xlabel('month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the overall monthly distribution is very odd. We have no explination for this phenomenon.\n",
    "\n",
    "\n",
    "We note that we can find dates as early as 2002 in the dataset. This must be an error, as twitter first went online in 2006.\n",
    "Due to the sparcity of tweets pre 2013 we only look at tweets after 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = tw[tw.published.map(lambda x: int(x.year) >= 2013)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = tw.published.map(lambda x: x.date()).value_counts()\n",
    "\n",
    "import matplotlib\n",
    "date = matplotlib.dates.date2num(yearly_distribution.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(data=yearly_distribution, time=date)\n",
    "#plt.xticks([  x.year for x in matplotlib.dates.num2date(date)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot we find is very odd. Again, we have no explination for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
