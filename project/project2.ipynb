{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Mental health in Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we describe the multiple steps of our preprocessing pipeline and the work done on the Twitter dataset to answer our research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li><b>Data Collection & Preliminary Analysis:</b> how we accessed the datasets and why we use them</li>\n",
    "  <li><b>Data Treatment:</b> cleaning and NLP on the first dataset</li>\n",
    "  <li><b>Datasets Description for Spinn3r:</b> a first look at the Spinn3r Dataset</li>\n",
    "  <li><b>Dictionary Construction:</b> creation of the dictionary we are going to us on the datasets</li>\n",
    "  <li><b>Dictionary Update:</b> updating the dictionnary with what we learned thanks to the dataset</li>\n",
    "  <li><b>Analysis of First Dataset:</b> anaysis of the smaller Spinn3r set for a proof of concept</li>\n",
    "  <li><b>Processing Pipeline on the Second Dataset:</b> apply what we learned on the previous dataset on the second one</li>\n",
    "  <li><b>Analysis of the Second Dataset:</b> applying what we learned on the smaller dataset on the swisscom-twitter dataset</li>\n",
    "  <li><b>Conclusion</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly import the libraries to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import pyspark as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Part 1\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#Part 2\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#Part 3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Collection & Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were provided two separate datasets containing Swiss tweets. They were formated differently and contained different fields. On top of this, while the first dataset contained data collected over multiple years (twitter-swisscom, refered to as second dataset), the other only covers a span of 10 months (Spinn3r, refered to as first dataset).\n",
    "\n",
    "\n",
    "The complete twitter data for both datsets was collected from the cluster provided in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example dataset of (_**twitter-swisscom**_) was retrieved from a .zip file provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset (_**Spinn3r**_), however, did not provide a specific example datset. As a standing we extracted the first JSON file to perform our analysis. To do this, we used the following comands after connecting to the cluster:\n",
    "\n",
    "```bash\n",
    "cluster$ hadoop fs -getmerge /datasets/swiss-tweet/harvest3r_twitter_data_01-01_0.json /buffer/example.json\n",
    "local$ scp -r gaspar@iccluster060.iccluster.epfl.ch:/buffer/example.json <local-path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis\n",
    "\n",
    "The goal of our preliminary analysis was to decide which dataset would be more useful but also to familiarize ourselves with the chosen dataset in order to know if we need to adapt our research questions or enrich the dataset with external information in order to perform our analysis.\n",
    "\n",
    "After the analysis (presented below), we decided to use **both datasets** for our project. While the twitter-swisscom contains more precise location information (longitude and latitude), the Spinn3r dataset contains a sentiment analysis field as well as a language field and has a lot less data. Thus, we decided to create our functions and methods using the smaller dataset (Spinn3r) and use it as a proof of concept before going on a larger scale and use the swisscom-twitter dataset to further our analysis.\n",
    "\n",
    "This dataset has an elaborate description of each field available on the [spinn3r website](http://docs.spinn3r.com/?Example#content-schema). Given the amount of data present in the cluster, we only look at one day to perform our first analysis (we will later show how we scale our operations using Spark).\n",
    "\n",
    "This dataset is given in JSON format and has nested elements. As the *read_json* function does not work well with nested JSON data, we use a JSON normalizer, which is provided in the *Pandas.io* libary. \n",
    "\n",
    "_Note: We will later see that Spark deals better with nested JSON._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields found in this dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PATH = 'swiss-tweet/example.json'\n",
    "\n",
    "with open(EXAMPLE_PATH) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "twitter_df = json_normalize(data)\n",
    "#rename columns for convenience\n",
    "twitter_df.columns = [column.replace('_source.','') for column in twitter_df.columns]\n",
    "twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all these columns, we find the ones below to be the most useful for our analysis:\n",
    "- **main**: contains the content of the tweet.\n",
    "- **published**: gives the time on which the content was posted.\n",
    "- **source_spam_probability**: probability of tweet being spam.\n",
    "- **source_location**: location of the tweet.\n",
    "- **tags**: tags associated with the tweet (provided by Spinn3r).\n",
    "- **lang**: language of the tweet.\n",
    "- **sentiment**: sentiment score of the tweet -POSITIVE, NEGATIVE, NEUTRAL-.\n",
    "- **author_gender**: gender of the author -MALE, FEMALE, UNKNOWN-.\n",
    "- **source_followers**: followers of the user who tweeted.\n",
    "- **source_following**: number of people the user follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider these tags to be the most useful as:\n",
    "\n",
    "- **Main** stands at the center of our analysis. We plan to perform NLP methods in order to identify relevant tweets and use the content as well to identify related words.\n",
    "- **Published** can be used to map the tweets over the duration of the year and look for seasonal changes.\n",
    "- **Source_location** can be used to look at the geographical distribution of the tweets.\n",
    "- **Lang** will be used to filter out unwanted languages, which we need to do in order to perform our NLP tasks.\n",
    "- **Author_gender** will be used to identify the gender and look at the differences between both gend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ that on the cluster we find additional fields, out of which we decide to use **'author_user_id'**, as means to uniquely identify an user and **'geo_point'**, containing latitude and longitude, as a way to identify user location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main', 'published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following']\n",
    "twitter_df = twitter_df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial analysis of categorical data\n",
    "\n",
    "We now give a quick look into data contained in the example dataset, to find how we need to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "twitter_df['source_spam_probability'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spam probability of the tweet set, we see that not a single tweet was labeled as spam. This makes us question the accuracy of the labeling as the set of tweets on that day most certainly contains spam. However, we will still use it as we assume the chance of having false positives to be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df.lang.isin(['de', 'fr', 'en'])]['source_location'].value_counts().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the location seems to be language dependent, we only care about tweets written in the 3 languages we need. We see that:\n",
    "- A lot of locations only differ in language (e.g. as Switzerland and Schweiz)\n",
    "- The name of the locations are not always given languages we are interested in (e.g. สวิตเซอร์แลนด์)\n",
    "- A vast majority of the dataset is just located in 'Switzerland'\n",
    "- As opposed to dataset 1, all tweets are located in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the missing values in the dataset, we see that for most tweets, the tags are missing. This may indicate that the tags won't be usefull for our analysis, but this does not influence our research at this stage.\n",
    "\n",
    "We also note that for some tweets, the sentiment is missing. As noted on Spinn3er, this may be due to the fact that some tweets do not contain enough linguistic information. As we will filter out such tweets, the remaining set should contain sentiment. Even if doesn't, this field is not central to our analysis, it is merely used to help us filter our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying potential issues\n",
    "\n",
    "While this set of tweets is not representative, we can still use it to find potential issues we might have with the tweets' content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "twitter_df.sample(n=10)['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample, we immediatly see that the tweets containing links are not relevant to our research question (they are mostly news or adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.main[twitter_df.main.map(lambda x: 'http://' in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more intently at the tweets containing links, we can make the assumption that the relationship between URLs and spam is a general rule (at any time of the year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a vastly simplified version of the dictionary matching we will preform to get relevant tweets and analyze the results.\n",
    "\n",
    "Here we look at the occurence of 'suicide' in the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "twitter_df[twitter_df['main'].map(lambda x: 'suicide' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing a simplistic dictionary matching using the occurences of the word *'suicide'* in our tweet set, we see that a lot of these tweets contain news. This further comforts us in our choice to remove tweets with URLs in order to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'therapie' in x) ]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the word *'therapy'* (in German), we can confirm once again the issue there is with URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[twitter_df['main'].map(lambda x: 'RT ' in x) ]['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the retweets to get an idea on how useful they could be. From what we see, and what others have seen [8], they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data exploration, we have to clean our dataset to be able to use it correctly for our research.\n",
    "\n",
    "First, we use Pandas locally on a small subset of the tweets to explore different cleaning methods and make sure the functions we chose work as expected. After proving that our concept works, we use Spark to scale up our operations and be able to perform them on all the files provided on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet filtering\n",
    "\n",
    "Given the context of our research, there are many useless tweets. \n",
    "- First, we would like to get rid of any tweets with a spam probability greater than 0.5 (an arbitrary value we found sound). \n",
    "- Finally, as we are focused on the Swiss population, we only keep tweets in the official languages (except for Italian as no one in our group speaks the language, meaning we won't be able to extract useful information) and in English (as most tweets in the dataset, and more generally on Twitter, are written in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mask = (twitter_df['source_spam_probability'] >= 0.5)\n",
    "twitter_df.drop(twitter_df[spam_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_mask = ~twitter_df.lang.isin(['de', 'en', 'fr'])\n",
    "twitter_df.drop(twitter_df[lang_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only have relevant tweets (following a mild cleaning), we need to work on the main subject of our analysis: the content of the tweets itself. This is necessary to ease the _3rd Part_ of our research (processing the text to find patterns using NLP, _Natural Language Processing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make later temporal analysis easier we transform the column containing the dates into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['published'] = pd.to_datetime(twitter_df['published'])\n",
    "twitter_df['published'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our treatment is to lower all the caracters (allows comparing the tweets without case-sensitive searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['main'] = twitter_df['main'].str.lower()\n",
    "twitter_df['main'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in _Part 1_, URLs are highly linked to spam (except for URLs linking pictures to the tweet). As image processing is not in the scope of this project, we remove all \"pic.twitter\" URLS and mentions of the format \"@user\". After that, we remove all tweets containing a URL and retweets as they do not give us any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_mask = twitter_df['main'].str.contains(\"www\\S+\") | twitter_df['main'].str.contains(\"http\\S+\")\n",
    "twitter_df['main'] = twitter_df['main'].str.replace(\"pic.twitter\\S+\", '')\n",
    "twitter_df.drop(twitter_df[url_mask].index, inplace=True)\n",
    "twitter_df.reset_index(drop=True, inplace=True)\n",
    "twitter_df['main'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanum_filter = lambda data: re.sub(r'[^\\w\\s]', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['main'] = twitter_df['main'].map(alphanum_filter)\n",
    "twitter_df['main'] = twitter_df['main'].str.replace('_', '') #not removed by previous regex\n",
    "twitter_df.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark version\n",
    "\n",
    "After performing all these cleaning steps, the size of the set is **significantly reduced** (the new file only weights **2.4MB** instead of the **55MB** of the original file, allowing us to reduce the set by **30x**). This is particularly helpful as the whole dataset represents around **30GB**.\n",
    "\n",
    "_Note:_ the code was defined as _**Raw NBConvert**_ as it is not meant to be run on the notebook but on the cluster. The code is only here as information for the reader, it's executable can be found in the run.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary elements and defining Spark environment variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lower, explode\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json('/datasets/swiss-tweet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import our data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#remove nesting\n",
    "df = df.select('_source.*')\n",
    "\n",
    "#list and keep relevant columns\n",
    "columns = ['main', 'author_user_id','published', 'source_spam_probability', 'source_location', 'tags', 'lang', 'sentiment',\n",
    "                   'author_gender', 'source_followers', 'source_following', 'geo_point']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter tweets using the same method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df = df.filter(df.source_spam_probability < 0.5) #remove spam\n",
    "df = df.filter(df.lang.isin('en', 'de', 'fr'))  #gets tweets with right language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text treatment is a little longer as it is more thorough in Spark."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#lower the characters\n",
    "df = df.withColumn('main', lower(df.main))\n",
    "\n",
    "#replace picture urls and mentions\n",
    "df = df.withColumn('main', regexp_replace(col('main'), 'pic.twitter\\S+', ' '))\n",
    "df = df.withColumn('main', regexp_replace(col('main'), '@\\S+', ' '))\n",
    "\n",
    "#remove tweets mentioning websites and retweets\n",
    "df = df.where(~df.main.like(\"%http%\"))\n",
    "df = df.where(~df.main.like(\"%.com%\"))\n",
    "df = df.where(~df.main.like(\"%.ch%\"))\n",
    "df = df.where(~df.main.like(\"%www%\"))\n",
    "df = df.where(~df.main.like(\"%rt%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the data in the _JSON_ we will be using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.write.json('tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description for Spinn3r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now preform a general analysis of the dataset on which we preform all the NLP methods and unsupervised clustering on.\n",
    "\n",
    "We use a pkl file that was generated using *processing_pipeline.py* for speed an convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pkl.load( open( \"processed_tweets.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at how many values we have for each field. The total number of tweets is $3936084$. We see that less than half of all tweets have a $geo\\_point$ value associated to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of categorical data\n",
    "\n",
    "We now look at the distribution of categorical features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['lang'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that almost half of all tweets are in english, followed by french and german. The latter is interessting as given the swiss population, one would expect there to be more german than french tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "twitter_df['author_gender'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most accounts do not contain information on the user's gender (meaning there is no way we can have an unbiased set). However, the dataset still contains over a third of profiles where the gender is documented meaning we could use these to look at the differences in mental distress between the genders. Also note that male and female are not equaly represented in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "twitter_df['sentiment'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sentiment, we see that about three-quaters of all tweets are labeled as being neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of temporal data\n",
    "\n",
    "we now look at the temporal distribution of the data. We look at daily and yearly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring functions\n",
    "def get_time_distribution(times):\n",
    "    publishing_time = times.map(lambda x: x.hour).value_counts()\n",
    "    publishing_time.sort_index(inplace=True)\n",
    "    return publishing_time\n",
    "\n",
    "def plot_daily(df):\n",
    "    times = get_time_distribution(df['published'])\n",
    "    return plt.plot(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily(twitter_df)\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('distribution of tweets over 24 hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number of tweets is the largest around 8pm and rapidly decreases through the night, reaching its lowest point at around 3 in the morning. We can also observe a dip in the number of tweets during lunch time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_distribution = twitter_df.published.map(lambda x: x.month).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(data=yearly_distribution, time=yearly_distribution.index)\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('distribution of tweets over 10 month period')\n",
    "plt.xticks(range(1, 11),['jan', 'feb', 'mar', 'apr', 'mai', 'jun', 'jul', 'aug', 'sep', 'oct'])\n",
    "plt.xlabel('month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at the yearly distribution we see that the dataset is not balanced. It is possible that some of this inbalance is due to the number of twitter users and thus tweets rappidly increasing over a 10 month period. It may also be linked with the way Spinn3r retrived data from the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of location data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously noted that only about 40% of all tweets have geo-location attached. We still look at how they are distributed over all. We note that we can find some points that are not in Switzerland. We do not filter out these tweets as we take the dataset as correct.\n",
    "\n",
    "We use the tutoral by http://andrewgaidus.com/leaflet_webmaps_python/ to create this map. We use the geodata provided in the 3rd Homework and transform it into a shapefile using an online converter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting this here for convenience\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "#from folium.element import IFrame\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import pysal as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should probably move this into processing\n",
    "def process_location_data(df):\n",
    "    \"\"\"preprocessed data in format as found on cluster:\n",
    "    df: dataframe, locations should be in 'geo_point'\n",
    "    \"\"\"\n",
    "    non_null = df[~df.geo_point.isnull()]\n",
    "    virgule = non_null[non_null.geo_point.str.contains(',')].geo_point.str.split(pat=',', expand=True).applymap(float)\n",
    "    no_virgule = non_null[~non_null.geo_point.str.contains(',')].geo_point.str.split(expand=True).applymap(float)\n",
    "    return pd.concat([virgule, no_virgule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = process_location_data(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfrom to geoseries, use the correct format\n",
    "location_geo = gpd.GeoSeries(locations.apply(lambda z: Point(z[1], z[0]), 1),crs={'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with dataframe\n",
    "location = gpd.GeoDataFrame(twitter_df[~twitter_df.geo_point.isnull()], geometry=location_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts = gpd.read_file('mygeodata/ch-cantons.topojson.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join based on location data, we associate a region with a location\n",
    "tract_counts = gpd.tools.sjoin(location, tracts).groupby('id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the counts to the tracts data\n",
    "tracts.set_index('id', inplace=True)\n",
    "tracts['counts'] = tract_counts\n",
    "tracts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_counts.head() #this is how it looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using this data we can simply create a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWISS_COORD = [46.85, 8.23] #location of switzerland\n",
    "swiss_map = folium.Map(location = SWISS_COORD, zoom_start = 8, tiles = 'cartodbpositron')\n",
    "\n",
    "swiss_map.choropleth(tracts.to_json(), data = tracts, key_on = 'feature.properties.{}'.format('id'),\n",
    "                columns = ['id', 'counts'], fill_color = 'YlOrRd', threshold_scale=[1000, 10000, 20000, 30000, 40000])\n",
    "swiss_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the tweets are concentrated in urban centers.\n",
    "#it'd be nice to have a more finely grained map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP _(Natural Language Processing)_ is necessary given the nature of our dataset: tweets. Following the steps used in previous courses and in the papers we read, we came up with the following pipeline in order to process the tweets (and dictionary). The actualy pipeline is realized in the *processing_pipeline.py* script, as the pipeline takes about 5 mintes to run on our set of tweets. We here explain it's functionality.\n",
    "\n",
    "_Note: we use nltk, a goto python NLP library which was very interesting for us as it offered operations in the various languages we are working on._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "(cf. _Next part_ below for information)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def remove_stops(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "    lang_set = stopwords.words(language)\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [word for word in tweet if word not in lang_set])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def stem_words(language, cleaned, frame):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    cleaned.loc[cleaned['lang'] == lang, frame] = cleaned.loc[cleaned['lang'] == lang, frame].\\\n",
    "        apply(lambda tweet: [stemmer.stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is mainly based on treating the data to obtain an adequate format to work with following 3 steps:\n",
    "- Tokenizing: separating words, we use a special tweettokenizer to work onthe tweets\n",
    "- Stop-words removal\n",
    "- Stemming: keeping only the radical of a word (allows better comparison)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tknzr = TweetTokenizer()\n",
    "twitter_df['tokenized'] = twitter_df['main'].map(lambda x: tknzr.tokenize(x))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "remove_stops('english', df, 'tokenized')\n",
    "remove_stops('french', df, 'tokenized')\n",
    "remove_stops('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stem_words('english', df, 'tokenized')\n",
    "stem_words('french', df, 'tokenized')\n",
    "stem_words('german', df, 'tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building our dictionary was doing research on previous dictionaries that were used for similar problems, such as can be seen in [2], [3]. Starting from this, we built our own English dictionary _($4^{th}$ reference in the Bibliography at the end of the file)_ and carefully translated it in French and German.\n",
    "\n",
    "_Note_: a more complete explanation of our dictionary construction is included in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_PATH = \"dictionary_th2.csv\"\n",
    "dictionaries = pd.read_csv(DICT_PATH)\n",
    "dictionaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = dictionaries['english'].dropna()\n",
    "fr_dict = dictionaries['french'].dropna()\n",
    "de_dict = dictionaries['german'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fetching our 3 datasets, we clean them as we did for the tweets in _Part 2_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "en_dict = en_dict.map(lambda x: tknzr.tokenize(x))\n",
    "fr_dict = fr_dict.map(lambda x: tknzr.tokenize(x))\n",
    "de_dict = de_dict.map(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our (clean) data, we simply run all the previous NLP methods on our dictionaries. \n",
    "\n",
    "_Note_: Special methods were written for the dictionaries as they do not follow the same format as tweets, but they are the same in essence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_cleaning(lang):\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_dict = lang_dict.astype(str).str.lower()\n",
    "\n",
    "def dict_remove_stops(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    lang_set = stopwords.words(language)\n",
    "    lang_dict = lang_dict.apply(lambda expression: [word for word in expression if word not in lang_set])\n",
    "\n",
    "def dict_stem_words(language):\n",
    "    lang = language[:2]\n",
    "    if language == 'german':\n",
    "        lang = 'de'\n",
    "\n",
    "    lang_dict = eval(lang + '_dict')\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    lang_dict = lang_dict.apply(lambda expression: [stemmer.stem(word) for word in expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_remove_stops('english')\n",
    "dict_remove_stops('french')\n",
    "dict_remove_stops('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_stem_words('english')\n",
    "dict_stem_words('french')\n",
    "dict_stem_words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the soundness of our method, we display the head of each dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we cross the dataset with our dictionaries to retrieve the tweets exhibiting mental distress. This is the first (less naive) step of our analysis before using Topic Modeling methods on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we can find a dict entry for each tweet, by using the following algorithm, that checks if a keyword occurs at least once in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, t):\n",
    "    \"\"\"compares to lists, and returns \n",
    "    true for all possible permutations of same list\n",
    "    s, t: lists to compare\n",
    "    \"\"\"\n",
    "    return Counter(s) == Counter(t)\n",
    "\n",
    "def match_dict(tweet, dict_):\n",
    "    \"\"\"returns keywords that match in string\n",
    "    tweet: tweet to find keywords in\n",
    "    dict_: list of keywords\n",
    "    \"\"\"\n",
    "    doc = [sublist for sublist in dict_ if compare(list(filter(lambda x: x in tweet, sublist)), sublist) ]\n",
    "    return doc #at least one match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following results for each language. We preform the actual algorithm in a script and save the results into a pickel file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to get the results would be to write the following line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "english_filtered = twitter_df[twitter_df.lang == 'en']['tokenized'].map(lambda x: match_dict(x, en_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load( open( \"keyword_frames/english_keyworded_tweets.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filtered = df[(df.lang == 'en') & (df.keywords.map(lambda x: x != [] ))]\n",
    "english_filtered.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see sadness an loneliness or fear in these tweets, but some of them do not actualy express distress.\n",
    "We also note that the number of tweets has greatly been reduces through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the French tweets and again find similar results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "french_filtered = df[df.lang == 'fr']['tokenized'].map(lambda x: match_dict(x, fr_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load( open( \"keyword_frames/french_keyworded_tweets.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filtered = df[(df.lang == 'fr') & (df.keywords.map(lambda x: x != [] ))]\n",
    "french_filtered.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for German we get the following results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "german_filtered = df[df.lang == 'de']['tokenized'].map(lambda x: match_dict(x, de_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkl.load( open( \"keyword_frames/german_keyworded_tweets.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_filtered = df[(df.lang == 'de') & (df.keywords.map(lambda x: x != [] ))]\n",
    "german_filtered.main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do all of this tweets reference the person, but they also talk about sadness or dissapointment.\n",
    "The remaining number is quite small, compared to the initial 4000 german tweets in the dataset, but it is a number we might expect.\n",
    "\n",
    "Still those results could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a script we build an LDA model and save it to the models folder.\n",
    "We quickly present how we create this model, and display the results using the pyLDAvis library.\n",
    "\n",
    "To build the LDA we create a dictionary and define the 'texts', in our case, the tweets.\n",
    "We pass it to gensim which implements the building of an LDA model. \n",
    "\n",
    "#idealy we should build multiple ones and keep the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(english_filtered.tokenized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.3) #remove too frequent and too infrequent words\n",
    "texts = english_filtered.tokenized.tolist() #list of all tweets\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.LdaMulticore(corpus, id2word=dictionary, num_topics=30, workers=3, iterations=100, passes=20)\n",
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the visualisation\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dictionary Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from Dataset 1 to Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting is as follows: **Column in Dataset 2** -> Column in Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **author_user_id**           -> userId\n",
    "- **geo_point**                -> longitude + latitude (or: placeLongitude + placeLatitude)\n",
    "- **main**                     -> text\n",
    "- **published**                -> createdAt\n",
    "- **source_followers**         -> followersCount\n",
    "- **source_following**         -> friendsCount (or is it folowwers that are following ?)\n",
    "- **source_location**          -> userLocation\n",
    "\n",
    "\n",
    "#### Dataset 1 having no equivalent : \n",
    "- id                                -> tweet ID\n",
    "- truncated                         -> no idea, only NaNs\n",
    "- placeId                           -> ID of where they are\n",
    "- inReplyTo                         -> If it is a tweet reply\n",
    "- source & sourceName, sourceUrl    -> what user used to send tweet (Android, website, and so on)\n",
    "- userName                          -> username\n",
    "- screenName                        -> name shown on tweets from username\n",
    "- statusesCount                     -> number of status of user\n",
    "\n",
    "#### Dataset 2 having no equivalent : \n",
    "- **author_gender**                 -> author's gender\n",
    "- **lang**                          -> language of tweet (given)\n",
    "- **sentiment**                     -> tweet's sentiment analysis (given)\n",
    "- **source_spam_probability**       -> spam probability of tweet (given)\n",
    "- **tags**                          -> hashtags contained in the tweet (in Dataset 1 stays in text)\n",
    "- **tokenized**                     -> tokenized text of tweet (done by us)\n",
    "- **keywords**                      -> keywords fund in tweets (done by us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processing Pipeline on the Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a *txt schema*, giving us an idea of the meaning of each column in the *tsv file* containing the tweets. We were also given a sample file to get an overview of the data, but we also optained the complete set of tweets (5GB) in a _.zip_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following useful columns:\n",
    "\n",
    "- **userId** : id identifying the user.\n",
    "- **createdAt** : time the tweet was posted.\n",
    "- **text** : content of the tweet.\n",
    "- **placeLatitude** : latitude of the tweet.\n",
    "- **placeLongitude** : longitude of the tweet.\n",
    "- **sourceName** : username.\n",
    "- **sourceUrl** : URL of the tweet.\n",
    "- **followersCount** : number of followers.\n",
    "- **friendsCount** : number of mutual follows.\n",
    "- **statusesCount** : number of statuses of user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset contains a lot of NaN values, and each column contains at least 1% or more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete analysis and code can be found in the [Basic Exploration dataset 1 notebook](Basic%20Exploration%20Dataset%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of the Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
